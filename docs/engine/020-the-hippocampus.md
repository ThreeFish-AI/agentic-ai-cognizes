---
id: the-hippocampus-implementation
sidebar_position: 2.0
title: Phase 2ï¼šThe Hippocampus éªŒè¯å®æ–½æ–¹æ¡ˆ
last_update:
  author: Aurelius Huang
  created_at: 2026-01-08
  updated_at: 2026-01-12
  version: 1.1
  status: Pending Review
tags:
  - The Hippocampus
  - Memory System
  - Implementation Plan
  - PostgreSQL
  - Zero-ETL
---

> [!NOTE]
>
> **æ–‡æ¡£å®šä½**ï¼šæœ¬æ–‡æ¡£æ˜¯ [000-roadmap.md](./000-roadmap.md) Phase 2 çš„è¯¦ç»†å·¥ç¨‹å®æ–½æ–¹æ¡ˆï¼Œç”¨äºæŒ‡å¯¼ã€Œ**The Hippocampus (ä»¿ç”Ÿè®°å¿†)**ã€çš„å®Œæ•´è½åœ°éªŒè¯å·¥ä½œã€‚æ¶µç›–æŠ€æœ¯è°ƒç ”ã€æ¶æ„è®¾è®¡ã€ä»£ç å®ç°ã€æµ‹è¯•éªŒè¯ç­‰å…¨æµç¨‹ã€‚
>
> **å‰ç½®ä¾èµ–**ï¼šæœ¬é˜¶æ®µä¾èµ– [010-the-pulse.md](./010-the-pulse.md) Phase 1 çš„å®Œæˆï¼Œéœ€å¤ç”¨å…¶ç»Ÿä¸€å­˜å‚¨åŸºåº§ (Unified Schema) å’Œä¼šè¯ç®¡ç†èƒ½åŠ›ã€‚

---

## 1. æ‰§è¡Œæ‘˜è¦

### 1.1 å®šä½ä¸ç›®æ ‡ (Phase 2)

**Phase 2: The Hippocampus** æ˜¯æ•´ä¸ªéªŒè¯è®¡åˆ’çš„è®°å¿†æ ¸å¿ƒé˜¶æ®µï¼Œå¯¹æ ‡äººç±»å¤§è„‘çš„**æµ·é©¬ä½“ (Hippocampus)** â€”â€” è´Ÿè´£å°†çŸ­æœŸè®°å¿†è½¬åŒ–ä¸ºé•¿æœŸè®°å¿†çš„å…³é”®è„‘åŒºã€‚æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

1. **å®ç° Zero-ETL è®°å¿†æ¶æ„**ï¼šæ‘’å¼ƒä¼ ç»Ÿ `Redis (App)` + `VectorDB (Mem)` çš„å‰²è£‚æ¶æ„ï¼ŒSession Log ä¸ Semantic Memory åŒåº“å­˜å‚¨
2. **éªŒè¯è®°å¿†å·©å›ºæœºåˆ¶**ï¼šå®ç°ä» Short-term åˆ° Long-term çš„æ— ç¼æµè½¬ï¼ˆFast Replay + Deep Reflectionï¼‰
3. **éªŒè¯ç”Ÿç‰©é—å¿˜æœºåˆ¶**ï¼šå®ç°è‰¾å®¾æµ©æ–¯è¡°å‡ç®—æ³•ï¼Œè‡ªåŠ¨æ¸…ç†ä½ä»·å€¼è®°å¿†
4. **éªŒè¯ Context Budgeting**ï¼šå®ç°åŠ¨æ€ä¸Šä¸‹æ–‡ç»„è£…ï¼Œç²¾å‡†æ§åˆ¶ Token é¢„ç®—

```mermaid
graph LR
    subgraph "Phase 2: The Hippocampus"
        F[Phase 1 åŸºåº§<br>Session Engine] --> H1[Memory Consolidation<br>è®°å¿†å·©å›º]
        F --> H2[Biological Retention<br>é—å¿˜ä¸ä¿æŒ]
        F --> H3[Context Budgeting<br>ä¸Šä¸‹æ–‡é¢„ç®—]
    end

    H1 & H2 & H3 --> V[Verification<br>éªŒæ”¶é€šè¿‡]
    V --> Phase3[Phase 3: Perception]

    style F fill:#065f46,stroke:#34d399,color:#fff
    style H1 fill:#7c2d12,stroke:#fb923c,color:#fff
    style H2 fill:#7c2d12,stroke:#fb923c,color:#fff
    style H3 fill:#7c2d12,stroke:#fb923c,color:#fff
```

### 1.2 æ ¸å¿ƒè®¤çŸ¥æ¶æ„ (Core Cognitive Architecture)

ä¸ºäº†æ„å»ºå…·å¤‡"é•¿æœŸå¿ƒæ™º"çš„ Agentï¼Œæˆ‘ä»¬å‚ç…§è®¤çŸ¥å¿ƒç†å­¦æ¨¡å‹ï¼Œè®¾è®¡äº†æ›´åŠ ä½“ç³»åŒ–çš„è®°å¿†ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä¸ä»…æ˜¯æ•°æ®çš„å­˜å‚¨åº“ï¼Œæ›´æ˜¯ä¿¡æ¯æµè½¬ä¸å‡ç»´çš„åŠ å·¥å‚ã€‚

#### 1.2.1 è®°å¿†æ¨¡å‹ï¼šæ­£äº¤çš„ä¸‰ç»´è§†å›¾ (Static View)

æˆ‘ä»¬å°†é•¿æœŸè®°å¿†è§£è€¦ä¸ºä¸‰ä¸ªæ­£äº¤ç»´åº¦ï¼Œåˆ†åˆ«è§£å†³"ç»å†"ã€"çŸ¥è¯†"ä¸"æŠ€èƒ½"çš„æŒä¹…åŒ–é—®é¢˜ï¼š

| è®°å¿†ç»´åº¦ (Dimension)                  | è®¤çŸ¥éšå–» (Metaphor) | æ•°æ®å½¢æ€ (Schema)                               | æ ¸å¿ƒèŒèƒ½ (Function)                                            | å­˜å‚¨å®ä½“       |
| :------------------------------------ | :------------------ | :---------------------------------------------- | :------------------------------------------------------------- | :------------- |
| **Episodic Memory**<br>(æƒ…æ™¯è®°å¿†)     | **"è‡ªä¼ ä½“æµ"**      | **æ—¶åºç‰‡æ®µ** + å‘é‡åµŒå…¥<br>(Time-Series Chunks) | è®°å½•"å‘ç”Ÿäº†ä»€ä¹ˆ"ã€‚æä¾›è¿ç»­çš„äº¤äº’ä¸Šä¸‹æ–‡ï¼Œç»´æŠ¤å¯¹è¯çš„å†å²è¿è´¯æ€§ã€‚ | `memories`     |
| **Semantic Memory**<br>(è¯­ä¹‰è®°å¿†)     | **"æ¦‚å¿µç½‘ç»œ"**      | **ç»“æ„åŒ–äº‹å®** + å…³ç³»<br>(Structured Facts)     | è®°å½•"æ˜¯ä»€ä¹ˆ"ã€‚æ²‰æ·€ç”¨æˆ·åå¥½ã€ç”»åƒä¸ä¸–ç•ŒçŸ¥è¯†ï¼Œè·¨ä¼šè¯å¤ç”¨ã€‚       | `facts`        |
| **Procedural Memory**<br>(ç¨‹åºæ€§è®°å¿†) | **"è‚Œè‚‰è®°å¿†"**      | **æŒ‡ä»¤é›†** + ç‰ˆæœ¬æ§åˆ¶<br>(Instructions)         | è®°å½•"æ€ä¹ˆåš"ã€‚å›ºåŒ– Agent çš„è¡Œä¸ºæ¨¡å¼ã€SOP ä¸å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚      | `instructions` |

#### 1.2.2 åŠ¨æ€æœºåˆ¶ï¼šæµ·é©¬ä½“å¾ªç¯ (Dynamic View)

æ¨¡ä»¿äººè„‘çš„æµ·é©¬ä½“ (Hippocampus) åŠŸèƒ½ï¼Œæˆ‘ä»¬åœ¨ç³»ç»Ÿä¸­å¼•å…¥äº†**è®°å¿†å·©å›º (Consolidation)** ä¸**å†æ¿€æ´» (Reactivation)** çš„åŠ¨æ€å¾ªç¯ï¼š

```mermaid
graph TB
    subgraph WM_Scope ["Working Memory (å·¥ä½œè®°å¿†)"]
        direction TB
        WM[Context Window<br>å½“å‰ä¸Šä¸‹æ–‡]
    end

    subgraph Processes ["Cognitive Processes (è®¤çŸ¥)"]
        direction TB
        Encoding(Encoding<br>åŒé‡ç¼–ç )
        Retrieval(Retrieval<br>è”æƒ³æ£€ç´¢)
        Consolidation(Consolidation<br>åå°å·©å›º)
        Decay(Decay<br>ç”Ÿç‰©è¡°å‡)
    end

    subgraph Hippocampus ["Hippocampus (LTM)"]
        direction TB
        EM[(Episodic<br>æƒ…æ™¯è®°å¿†)]
        SM[(Semantic<br>è¯­ä¹‰è®°å¿†)]
        PM[(Procedural<br>ç¨‹åºæ€§è®°å¿†)]
    end

    %% Flows
    Input(User Input) --> WM
    WM -->|å®æ—¶å†™å…¥| Encoding
    Encoding -->|å­˜å‚¨| Hippocampus

    WM -.->|å¼‚æ­¥æç‚¼| Consolidation
    Consolidation -->|æå–äº‹å®| Hippocampus
    Consolidation -->|å‹ç¼©å½’æ¡£| Hippocampus

    Retrieval -->|æ³¨å…¥| WM
    Hippocampus -.->|å‘é‡ç´¢å¼•| Retrieval
    Hippocampus -.->|å®šæœŸæ¸…ç†| Decay

    style WM_Scope fill:#065f46,stroke:#34d399,color:#fff
    style Hippocampus fill:#1e3a5f,stroke:#60a5fa,color:#fff
    style Processes fill:#7c2d12,stroke:#fb923c,color:#fff
```

1. **Working Memory (å·¥ä½œè®°å¿†)**ï¼šä½œä¸ºç³»ç»Ÿçš„"å‰é¢å¶"ï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶ç»´æŠ¤å½“å‰çš„ä¸Šä¸‹æ–‡çª—å£ (`Context Window`)ã€‚
2. **Hippocampus (æµ·é©¬ä½“/LTM)**ï¼šé•¿æ—¶è®°å¿†çš„å­˜å‚¨ä¸­å¿ƒï¼Œç”±æƒ…æ™¯ (`Episodic`)ã€è¯­ä¹‰ (`Semantic`) å’Œç¨‹åºæ€§ (`Procedural`) ä¸‰ä¸ªæ­£äº¤çš„è®°å¿†åŒºç»„æˆã€‚
3. **Cognitive Processes (è®¤çŸ¥è¿‡ç¨‹)**ï¼šè¿æ¥ WM ä¸ LTM çš„åŠ¨æ€æœºåˆ¶ï¼Œé€šè¿‡ä»¥ä¸‹å››ä¸ªå…³é”®è¿‡ç¨‹ç»´æŒç³»ç»Ÿçš„"æ–°é™ˆä»£è°¢"ï¼š
   - **Encoding (ç¼–ç )**ï¼šå°†å®æ—¶çš„çŸ­æœŸäº¤äº’è½¬åŒ–ä¸ºå¯å­˜å‚¨çš„è®°å¿†ç—•è¿¹ã€‚
   - **Consolidation (å·©å›º)**ï¼šåœ¨åå°å¼‚æ­¥è¿è¡Œï¼Œå°†ç¢ç‰‡åŒ–çš„å¯¹è¯å†å²æç‚¼ä¸ºç»“æ„åŒ–çš„äº‹å®ä¸çŸ¥è¯†ã€‚
   - **Retrieval (æ£€ç´¢)**ï¼šåŸºäºè¯­ä¹‰ç›¸å…³æ€§ï¼Œåœ¨éœ€è¦æ—¶å°†æ²‰ç¡çš„é•¿æœŸè®°å¿†"å†æ¿€æ´»"å¹¶åŠ è½½å›å·¥ä½œè®°å¿†ã€‚
   - **Decay (è¡°å‡)**ï¼šæ¨¡æ‹Ÿç”Ÿç‰©é—å¿˜æœºåˆ¶ï¼Œå®šæœŸæ¸…ç†ä½ä»·å€¼æˆ–é•¿æœŸæœªè¢«è®¿é—®çš„è®°å¿†ï¼Œé˜²æ­¢è®°å¿†åº“è‡ƒè‚¿ã€‚

#### 1.2.3 å…³é”®ç‰¹æ€§ (Key Features)

1. **åŒé‡è·¯å¾„ (Dual Pathways)**:
   - **å¿«è·¯å¾„ (Fast Path)**: å®æ—¶å¯¹è¯æµç›´æ¥è¿›å…¥å·¥ä½œè®°å¿†ï¼Œä¿è¯å“åº”é€Ÿåº¦ã€‚
   - **æ…¢è·¯å¾„ (Slow Path)**: å¼‚æ­¥è¿›ç¨‹åœ¨åå°è¿›è¡Œ"åæ€"ä¸"å·©å›º"ï¼Œå°†ç¢ç‰‡åŒ–å¯¹è¯è½¬åŒ–ä¸ºç»“æ„åŒ–çŸ¥è¯†ã€‚

2. **è”æƒ³å¬å› (Associative Recall)**:
   - æ‘’å¼ƒå•çº¯çš„å…³é”®è¯åŒ¹é…ï¼Œåˆ©ç”¨ **Embedding Vector** å®ç°åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ¨¡ç³Šå¬å›ï¼Œæ¨¡æ‹Ÿ"è§¦æ™¯ç”Ÿæƒ…"çš„è®¤çŸ¥ä½“éªŒã€‚

3. **ç”Ÿç‰©æ€§é—å¿˜ (Biological Decay)**:
   - å¼•å…¥åŸºäº Ebbinghaus é—å¿˜æ›²çº¿çš„ `Retention Score`ï¼Œè®©ä½ä»·å€¼è®°å¿†éšæ—¶é—´è‡ªç„¶æ¶ˆé€€ï¼Œä¿æŒè®°å¿†åº“çš„"ä¿¡å™ªæ¯”"ä¸é²œæ´»æ€§ã€‚

### 1.3 æ‰§è¡Œå¯¼å›¾ (Execution Map)

ä¸ºäº†ç¡®ä¿ç³»ç»Ÿçš„**æ­£äº¤æ€§ (Orthogonality)** ä¸**è‡ªæ´½æ€§ (Self-consistency)**ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œè®¡åˆ’é‡æ„ä¸ºåˆ†å±‚é€’è¿›çš„å®æ–½è·¯å¾„ï¼Œç¡®ä¿æ¯ä¸€å±‚éƒ½åœ¨åšå®çš„åŸºç¡€ä¸Šæ„å»ºã€‚

#### 1.3.1 ä»»åŠ¡-æ–‡æ¡£é”šå®š

æˆ‘ä»¬å°†å·¥ç¨‹ä»»åŠ¡æ˜ å°„åˆ°æ¶æ„çš„ä¸‰ä¸ªæ­£äº¤åˆ‡é¢ï¼š**åŸºç¡€æ¶æ„ (Infra)**ã€**è®¤çŸ¥è¿‡ç¨‹ (Process)** ä¸ **æœåŠ¡é›†æˆ (Service)**ã€‚

> [!NOTE]
>
> **ç‰ˆæœ¬å¯¹ç…§**ï¼šæœ¬è®¡åˆ’å±äº **Engine Roadmap (Phase 2)**ï¼Œå¯¹åº” **Project Roadmap ([002-task-checklist](../002-task-checklist.md))** ä¸­çš„ **Phase 3 (T3.3 è®°å¿†æŒä¹…åŒ–)**ã€‚

| æ¶æ„åˆ‡é¢ (Layer)                  | æ ¸å¿ƒç»„ä»¶ (Component)               | å…³é”®èŒè´£ (Responsibility)                                                           | å¯¹åº”ä»»åŠ¡é›† (Engine)                                                           | å¯¹åº”ä»»åŠ¡é›† (Project)                                 |
| :-------------------------------- | :--------------------------------- | :---------------------------------------------------------------------------------- | :---------------------------------------------------------------------------- | :--------------------------------------------------- |
| **L0: Foundation**<br>é™æ€å­˜å‚¨å±‚  | **Unified Schema**<br>Repositories | å®šä¹‰è®°å¿†çš„ä¸‰ç»´å½¢æ€ (`Episodic`, `Semantic`, `Procedural`) åŠå…¶æŒä¹…åŒ–æ¥å£ã€‚          | **P2-2 (Part)**<br>- Schema Definition<br>- Repository Implementation         | **T3.3.1 - T3.3.3**<br>- çŸ­æœŸ/é•¿æœŸ/æƒ…æ™¯è®°å¿†å­˜å‚¨      |
| **L1: Inflow**<br>åŠ¨æ€ç”Ÿæˆå±‚      | **Consolidation Worker**           | å®ç°è®°å¿†çš„**åŒé‡ç¼–ç **ï¼š<br>- Fast Replay (æ‘˜è¦)<br>- Deep Reflection (äº‹å®æå–)    | **P2-2 (Main)**<br>- Worker Skeleton<br>- Prompt Engineering<br>- Async Queue | **T3.3.7**<br>- è®°å¿†å›ºåŒ–æœºåˆ¶                         |
| **L2: Lifecycle**<br>åŠ¨æ€ç»´æŠ¤å±‚   | **Retention Manager**              | å®ç°è®°å¿†çš„**ç”Ÿç‰©å‘¨æœŸ**ï¼š<br>- Ebbinghaus Decay (é—å¿˜)<br>- Context Budgeting (ç»„è£…) | **P2-3**<br>- Scoring Algorithm<br>- Window Assembly                          | **T3.3.7**<br>- è‡ªåŠ¨ç»´æŠ¤ä¸æ¸…ç†                       |
| **L3: Integration**<br>æœåŠ¡é€‚é…å±‚ | **Memory Service**                 | å®ç°ä¸ ADK çš„**æ ‡å‡†å¥‘çº¦**ï¼š<br>- Interface Adapter<br>- Hybrid Search               | **P2-4**<br>- ADK Integration<br>- E2E Verification                           | **T3.3.5, T3.3.6**<br>- è®°å¿†ç®¡ç†å™¨<br>- è®°å¿†æ£€ç´¢åŠŸèƒ½ |

#### 1.3.2 å·¥æœŸå®‰æ’ (2.5 Days)

| é˜¶æ®µ          | é‡Œç¨‹ç¢‘å®šä¹‰ (Milestone)                   | å…³é”®äº¤ä»˜ç‰© (Deliverables)                                             | é¢„ä¼°å·¥æœŸ |
| :------------ | :--------------------------------------- | :-------------------------------------------------------------------- | :------- |
| **Phase 2.1** | **Cognitive Alignment**<br>(è®¤çŸ¥å¯¹é½)    | âœ… è®°å¿†æœºåˆ¶è°ƒç ”æŠ¥å‘Š<br>âœ… æŠ€æœ¯é€‰å‹å¯¹æ¯”è¡¨                              | 0.25 Day |
| **Phase 2.2** | **Memory Formation**<br>(è®°å¿†ç”Ÿæˆæœºåˆ¶)   | âœ… Hippocampus Schema DDL<br>âœ… `ConsolidationWorker` (Alpha)         | 1.0 Day  |
| **Phase 2.3** | **Memory Dynamics**<br>(è®°å¿†åŠ¨åŠ›å­¦)      | âœ… `retention_score` ç®—æ³•å®ç°<br>âœ… `get_context_window` å­˜å‚¨è¿‡ç¨‹     | 0.5 Day  |
| **Phase 2.4** | **Cortex Integration**<br>(å…¨è„‘é›†æˆéªŒæ”¶) | âœ… `PostgresMemoryService` (ADK compliant)<br>âœ… è®°å¿†ç³»ç»ŸéªŒæ”¶æµ‹è¯•æŠ¥å‘Š | 0.25 Day |
| **Phase 2.5** | æµ‹è¯•                                     | æµ‹è¯•ä»£ç  + éªŒè¯æ–‡æ¡£                                                   | 0.5 Day  |

---

## 2. æ ¸å¿ƒå‚è€ƒæ¨¡å‹ï¼šä»¿ç”Ÿè®°å¿†æœºåˆ¶

### 2.1 Google ADK

#### 2.1.1 å¯¹æ ‡åˆ†æï¼šGoogle ADK MemoryService

åŸºäº Google ADK å®˜æ–¹æ–‡æ¡£<sup>[[3]](#ref3)</sup>ï¼Œæˆ‘ä»¬å°†å¤åˆ»å…¶æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶æ˜ å°„åˆ° PostgreSQL ç”Ÿæ€ï¼š

| ADK æ ¸å¿ƒæ¦‚å¿µ      | å®šä¹‰                          | æˆ‘ä»¬çš„å¤åˆ»å®ç° (PostgreSQL)                 | é”šå®šä»£ç                                                                                    |
| :---------------- | :---------------------------- | :------------------------------------------ | :----------------------------------------------------------------------------------------- |
| **MemoryService** | è·¨ä¼šè¯çš„å¯æœç´¢çŸ¥è¯†åº“ç®¡ç†æ¥å£  | `PostgresMemoryService`                     | [memory_service.py](file:///src/cognizes/adapters/postgres/memory_service.py)              |
| **Memory**        | ä»å¯¹è¯ä¸­æå–çš„ç»“æ„åŒ–çŸ¥è¯†ç‰‡æ®µ  | `memories` è¡¨ (å‘é‡)<br>`facts` è¡¨ (ç»“æ„åŒ–) | [schema/hippocampus_schema.sql](file:///src/cognizes/engine/schema/hippocampus_schema.sql) |
| **add_session**   | å°† Session è½¬åŒ–ä¸ºå¯æœç´¢çš„è®°å¿† | `ConsolidationWorker` (å¼‚æ­¥)                | [consolidation_worker.py](file:///src/cognizes/engine/hippocampus/consolidation_worker.py) |
| **search_memory** | åŸºäº Query æ£€ç´¢ç›¸å…³è®°å¿†       | æ··åˆæ£€ç´¢ (Vector + JSONB)                   | `search_memory()`                                                                          |

#### 2.1.2 æ¥å£å¥‘çº¦ (Interface Contract)

æˆ‘ä»¬éµå¾ª ADK çš„ `BaseMemoryService` æ ‡å‡†æ¥å£ï¼Œç¡®ä¿ **Drop-in Compatible**ï¼š

```python
class BaseMemoryService(ABC):
    @abstractmethod
    async def add_session_to_memory(self, session: Session) -> None:
        """Trigger: å¼‚æ­¥è§¦å‘è®°å¿†å·©å›º (Inflow)"""
        ...

    @abstractmethod
    async def search_memory(self, *, app_name: str, user_id: str, query: str) -> SearchMemoryResponse:
        """Trigger: å®æ—¶æ£€ç´¢ç›¸å…³è®°å¿† (Retrieval)"""
        ...
```

#### 2.1.3 å·¥ä½œæµå‚è€ƒ (Workflow Reference)

Memory Bank çš„æ ¸å¿ƒä»·å€¼åœ¨äºå°† **å†™å…¥ (Consolidation)** ä¸ **è¯»å– (Retrieval)** è§£è€¦ï¼š

```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant Session as SessionService
    participant Memory as MemoryService
    participant Worker as ConsolidationWorker

    Note over Agent, Memory: Hot Path (å®æ—¶å“åº”)
    User->>Agent: ç”¨æˆ·æ¶ˆæ¯
    Agent->>Session: append_event()
    Agent->>Memory: search_memory(query)
    Memory-->>Agent: ç›¸å…³è®°å¿† (Context)
    Agent->>User: è¿”å›å›å¤

    Note over Session, Worker: Background Path (å¼‚æ­¥å·©å›º)
    Session-)Memory: add_session_to_memory()
    Memory-)Worker: dispatch_job()
    Worker->>Worker: LLM Extraction (Facts/Insights)
    Worker->>Memory: Persist (Vector + Struct)
```

**å…³é”®æ´å¯Ÿ**ï¼š

1. **æ­£äº¤æ€§**: è®°å¿†ç”Ÿæˆ (Worker) ä¸ è®°å¿†ä½¿ç”¨ (Agent) äº’ä¸é˜»å¡ã€‚
2. **åŒå‘æµ**: Session æ•°æ®æµå…¥ Memoryï¼ŒMemory çŸ¥è¯†æµå› Agentã€‚
3. **ç™½ç›’åŒ–**: æˆ‘ä»¬å°†åŸç‰ˆé»‘ç›’çš„ Vertex AI é€»è¾‘æ›¿æ¢ä¸ºå¯è§‚æµ‹çš„ `ConsolidationWorker`ã€‚

#### 2.1.4 å†™å…¥ç­–ç•¥ (Writing Strategy)

ç»“åˆ LangGraph çš„è®¾è®¡ç†å¿µ<sup>[[2]](#ref2)</sup>ï¼Œæˆ‘ä»¬åœ¨æ—¶åºå›¾ä¸­æ˜ç¡®åŒºåˆ†äº†ä¸¤ç§å†™å…¥è·¯å¾„ï¼š

| è·¯å¾„ (Path)    | æ¨¡å¼ (Mode)  | å¯¹åº”æœºåˆ¶                       | ä¼˜åŠ¿ (Pros)                   | åŠ£åŠ¿ (Cons)                |
| :------------- | :----------- | :----------------------------- | :---------------------------- | :------------------------- |
| **Hot Path**   | åŒæ­¥ (Sync)  | `append_event()` (Session)     | ç«‹å³ä¸€è‡´æ€§ (Read-Your-Writes) | å¢åŠ ç”¨æˆ·ç­‰å¾…å»¶è¿Ÿ           |
| **Background** | å¼‚æ­¥ (Async) | `ConsolidationWorker` (Memory) | é«˜ååï¼Œä¸é˜»å¡ç”¨æˆ·ä½“éªŒ        | å­˜åœ¨çŸ­æš‚çš„"è®°å¿†ä¸ä¸€è‡´çª—å£" |

**æˆ‘ä»¬çš„å†³ç­–**ï¼š

- **Fast Replay**: ä½œä¸ºçƒ­è·¯å¾„çš„è¡¥å……ï¼Œé€šè¿‡ Session å¿«é€Ÿå›æº¯ã€‚
- **Deep Reflection**: **å¿…é¡»å¼‚æ­¥**ã€‚å› ä¸º Fact Extraction éœ€è¦æ˜‚è´µçš„ LLM æ¨ç†ï¼Œç»ä¸èƒ½é˜»å¡ç”¨æˆ·å¯¹è¯ã€‚

### 2.2 LangGraph Memory è®¾è®¡æ¨¡å¼

LangGraph çš„ Memory è®¾è®¡ä¸ºæˆ‘ä»¬æä¾›äº†é‡è¦çš„**å®ç°å‚è€ƒ**<sup>[[2]](#ref2)</sup>ã€‚

#### 2.2.1 æŒä¹…åŒ–æœºåˆ¶å¯¹ç…§

LangGraph æä¾›ä¸¤å¥—äº’è¡¥çš„æŒä¹…åŒ–æœºåˆ¶ï¼Œä¸æˆ‘ä»¬çš„å®ç°å½¢æˆæ¸…æ™°æ˜ å°„ï¼š

| LangGraph æœºåˆ¶   | å­˜å‚¨èŒƒå›´    | å¯¹åº”æˆ‘ä»¬çš„å®ç°                               | é”šå®šè¡¨/æ¨¡å—                         |
| :--------------- | :---------- | :------------------------------------------- | :---------------------------------- |
| **Checkpointer** | å•ä¸ª Thread | Phase 1 çŸ­æœŸè®°å¿† (Session State)             | `threads`, `events`                 |
| **Store**        | è·¨ Thread   | Phase 2 é•¿æœŸè®°å¿† (Consolidated Memory/Facts) | `memories`, `facts`, `instructions` |

#### 2.2.2 ä¸‰ç±»è®°å¿†çš„å®ç°å‚è€ƒ

LangGraph çš„ä¸‰ç±»è®°å¿†åœ¨æˆ‘ä»¬çš„æ–¹æ¡ˆä¸­é€šè¿‡**ç»Ÿä¸€çš„ Repository æ¥å£**å®ç°ï¼š

| è®°å¿†ç±»å‹                  | LangGraph ç”¨é€”       | æˆ‘ä»¬çš„å­˜å‚¨è¡¨   | Repository æ¥å£                                                                  |
| :------------------------ | :------------------- | :------------- | :------------------------------------------------------------------------------- |
| **Semantic** (è¯­ä¹‰è®°å¿†)   | ç”¨æˆ·åå¥½ã€Profile    | `facts`        | [FactsRepository](file:///src/cognizes/core/repositories/facts.py)               |
| **Episodic** (æƒ…æ™¯è®°å¿†)   | å¯¹è¯åˆ‡ç‰‡ã€Few-shot   | `memories`     | [MemoryRepository](file:///src/cognizes/core/repositories/memory.py)             |
| **Procedural** (ç¨‹åºè®°å¿†) | Agent æŒ‡ä»¤ã€è¡Œä¸ºè§„åˆ™ | `instructions` | [InstructionsRepository](file:///src/cognizes/core/repositories/instructions.py) |

<details>
<summary>ğŸ“– LangGraph åŸå§‹ä»£ç å‚è€ƒ (ç‚¹å‡»å±•å¼€)</summary>

```python
# Semantic Memory: ç”¨æˆ·åå¥½å­˜å‚¨
store.put(namespace=(user_id, "preferences"), key="food", value={"likes": ["pizza"], "dislikes": ["spicy"]})

# Episodic Memory: æƒ…æ™¯è®°å¿†æ£€ç´¢
memories = store.search(namespace=(user_id, "episodes"), query="similar task")

# Procedural Memory: Agent è‡ªæˆ‘è¿›åŒ–
store.put(("agent_instructions",), "main", {"instructions": new_instructions})
```

</details>

### 2.3 ç»¼åˆå¯¹æ¯”åˆ†æ (Comparative Analysis)

åŸºäºä¸Šè¿°è°ƒç ”ï¼Œæˆ‘ä»¬å°†å–é•¿è¡¥çŸ­ï¼Œæ„å»º **The Hippocampus** å¼•æ“ï¼š

| ç»´åº¦         | Google ADK MemoryService       | LangGraph Store                  | Open Memory Engine (æˆ‘ä»¬)        |
| :----------- | :----------------------------- | :------------------------------- | :------------------------------- |
| **å­˜å‚¨åç«¯** | Vertex AI Vector Search        | InMemory / Postgres / Redis      | PostgreSQL + PGVector            |
| **è®°å¿†ç±»å‹** | å•ä¸€ Memory ç±»å‹               | Semantic / Episodic / Procedural | ä¸‰ç§è®°å¿†ç±»å‹ + ç»Ÿä¸€å­˜å‚¨          |
| **å†™å…¥æœºåˆ¶** | å¼‚æ­¥ `add_session_to_memory()` | Hot Path / Background å¯é€‰       | Fast Replay + Async Worker       |
| **æ£€ç´¢æ–¹å¼** | `search_memory()` å‘é‡æ£€ç´¢     | `store.search()` è¯­ä¹‰æ£€ç´¢        | æ··åˆæ£€ç´¢ (Vector + JSONB + Time) |
| **å·©å›ºç­–ç•¥** | LLM æå– â†’ è‡ªåŠ¨å‘é‡åŒ–          | åº”ç”¨å±‚æ§åˆ¶                       | ä¸¤é˜¶æ®µå·©å›º + è‰¾å®¾æµ©æ–¯è¡°å‡        |
| **å¼€æ”¾ç¨‹åº¦** | é»‘ç›’ (ä¾èµ– Vertex AI)          | ç™½ç›’ (å®Œå…¨å¯æ§)                  | ç™½ç›’ (PostgreSQL åŸç”Ÿ)           |

### 2.4 è°ƒç ”äº¤ä»˜ç‰©æ‘˜è¦

> [!NOTE]
> æœ¬èŠ‚æ±‡æ€»ä»»åŠ¡ **P2-1-1 ~ P2-1-5** çš„è°ƒç ”æˆæœã€‚è¯¦ç»†çš„æŠ€æœ¯åˆ†æå·²åœ¨å‰æ–‡å±•å¼€ï¼Œæ­¤å¤„ä»…åšç´¢å¼•ç´¢å¼•ä¸äº¤ä»˜ç¡®è®¤ã€‚

#### 2.4.1 æ ¸å¿ƒäº¤ä»˜ç‰©ç´¢å¼•

| ä»»åŠ¡ ID    | ä»»åŠ¡æè¿°                         | äº¤ä»˜å†…å®¹ç´¢å¼•                                                 |
| :--------- | :------------------------------- | :----------------------------------------------------------- |
| **P2-1-1** | ADK `MemoryService` æ¥å£åˆ†æ     | è§ [2.1.2 æ¥å£å¥‘çº¦](#212-æ¥å£å¥‘çº¦-interface-contract)        |
| **P2-1-2** | Memory Bank å·¥ä½œæµåˆ†æ           | è§ [2.1.3 å·¥ä½œæµå‚è€ƒ](#213-å·¥ä½œæµå‚è€ƒ-workflow-reference)    |
| **P2-1-3** | LangGraph `Checkpointer` åˆ†æ    | è§ [2.2.1 æŒä¹…åŒ–æœºåˆ¶å¯¹ç…§](#221-æŒä¹…åŒ–æœºåˆ¶å¯¹ç…§)               |
| **P2-1-4** | LangGraph `Store` è·¨ Thread åˆ†æ | è§ [2.2.2 ä¸‰ç±»è®°å¿†çš„å®ç°å‚è€ƒ](#222-ä¸‰ç±»è®°å¿†çš„å®ç°å‚è€ƒ)       |
| **P2-1-5** | ç»¼åˆå¯¹æ¯”åˆ†æè¡¨                   | è§ [2.3 ç»¼åˆå¯¹æ¯”åˆ†æ](#23-ç»¼åˆå¯¹æ¯”åˆ†æ-comparative-analysis) |

#### 2.4.2 å…³é”®æŠ€æœ¯é€‰å‹ç¡®è®¤

åŸºäºä¸Šè¿°è°ƒç ”ï¼Œæˆ‘ä»¬ç¡®è®¤ä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯æ ˆæ˜ å°„ï¼š

| ç»„ä»¶å±‚çº§         | Google ADK (åŸç‰ˆ)       | The Hippocampus (æˆ‘ä»¬)          | é€‰å‹ä¾æ®                                     |
| :--------------- | :---------------------- | :------------------------------ | :------------------------------------------- |
| **Vector Store** | Vertex AI Vector Search | **PostgreSQL + PGVector**       | ç»Ÿä¸€æŠ€æœ¯æ ˆï¼Œå‡å°‘è¿ç»´ç†µå¢ (Entropy Reduction) |
| **Embedding**    | `textembedding-gecko`   | **Gemini `text-embedding-005`** | é«˜æ€§èƒ½ä¸”æˆæœ¬å¯æ§                             |
| **Extraction**   | Gemini Pro              | **Gemini 3.0 Flash**            | æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œé€‚åˆåå°æ‰¹å¤„ç†               |
| **Index Algo**   | ScaNN                   | **HNSW**                        | PGVector æ ‡é…ï¼Œå…¼é¡¾å¬å›ç‡ä¸æ€§èƒ½              |

---

## 3. æ¶æ„è®¾è®¡ï¼šHippocampus Schema æ‰©å±•

### 3.1 Schema æ‰©å±•è®¾è®¡

åœ¨ Phase 1 çš„ Unified Schema åŸºç¡€ä¸Šï¼Œæ–°å¢ä»¥ä¸‹è®°å¿†ç›¸å…³è¡¨ï¼š

```mermaid
erDiagram
    %% Core Relationships
    threads ||--o{ events : contains
    threads ||--o{ consolidation_jobs : triggers

    %% Process Flow: Inflow
    consolidation_jobs ||--o{ memories : generates
    consolidation_jobs ||--o{ facts : extracts
    consolidation_jobs }o..o| instructions : "updates (implicit)"

    %% Data Ownership (FKs)
    threads ||--o{ memories : "source of"
    threads ||--o{ facts : "source of"

    memories {
        uuid id PK "è®°å¿†å”¯ä¸€æ ‡è¯†"
        uuid thread_id FK "æ¥æºä¼šè¯"
        varchar user_id "ç”¨æˆ·æ ‡è¯†"
        varchar app_name "åº”ç”¨åç§°"
        varchar memory_type "è®°å¿†ç±»å‹: episodic/semantic"
        text content "è®°å¿†å†…å®¹"
        vector embedding "å‘é‡åµŒå…¥ (1536ç»´)"
        jsonb metadata "å…ƒæ•°æ®"
        float retention_score "ä¿ç•™åˆ†æ•°"
        integer access_count "è®¿é—®æ¬¡æ•°"
        timestamp last_accessed_at "æœ€åè®¿é—®æ—¶é—´"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }

    facts {
        uuid id PK "äº‹å®å”¯ä¸€æ ‡è¯†"
        uuid thread_id FK "æ¥æºä¼šè¯"
        varchar user_id "ç”¨æˆ·æ ‡è¯†"
        varchar app_name "åº”ç”¨åç§°"
        varchar fact_type "äº‹å®ç±»å‹: preference/rule/profile"
        varchar key "äº‹å®é”®"
        jsonb value "äº‹å®å€¼"
        vector embedding "å‘é‡åµŒå…¥"
        float confidence "ç½®ä¿¡åº¦"
        timestamp valid_from "ç”Ÿæ•ˆæ—¶é—´"
        timestamp valid_until "å¤±æ•ˆæ—¶é—´"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }

    consolidation_jobs {
        uuid id PK "ä»»åŠ¡å”¯ä¸€æ ‡è¯†"
        uuid thread_id FK "ç›®æ ‡ä¼šè¯"
        varchar status "çŠ¶æ€: pending/running/completed/failed"
        varchar job_type "ä»»åŠ¡ç±»å‹: fast_replay/deep_reflection"
        jsonb result "å¤„ç†ç»“æœ"
        text error "é”™è¯¯ä¿¡æ¯"
        timestamp started_at "å¼€å§‹æ—¶é—´"
        timestamp completed_at "å®Œæˆæ—¶é—´"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }

    instructions {
        uuid id PK "æŒ‡ä»¤å”¯ä¸€æ ‡è¯†"
        varchar app_name "åº”ç”¨åç§°"
        varchar instruction_key "æŒ‡ä»¤é”®"
        text content "æŒ‡ä»¤å†…å®¹"
        integer version "ç‰ˆæœ¬å·"
        jsonb metadata "å…ƒæ•°æ®"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }
```

### 3.2 è®°å¿†æ¨¡å‹èŒè´£è¾¹ç•Œ (Memory Responsibilities)

éµå¾ª **AGENTS.md** çš„ **Orthogonal Decomposition (æ­£äº¤åˆ†è§£)** åŸåˆ™ï¼Œæˆ‘ä»¬å°†ä¸‰ç§è®°å¿†ä¸¥æ ¼æ˜ å°„åˆ°ä¸‰å¼ è¡¨ä¸­ï¼Œç¡®ä¿èŒè´£äº’ä¸é‡å ä¸”è‡ªæ´½ã€‚

#### 3.2.1 èŒè´£æ­£äº¤çŸ©é˜µ

| ç»´åº¦         | **memories** (æƒ…æ™¯æµ)            | **facts** (äº‹å®æ€)                        | **instructions** (è¡Œä¸ºè§„)      |
| :----------- | :------------------------------- | :---------------------------------------- | :----------------------------- |
| **æ ¸å¿ƒèŒè´£** | **Store Experience** (ç»å†)      | **Store Knowledge** (çŸ¥è¯†)                | **Store Behavior** (è¡Œä¸º)      |
| **æ•°æ®å½¢æ€** | **Unstructured Text** (éç»“æ„åŒ–) | **Structured KV** (ç»“æ„åŒ–)                | **System Prompt** (æŒ‡ä»¤æ–‡æœ¬)   |
| **æ—¶åºç‰¹å¾** | **Time-Series** (æµå¼è¿½åŠ )       | **Current State** (çŠ¶æ€è¦†ç›–)              | **Versioned** (ç‰ˆæœ¬æ§åˆ¶)       |
| **å…¸å‹å†…å®¹** | å¯¹è¯åˆ‡ç‰‡ã€é˜¶æ®µæ€§æ€»ç»“ (`summary`) | ç”¨æˆ·ç”»åƒ (`profile`)ã€åå¥½ (`preference`) | Agent äººè®¾ã€äº¤äº’å‡†åˆ™           |
| **æ£€ç´¢æ¨¡å¼** | è¯­ä¹‰ç›¸ä¼¼åº¦ (`search_vector`)     | ç²¾ç¡®é”®å€¼åŒ¹é… + è¯­ä¹‰ (`get` + `search`)    | é”®å€¼åŠ è½½ (`load_instructions`) |

#### 3.2.2 å…³äº `memory_type='semantic'` çš„æ¶ˆæ­§

åœ¨ `memories` è¡¨çš„å®šä¹‰ä¸­ï¼Œ`memory_type` åŒ…å« `semantic` æšä¸¾ï¼Œè¿™ä¸ `facts` è¡¨çœ‹ä¼¼é‡å ã€‚ä¸ºäº†æ¶ˆé™¤æ­§ä¹‰ (Entropy Reduction)ï¼Œæˆ‘ä»¬åšå‡ºä»¥ä¸‹ **æ˜ç¡®ç•Œå®š**ï¼š

1. **`facts` è¡¨ (Primary Semantic)**:
   - **å®šä¹‰**: ç»è¿‡**æ·±åº¦å›ºåŒ–**ã€**å»é‡**ä¸”**ç»“æ„åŒ–**çš„ç¡®åˆ‡çŸ¥è¯†ã€‚
   - **åœºæ™¯**: "ç”¨æˆ·ä¸å–œæ¬¢åƒè¾£", "ç”¨æˆ·çš„èŒä¸šæ˜¯å·¥ç¨‹å¸ˆ"ã€‚è¿™æ˜¯ç³»ç»Ÿè®¤ä¸º"ä¸ºçœŸ"çš„äº‹å®ã€‚

2. **`memories` è¡¨ä¸­çš„ `semantic` ç±»å‹ (Secondary/Transient)**:
   - **å®šä¹‰**: å°šæœªå®Œå…¨ç»“æ„åŒ–ï¼Œæˆ–éš¾ä»¥ç”¨ KV è¡¨è¾¾çš„**æ³›åŒ–çŸ¥è¯†ç‰‡æ®µ**ã€‚ä¹Ÿå¯ä»¥ç†è§£ä¸º"å…³äºæŸä¸ªçŸ¥è¯†ç‚¹çš„éç»“æ„åŒ–æè¿°"ã€‚
   - **åœºæ™¯**: "ç”¨æˆ·è¯¦ç»†é˜è¿°äº†ä»–å¯¹äººå·¥æ™ºèƒ½æœªæ¥çš„çœ‹æ³•"ï¼ˆä¸€æ®µ 500 å­—çš„è§‚ç‚¹ï¼‰ã€‚è¿™ä¸é€‚åˆå­˜ä¸º KV Factï¼Œä½†å®ƒæ˜¯ä¸€æ®µå…·å¤‡"è¯­ä¹‰ä»·å€¼"çš„è®°å¿†ï¼Œæ¯”å•çº¯çš„"å¯¹è¯åˆ‡ç‰‡ (`episodic`)"æ›´æŠ½è±¡ã€‚
   - **æ¨èç­–ç•¥**: åˆæœŸ **ä¼˜å…ˆä½¿ç”¨ `episodic` å’Œ `summary`**ã€‚ä»…å½“éœ€è¦å­˜å‚¨å¤§æ®µéç»“æ„åŒ–çŸ¥è¯†ï¼ˆå¦‚æ–‡æ¡£ç‰‡æ®µ RAGï¼‰æ—¶ä½¿ç”¨ `semantic` ç±»å‹ã€‚æ­¤æ—¶ `memories` å……å½“äº†è½»é‡çº§çš„ Vector DBã€‚

> [!TIP]
>
> **è®¾è®¡å¿ƒæ³•**:
>
> - **memories** æ˜¯ Agent çš„ **"æ—¥è®°æœ¬"** (å™äº‹)ã€‚
> - **facts** æ˜¯ Agent çš„ **"æ¡£æ¡ˆåº“"** (ç”»åƒ)ã€‚
> - **instructions** æ˜¯ Agent çš„ **"å‘˜å·¥æ‰‹å†Œ"** (è§„åˆ™)ã€‚

### 3.3 æ ¸å¿ƒ Schema å®šä¹‰ (Single Source of Truth)

ä¸ºäº†éµå¾ª **Entropy Reduction (ç†µå‡)** åŸåˆ™ï¼Œé¿å…æ–‡æ¡£ä¸ä»£ç çš„ driftï¼Œæ‰€æœ‰çš„ DDL å’Œ SQL å‡½æ•°å®šä¹‰å·²æ”¶æ•›è‡³ç»Ÿä¸€çš„ Schema æ–‡ä»¶ç»´æŠ¤ã€‚

> [!IMPORTANT]
>
> **Source of Truth**: [src/cognizes/engine/schema/hippocampus_schema.sql](file:///src/cognizes/engine/schema/hippocampus_schema.sql)
>
> è¯¥æ–‡ä»¶åŒ…å«ï¼š
>
> 1. **Tables**: `memories`, `facts`, `consolidation_jobs`, `instructions`
> 2. **Indexes**: PGVector HNSW ç´¢å¼•ä¸ B-Tree è¾…åŠ©ç´¢å¼•
> 3. **Functions**: `calculate_retention_score` (è‰¾å®¾æµ©æ–¯è¡°å‡), `cleanup_low_value_memories` (è‡ªåŠ¨æ¸…ç†), `get_context_window` (ä¸Šä¸‹æ–‡ç»„è£…)

---

## 4. å®æ–½æŒ‡å—

### 4.1 Step 1: è®°å¿† Schema æ‰©å±•éƒ¨ç½²

#### 4.1.1 Schema éƒ¨ç½²è„šæœ¬

åˆ›å»º `src/cognizes/engine/schema/hippocampus_schema.sql`ï¼š

```sql
-- ============================================
-- Agentic AI Engine - Hippocampus Schema Extension
-- Version: 1.0
-- Target: PostgreSQL 16+ with pgvector
-- Prerequisite: Phase 1 agent_schema.sql å·²éƒ¨ç½²
-- ============================================

-- ç¡®ä¿ä¾èµ–çš„æ‰©å±•å·²å¯ç”¨
CREATE EXTENSION IF NOT EXISTS "vector";

-- ============================================
-- 1. memories è¡¨ (æƒ…æ™¯è®°å¿†)
-- ============================================
CREATE TABLE IF NOT EXISTS memories (
    id                  UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    thread_id           UUID REFERENCES threads(id) ON DELETE SET NULL,
    user_id             VARCHAR(255) NOT NULL,
    app_name            VARCHAR(255) NOT NULL,
    memory_type         VARCHAR(50) NOT NULL DEFAULT 'episodic',
    content             TEXT NOT NULL,
    embedding           vector(1536),
    metadata            JSONB DEFAULT '{}',
    retention_score     FLOAT NOT NULL DEFAULT 1.0,
    access_count        INTEGER NOT NULL DEFAULT 0,
    last_accessed_at    TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    created_at          TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_memories_user_app ON memories(user_id, app_name);
CREATE INDEX IF NOT EXISTS idx_memories_thread ON memories(thread_id);
CREATE INDEX IF NOT EXISTS idx_memories_retention ON memories(retention_score DESC);
CREATE INDEX IF NOT EXISTS idx_memories_created_at ON memories(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_memories_embedding
    ON memories USING hnsw (embedding vector_cosine_ops)
    WITH (m = 16, ef_construction = 64);
CREATE INDEX IF NOT EXISTS idx_memories_time_bucket
    ON memories(user_id, app_name, created_at DESC);

-- ============================================
-- 2. facts è¡¨ (è¯­ä¹‰è®°å¿†)
-- ============================================
CREATE TABLE IF NOT EXISTS facts (
    id                  UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    thread_id           UUID REFERENCES threads(id) ON DELETE SET NULL,
    user_id             VARCHAR(255) NOT NULL,
    app_name            VARCHAR(255) NOT NULL,
    fact_type           VARCHAR(50) NOT NULL DEFAULT 'preference',
    key                 VARCHAR(255) NOT NULL,
    value               JSONB NOT NULL,
    embedding           vector(1536),
    confidence          FLOAT NOT NULL DEFAULT 1.0,
    valid_from          TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    valid_until         TIMESTAMP WITH TIME ZONE,
    created_at          TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT facts_user_key_unique UNIQUE (user_id, app_name, fact_type, key)
);

CREATE INDEX IF NOT EXISTS idx_facts_user_app ON facts(user_id, app_name);
CREATE INDEX IF NOT EXISTS idx_facts_type_key ON facts(fact_type, key);
CREATE INDEX IF NOT EXISTS idx_facts_value ON facts USING GIN (value);
CREATE INDEX IF NOT EXISTS idx_facts_validity
    ON facts(user_id, app_name)
    WHERE valid_until IS NULL;

-- ============================================
-- 3. consolidation_jobs è¡¨ (å·©å›ºä»»åŠ¡é˜Ÿåˆ—)
-- ============================================
CREATE TABLE IF NOT EXISTS consolidation_jobs (
    id                  UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    thread_id           UUID NOT NULL REFERENCES threads(id) ON DELETE CASCADE,
    status              VARCHAR(20) NOT NULL DEFAULT 'pending',
    job_type            VARCHAR(50) NOT NULL,
    result              JSONB DEFAULT '{}',
    error               TEXT,
    started_at          TIMESTAMP WITH TIME ZONE,
    completed_at        TIMESTAMP WITH TIME ZONE,
    created_at          TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_consolidation_jobs_status ON consolidation_jobs(status);
CREATE INDEX IF NOT EXISTS idx_consolidation_jobs_thread ON consolidation_jobs(thread_id);
CREATE INDEX IF NOT EXISTS idx_consolidation_jobs_pending
    ON consolidation_jobs(created_at)
    WHERE status = 'pending';

-- ============================================
-- 4. instructions è¡¨ (ç¨‹åºæ€§è®°å¿†)
-- ============================================
CREATE TABLE IF NOT EXISTS instructions (
    id                  UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    app_name            VARCHAR(255) NOT NULL,
    instruction_key     VARCHAR(255) NOT NULL,
    content             TEXT NOT NULL,
    version             INTEGER NOT NULL DEFAULT 1,
    metadata            JSONB DEFAULT '{}',
    created_at          TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    CONSTRAINT instructions_app_key_version_unique UNIQUE (app_name, instruction_key, version)
);

CREATE INDEX IF NOT EXISTS idx_instructions_app ON instructions(app_name);
CREATE INDEX IF NOT EXISTS idx_instructions_key ON instructions(instruction_key);

-- ============================================
-- 5. SQL å‡½æ•°: è‰¾å®¾æµ©æ–¯è¡°å‡è®¡ç®—
-- ============================================
CREATE OR REPLACE FUNCTION calculate_retention_score(
    p_access_count INTEGER,
    p_last_accessed_at TIMESTAMP WITH TIME ZONE,
    p_decay_rate FLOAT DEFAULT 0.1
)
RETURNS FLOAT AS $$
DECLARE
    days_elapsed FLOAT;
    time_decay FLOAT;
    frequency_boost FLOAT;
BEGIN
    days_elapsed := EXTRACT(EPOCH FROM (NOW() - p_last_accessed_at)) / 86400.0;
    time_decay := EXP(-p_decay_rate * days_elapsed);
    frequency_boost := 1.0 + LN(1.0 + p_access_count);
    RETURN LEAST(1.0, time_decay * frequency_boost / 5.0);
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- ============================================
-- 6. SQL å‡½æ•°: æ¸…ç†ä½ä»·å€¼è®°å¿†
-- ============================================
CREATE OR REPLACE FUNCTION cleanup_low_value_memories(
    p_threshold FLOAT DEFAULT 0.1,
    p_min_age_days INTEGER DEFAULT 7
)
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
BEGIN
    UPDATE memories
    SET retention_score = calculate_retention_score(access_count, last_accessed_at);

    DELETE FROM memories
    WHERE retention_score < p_threshold
      AND created_at < NOW() - INTERVAL '1 day' * p_min_age_days;

    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;

-- ============================================
-- 7. pg_cron å®šæ—¶ä»»åŠ¡ (å¯é€‰)
-- ============================================
-- æ¯å¤©å‡Œæ™¨ 2 ç‚¹æ‰§è¡Œè®°å¿†æ¸…ç†
-- SELECT cron.schedule('cleanup_memories', '0 2 * * *', $$SELECT cleanup_low_value_memories(0.1, 7)$$);
```

#### 4.1.2 éƒ¨ç½²éªŒè¯

```bash
# éƒ¨ç½² Hippocampus Schema
psql -d 'cognizes-engine' -f src/cognizes/engine/schema/hippocampus_schema.sql

# éªŒè¯è¡¨åˆ›å»º
psql -d 'cognizes-engine' -c "\dt"
# åº”æ˜¾ç¤º: memories, facts, consolidation_jobs, instructions

# éªŒè¯ç´¢å¼•
psql -d 'cognizes-engine' -c "\di"

# éªŒè¯å‡½æ•°
psql -d 'cognizes-engine' -c "\df calculate_retention_score"
psql -d 'cognizes-engine' -c "\df cleanup_low_value_memories"

# æµ‹è¯•è¡°å‡å‡½æ•°
psql -d 'cognizes-engine' -c "SELECT calculate_retention_score(5, NOW() - INTERVAL '3 days');"
```

#### 4.1.3 pg_cron å®šæ—¶ä»»åŠ¡é…ç½® (P2-2-8)

> [!NOTE]
>
> pg_cron æ˜¯ PostgreSQL çš„å®šæ—¶ä»»åŠ¡æ‰©å±•ï¼Œç”¨äºå®ç°è‡ªåŠ¨è®°å¿†æ¸…ç†å’Œå·©å›ºè§¦å‘ã€‚

**Step 1: å®‰è£… pg_cron**

å‚è§ Phase 1ã€‚

**Step 2: é…ç½® postgresql.conf**

å‚è§ Phase 1ã€‚

**Step 3: é‡å¯ PostgreSQL å¹¶å¯ç”¨æ‰©å±•**

å‚è§ Phase 1ã€‚

**Step 4: é…ç½®å®šæ—¶ä»»åŠ¡**

```sql
-- æŸ¥çœ‹ç°æœ‰ä»»åŠ¡
SELECT * FROM cron.job;

-- æ¯å¤©å‡Œæ™¨ 2 ç‚¹æ‰§è¡Œè®°å¿†æ¸…ç† (P2-3-4)
SELECT cron.schedule(
    'cleanup_memories',
    '0 2 * * *',
    $$SELECT cleanup_low_value_memories(0.1, 7)$$
);

-- æ¯å°æ—¶è§¦å‘ä¸€æ¬¡è®°å¿†å·©å›ºæ£€æŸ¥ (å¯é€‰)
SELECT cron.schedule(
    'trigger_consolidation',
    '0 * * * *',
    $$
    INSERT INTO consolidation_jobs (thread_id, job_type, status)
    SELECT id, 'full_consolidation', 'pending'
    FROM threads
    WHERE updated_at > NOW() - INTERVAL '1 hour'
      AND id NOT IN (
          SELECT thread_id FROM consolidation_jobs
          WHERE created_at > NOW() - INTERVAL '1 hour'
      )
    $$
);

-- åˆ é™¤ä»»åŠ¡
-- SELECT cron.unschedule('cleanup_memories');

-- æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œæ—¥å¿—
SELECT * FROM cron.job_run_details ORDER BY start_time DESC LIMIT 10;
```

**éªŒè¯ pg_cron å®‰è£…**ï¼š

```bash
# æ£€æŸ¥æ‰©å±•æ˜¯å¦å¯ç”¨
psql -d 'cognizes-engine' -c "SELECT * FROM pg_extension WHERE extname = 'pg_cron';"

# æ£€æŸ¥å®šæ—¶ä»»åŠ¡åˆ—è¡¨
psql -d 'cognizes-engine' -c "SELECT jobid, schedule, command FROM cron.job;"
```

### 4.2 Step 2: Memory Consolidation Worker å®ç°

#### 4.2.1 æ ¸å¿ƒæ¶æ„è®¾è®¡

Memory Consolidation Worker é‡‡ç”¨**ä¸¤é˜¶æ®µå·©å›º**ç­–ç•¥ï¼Œæ¨¡æ‹Ÿäººç±»å¤§è„‘çš„è®°å¿†å·©å›ºè¿‡ç¨‹ï¼š

```mermaid
graph TB
    subgraph "é˜¶æ®µ 1: Fast Replay (å¿«å›æ”¾)"
        E[Events äº‹ä»¶æµ] --> ER[extract_recent_events]
        ER --> GS[generate_summary]
        GS --> SS[store_summary]
    end

    subgraph "é˜¶æ®µ 2: Deep Reflection (æ·±åæ€)"
        E --> EF[extract_facts]
        EF --> VI[vectorize_insights]
        VI --> SM[store_to_memories]
    end

    SS --> M[(memories è¡¨)]
    SM --> F[(facts è¡¨)]

    style E fill:#065f46,stroke:#34d399,color:#fff
    style M fill:#1e3a5f,stroke:#60a5fa,color:#fff
    style F fill:#7c2d12,stroke:#fb923c,color:#fff
```

#### 4.2.2 Consolidation Worker å®Œæ•´å®ç°

åˆ›å»º `src/cognizes/engine/hippocampus/consolidation_worker.py`ï¼š

````python
"""
MemoryConsolidationWorker: è®°å¿†å·©å›º Worker

å®ç°å¯¹æ ‡ Google ADK MemoryBankService çš„è®°å¿†å·©å›ºèƒ½åŠ›ï¼š
- Fast Replay: å¿«é€Ÿæ‘˜è¦æœ€è¿‘å¯¹è¯
- Deep Reflection: æ·±åº¦æå– Facts å’Œ Insights
- Vectorization: å‘é‡åŒ–å¹¶å­˜å…¥ memories/facts è¡¨

å‚è€ƒ:
- Google ADK MemoryService: https://google.github.io/adk-docs/sessions/memory/
- LangGraph Memory: https://docs.langchain.com/oss/python/langgraph/memory
"""

from __future__ import annotations

import asyncio
import json
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

import asyncpg
import google.generativeai as genai

# ========================================
# æ•°æ®ç±»å‹å®šä¹‰
# ========================================

class JobType(str, Enum):
    FAST_REPLAY = "fast_replay"
    DEEP_REFLECTION = "deep_reflection"
    FULL_CONSOLIDATION = "full_consolidation"


class JobStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


@dataclass
class ConsolidationJob:
    """è®°å¿†å·©å›ºä»»åŠ¡"""
    id: str
    thread_id: str
    job_type: JobType
    status: JobStatus = JobStatus.PENDING
    result: dict[str, Any] = field(default_factory=dict)
    error: str | None = None
    started_at: datetime | None = None
    completed_at: datetime | None = None
    created_at: datetime | None = None


@dataclass
class Memory:
    """è®°å¿†å¯¹è±¡"""
    id: str
    thread_id: str | None
    user_id: str
    app_name: str
    memory_type: str  # 'episodic', 'semantic', 'summary'
    content: str
    embedding: list[float] | None = None
    metadata: dict[str, Any] = field(default_factory=dict)
    retention_score: float = 1.0
    access_count: int = 0


@dataclass
class Fact:
    """äº‹å®å¯¹è±¡"""
    id: str
    thread_id: str | None
    user_id: str
    app_name: str
    fact_type: str  # 'preference', 'rule', 'profile'
    key: str
    value: dict[str, Any]
    confidence: float = 1.0


# ========================================
# Prompt æ¨¡æ¿
# ========================================

FAST_REPLAY_PROMPT = """ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸ºä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚

å¯¹è¯å†å²:
{conversation}

è¦æ±‚:
1. æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡ 200 å­—
2. ä¿ç•™ç”¨æˆ·çš„å…³é”®é—®é¢˜å’Œ Agent çš„æ ¸å¿ƒå›ç­”
3. ä¿ç•™ä»»ä½•é‡è¦çš„å†³ç­–æˆ–ç»“è®º
4. ä½¿ç”¨ç¬¬ä¸‰äººç§°æè¿°

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€æˆ–è§£é‡Šã€‚"""

DEEP_REFLECTION_PROMPT = """ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·ç”»åƒåˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–ç”¨æˆ·çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬åå¥½ã€è§„åˆ™å’Œäº‹å®ã€‚

å¯¹è¯å†å²:
{conversation}

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹:
```json
{{
    "facts": [
        {{
            "type": "preference|rule|profile",
            "key": "åå¥½/è§„åˆ™çš„å”¯ä¸€æ ‡è¯†ï¼Œå¦‚ food_preference",
            "value": {{"å…·ä½“çš„åå¥½å†…å®¹"}},
            "confidence": 0.0-1.0 çš„ç½®ä¿¡åº¦åˆ†æ•°
        }}
    ],
    "insights": [
        {{
            "content": "ä»å¯¹è¯ä¸­æç‚¼çš„æ·±å±‚æ´å¯Ÿ",
            "importance": "high|medium|low"
        }}
    ]
}}
```

è¦æ±‚:

1. åªæå–æ˜ç¡®è¡¨è¾¾æˆ–å¯é æ¨æ–­çš„ä¿¡æ¯
2. preference: ç”¨æˆ·çš„å–œå¥½ï¼ˆå¦‚é¥®é£Ÿã€é£æ ¼åå¥½ï¼‰
3. rule: ç”¨æˆ·è®¾å®šçš„è§„åˆ™ï¼ˆå¦‚"æ¯å‘¨äº”ä¸å¼€ä¼š"ï¼‰
4. profile: ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ï¼ˆå¦‚èŒä¸šã€ä½ç½®ï¼‰
5. å¦‚æœæ²¡æœ‰å¯æå–çš„ä¿¡æ¯ï¼Œè¿”å›ç©ºæ•°ç»„

è¯·åªè¾“å‡º JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

# ========================================

# Memory Consolidation Worker

# ========================================

class MemoryConsolidationWorker:
    """
    è®°å¿†å·©å›º Worker

    è´Ÿè´£å°† Session ä¸­çš„å¯¹è¯è½¬åŒ–ä¸ºæŒä¹…åŒ–çš„è®°å¿†ï¼š
    1. Fast Replay: ç”Ÿæˆå¯¹è¯æ‘˜è¦
    2. Deep Reflection: æå– Facts å’Œ Insights
    3. Vectorization: å‘é‡åŒ–å¹¶å†™å…¥æ•°æ®åº“
    """

    def __init__(
        self,
        pool: asyncpg.Pool,
        model_name: str = "gemini-2.0-flash",
        embedding_model: str = "text-embedding-004",
    ):
        self.pool = pool
        self.model_name = model_name
        self.embedding_model = embedding_model
        self.model = genai.GenerativeModel(model_name)

    # ========================================
    # ä¸»å…¥å£å‡½æ•°
    # ========================================

    async def consolidate(
        self,
        thread_id: str,
        job_type: JobType = JobType.FULL_CONSOLIDATION,
    ) -> ConsolidationJob:
        """
        æ‰§è¡Œè®°å¿†å·©å›ºä»»åŠ¡

        Args:
            thread_id: è¦å·©å›ºçš„ä¼šè¯ ID
            job_type: ä»»åŠ¡ç±»å‹
                - FAST_REPLAY: ä»…ç”Ÿæˆæ‘˜è¦
                - DEEP_REFLECTION: ä»…æå– Facts
                - FULL_CONSOLIDATION: ä¸¤è€…éƒ½æ‰§è¡Œ

        Returns:
            ConsolidationJob: ä»»åŠ¡æ‰§è¡Œç»“æœ
        """
        # åˆ›å»ºä»»åŠ¡è®°å½•
        job = await self._create_job(thread_id, job_type)

        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            await self._update_job_status(job.id, JobStatus.RUNNING)
            job.started_at = datetime.now()

            # è·å–ä¼šè¯ä¿¡æ¯
            thread_info = await self._get_thread_info(thread_id)
            if not thread_info:
                raise ValueError(f"Thread {thread_id} not found")

            user_id = thread_info["user_id"]
            app_name = thread_info["app_name"]

            # æå–æœ€è¿‘äº‹ä»¶
            events = await self._extract_recent_events(thread_id)
            if not events:
                job.result = {"message": "No events to consolidate"}
                await self._update_job_status(job.id, JobStatus.COMPLETED, job.result)
                return job

            # æ„å»ºå¯¹è¯æ–‡æœ¬
            conversation = self._format_conversation(events)

            result = {}

            # é˜¶æ®µ 1: Fast Replay (å¿«å›æ”¾)
            if job_type in [JobType.FAST_REPLAY, JobType.FULL_CONSOLIDATION]:
                summary = await self._generate_summary(conversation)
                memory = await self._store_summary(
                    thread_id=thread_id,
                    user_id=user_id,
                    app_name=app_name,
                    content=summary,
                )
                result["summary"] = {
                    "memory_id": memory.id,
                    "content": summary[:100] + "..." if len(summary) > 100 else summary,
                }

            # é˜¶æ®µ 2: Deep Reflection (æ·±åæ€)
            if job_type in [JobType.DEEP_REFLECTION, JobType.FULL_CONSOLIDATION]:
                extraction = await self._extract_facts(conversation)
                facts_stored = []
                insights_stored = []

                # å­˜å‚¨ Facts
                for fact_data in extraction.get("facts", []):
                    fact = await self._store_fact(
                        thread_id=thread_id,
                        user_id=user_id,
                        app_name=app_name,
                        fact_data=fact_data,
                    )
                    facts_stored.append({
                        "fact_id": fact.id,
                        "key": fact.key,
                    })

                # å­˜å‚¨ Insights ä½œä¸ºè¯­ä¹‰è®°å¿†
                for insight_data in extraction.get("insights", []):
                    memory = await self._store_insight(
                        thread_id=thread_id,
                        user_id=user_id,
                        app_name=app_name,
                        insight_data=insight_data,
                    )
                    insights_stored.append({
                        "memory_id": memory.id,
                        "importance": insight_data.get("importance", "medium"),
                    })

                result["facts"] = facts_stored
                result["insights"] = insights_stored

            # ä»»åŠ¡å®Œæˆ
            job.result = result
            job.completed_at = datetime.now()
            await self._update_job_status(job.id, JobStatus.COMPLETED, result)

            return job

        except Exception as e:
            # ä»»åŠ¡å¤±è´¥
            job.error = str(e)
            job.status = JobStatus.FAILED
            await self._update_job_status(job.id, JobStatus.FAILED, error=str(e))
            raise

    # ========================================
    # é˜¶æ®µ 1: Fast Replay (å¿«å›æ”¾)
    # ========================================

    async def _extract_recent_events(
        self,
        thread_id: str,
        limit: int = 50,
    ) -> list[dict[str, Any]]:
        """æå–æœ€è¿‘çš„äº‹ä»¶"""
        query = """
            SELECT id, author, event_type, content, created_at
            FROM events
            WHERE thread_id = $1
            ORDER BY sequence_num DESC
            LIMIT $2
        """
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query, uuid.UUID(thread_id), limit)
            # åè½¬é¡ºåºä½¿å…¶æŒ‰æ—¶é—´æ­£åº
            return [dict(row) for row in reversed(rows)]

    def _format_conversation(self, events: list[dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        lines = []
        for event in events:
            author = event["author"]
            content = event.get("content", {})

            # æå–æ¶ˆæ¯æ–‡æœ¬
            if isinstance(content, dict):
                text = content.get("text", content.get("message", str(content)))
            else:
                text = str(content)

            role_label = {
                "user": "ç”¨æˆ·",
                "agent": "åŠ©æ‰‹",
                "tool": "å·¥å…·",
            }.get(author, author)

            lines.append(f"{role_label}: {text}")

        return "\n".join(lines)

    async def _generate_summary(self, conversation: str) -> str:
        """ç”Ÿæˆå¯¹è¯æ‘˜è¦ (Fast Replay)"""
        prompt = FAST_REPLAY_PROMPT.format(conversation=conversation)
        response = await asyncio.to_thread(
            self.model.generate_content, prompt
        )
        return response.text.strip()

    async def _store_summary(
        self,
        thread_id: str,
        user_id: str,
        app_name: str,
        content: str,
    ) -> Memory:
        """å­˜å‚¨æ‘˜è¦ä½œä¸ºè®°å¿†"""
        # ç”Ÿæˆå‘é‡åµŒå…¥
        embedding = await self._generate_embedding(content)

        memory_id = str(uuid.uuid4())

        query = """
            INSERT INTO memories (id, thread_id, user_id, app_name, memory_type, content, embedding, metadata)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
            RETURNING id, created_at
        """
        async with self.pool.acquire() as conn:
            await conn.execute(
                query,
                uuid.UUID(memory_id),
                uuid.UUID(thread_id),
                user_id,
                app_name,
                "summary",
                content,
                embedding,
                json.dumps({"source": "fast_replay"}),
            )

        return Memory(
            id=memory_id,
            thread_id=thread_id,
            user_id=user_id,
            app_name=app_name,
            memory_type="summary",
            content=content,
            embedding=embedding,
        )

    # ========================================
    # é˜¶æ®µ 2: Deep Reflection (æ·±åæ€)
    # ========================================

    async def _extract_facts(self, conversation: str) -> dict[str, Any]:
        """ä»å¯¹è¯ä¸­æå– Facts å’Œ Insights"""
        prompt = DEEP_REFLECTION_PROMPT.format(conversation=conversation)
        response = await asyncio.to_thread(
            self.model.generate_content, prompt
        )

        # è§£æ JSON å“åº”
        text = response.text.strip()
        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        if text.startswith("```json"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]

        try:
            return json.loads(text.strip())
        except json.JSONDecodeError:
            return {"facts": [], "insights": []}

    async def _store_fact(
        self,
        thread_id: str,
        user_id: str,
        app_name: str,
        fact_data: dict[str, Any],
    ) -> Fact:
        """å­˜å‚¨æå–çš„äº‹å® (Upsert é€»è¾‘)"""
        fact_id = str(uuid.uuid4())
        fact_type = fact_data.get("type", "preference")
        key = fact_data.get("key", "unknown")
        value = fact_data.get("value", {})
        confidence = fact_data.get("confidence", 1.0)

        # ç”Ÿæˆå‘é‡åµŒå…¥ (ç”¨äºè¯­ä¹‰æ£€ç´¢)
        content_for_embedding = f"{key}: {json.dumps(value)}"
        embedding = await self._generate_embedding(content_for_embedding)

        # Upsert: å¦‚æœå·²å­˜åœ¨ç›¸åŒ key åˆ™æ›´æ–°
        query = """
            INSERT INTO facts (id, thread_id, user_id, app_name, fact_type, key, value, embedding, confidence)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
            ON CONFLICT (user_id, app_name, fact_type, key)
            DO UPDATE SET
                value = EXCLUDED.value,
                embedding = EXCLUDED.embedding,
                confidence = EXCLUDED.confidence,
                thread_id = EXCLUDED.thread_id
            RETURNING id
        """
        async with self.pool.acquire() as conn:
            result = await conn.fetchrow(
                query,
                uuid.UUID(fact_id),
                uuid.UUID(thread_id),
                user_id,
                app_name,
                fact_type,
                key,
                json.dumps(value),
                embedding,
                confidence,
            )
            actual_id = str(result["id"])

        return Fact(
            id=actual_id,
            thread_id=thread_id,
            user_id=user_id,
            app_name=app_name,
            fact_type=fact_type,
            key=key,
            value=value,
            confidence=confidence,
        )

    async def _store_insight(
        self,
        thread_id: str,
        user_id: str,
        app_name: str,
        insight_data: dict[str, Any],
    ) -> Memory:
        """å­˜å‚¨ Insight ä½œä¸ºè¯­ä¹‰è®°å¿†"""
        content = insight_data.get("content", "")
        importance = insight_data.get("importance", "medium")

        # ç”Ÿæˆå‘é‡åµŒå…¥
        embedding = await self._generate_embedding(content)

        # æ ¹æ®é‡è¦æ€§è®¾ç½®åˆå§‹ä¿ç•™åˆ†æ•°
        retention_score = {
            "high": 1.0,
            "medium": 0.7,
            "low": 0.4,
        }.get(importance, 0.7)

        memory_id = str(uuid.uuid4())

        query = """
            INSERT INTO memories (id, thread_id, user_id, app_name, memory_type, content, embedding, metadata, retention_score)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
            RETURNING id
        """
        async with self.pool.acquire() as conn:
            await conn.execute(
                query,
                uuid.UUID(memory_id),
                uuid.UUID(thread_id),
                user_id,
                app_name,
                "semantic",
                content,
                embedding,
                json.dumps({"source": "deep_reflection", "importance": importance}),
                retention_score,
            )

        return Memory(
            id=memory_id,
            thread_id=thread_id,
            user_id=user_id,
            app_name=app_name,
            memory_type="semantic",
            content=content,
            embedding=embedding,
            retention_score=retention_score,
        )

    # ========================================
    # å‘é‡åŒ–
    # ========================================

    async def _generate_embedding(self, text: str) -> list[float]:
        """ç”Ÿæˆæ–‡æœ¬çš„å‘é‡åµŒå…¥"""
        result = await asyncio.to_thread(
            genai.embed_content,
            model=f"models/{self.embedding_model}",
            content=text,
            task_type="retrieval_document",
        )
        return result["embedding"]

    # ========================================
    # ä»»åŠ¡ç®¡ç†
    # ========================================

    async def _create_job(self, thread_id: str, job_type: JobType) -> ConsolidationJob:
        """åˆ›å»ºå·©å›ºä»»åŠ¡"""
        job_id = str(uuid.uuid4())
        query = """
            INSERT INTO consolidation_jobs (id, thread_id, job_type, status, created_at)
            VALUES ($1, $2, $3, $4, NOW())
            RETURNING created_at
        """
        async with self.pool.acquire() as conn:
            result = await conn.fetchrow(
                query,
                uuid.UUID(job_id),
                uuid.UUID(thread_id),
                job_type.value,
                JobStatus.PENDING.value,
            )

        return ConsolidationJob(
            id=job_id,
            thread_id=thread_id,
            job_type=job_type,
            status=JobStatus.PENDING,
            created_at=result["created_at"],
        )

    async def _update_job_status(
        self,
        job_id: str,
        status: JobStatus,
        result: dict | None = None,
        error: str | None = None,
    ) -> None:
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        query = """
            UPDATE consolidation_jobs
            SET status = $2,
                result = COALESCE($3, result),
                error = COALESCE($4, error),
                started_at = CASE WHEN $2 = 'running' THEN NOW() ELSE started_at END,
                completed_at = CASE WHEN $2 IN ('completed', 'failed') THEN NOW() ELSE completed_at END
            WHERE id = $1
        """
        async with self.pool.acquire() as conn:
            await conn.execute(
                query,
                uuid.UUID(job_id),
                status.value,
                json.dumps(result) if result else None,
                error,
            )

    async def _get_thread_info(self, thread_id: str) -> dict[str, Any] | None:
        """è·å–ä¼šè¯ä¿¡æ¯"""
        query = """
            SELECT id, user_id, app_name, state, version
            FROM threads
            WHERE id = $1
        """
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(query, uuid.UUID(thread_id))
            return dict(row) if row else None

# ========================================

# ä¾¿æ·å‡½æ•°

# ========================================

async def consolidate_thread(
    pool: asyncpg.Pool,
    thread_id: str,
    job_type: JobType = JobType.FULL_CONSOLIDATION,
) -> ConsolidationJob:
    """ä¾¿æ·å‡½æ•°ï¼šå·©å›ºæŒ‡å®šä¼šè¯çš„è®°å¿†"""
    worker = MemoryConsolidationWorker(pool)
    return await worker.consolidate(thread_id, job_type)
````

#### 4.2.3 ä½¿ç”¨ç¤ºä¾‹

```python
# ä½¿ç”¨ç¤ºä¾‹
import asyncio
import asyncpg

async def main():
    # åˆ›å»ºæ•°æ®åº“è¿æ¥æ± 
    pool = await asyncpg.create_pool(
        "postgresql://aigc:@localhost/cognizes-engine"
    )

    # åˆ›å»º Worker
    worker = MemoryConsolidationWorker(pool)

    # æ‰§è¡Œå®Œæ•´å·©å›º
    job = await worker.consolidate(
        thread_id="your-thread-id",
        job_type=JobType.FULL_CONSOLIDATION
    )

    print(f"Job completed: {job.result}")

    await pool.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### 4.3 Step 3: Biological Retention å®ç°

#### 4.3.1 è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿åŸç†

è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿æè¿°äº†è®°å¿†éšæ—¶é—´è¡°å‡çš„è§„å¾‹ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äº Agent è®°å¿†ç³»ç»Ÿï¼š

```mermaid
graph LR
    subgraph "é—å¿˜æ›²çº¿æ¨¡å‹"
        T[Time æ—¶é—´] --> D[Decay è¡°å‡]
        F[Frequency è®¿é—®é¢‘ç‡] --> B[Boost åŠ æˆ]
        D & B --> R[Retention Score ä¿ç•™åˆ†æ•°]
    end

    R --> C{Score é˜ˆå€¼}
    C -->|>= 0.1| K[ä¿ç•™]
    C -->|< 0.1| DEL[æ¸…ç†]

    style R fill:#7c2d12,stroke:#fb923c,color:#fff
```

**å…¬å¼**ï¼š

$$
\text{retention\_score} = \min(1.0, \frac{\text{time\_decay} \times \text{frequency\_boost}}{5.0})
$$

å…¶ä¸­ï¼š

- $\text{time\_decay} = e^{-\lambda \times \text{days\_elapsed}}$ (æŒ‡æ•°è¡°å‡)
- $\text{frequency\_boost} = 1 + \ln(1 + \text{access\_count})$ (å¯¹æ•°åŠ æˆ)
- $\lambda = 0.1$ (é»˜è®¤è¡°å‡ç³»æ•°)

#### 4.3.2 Memory Retention Manager å®ç°

åˆ›å»º `src/cognizes/engine/hippocampus/retention_manager.py`ï¼š

```python
"""
MemoryRetentionManager: è®°å¿†ä¿æŒç®¡ç†å™¨

å®ç°è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿ç®—æ³•ï¼Œè‡ªåŠ¨ç®¡ç†è®°å¿†çš„ä¿æŒä¸æ¸…ç†ï¼š
- è®¡ç®—è®°å¿†ä¿ç•™åˆ†æ•°
- å®šæœŸæ¸…ç†ä½ä»·å€¼è®°å¿†
- è®°å½•è®¿é—®å†å²ï¼Œæå‡é«˜é¢‘è®°å¿†çš„ä¿ç•™åˆ†æ•°
"""

from __future__ import annotations

import asyncio
import uuid
from dataclasses import dataclass
from datetime import datetime
from typing import Any

import asyncpg


@dataclass
class MemoryStats:
    """è®°å¿†ç»Ÿè®¡ä¿¡æ¯"""
    total_memories: int
    high_value_count: int  # retention_score >= 0.7
    medium_value_count: int  # 0.3 <= retention_score < 0.7
    low_value_count: int  # retention_score < 0.3
    avg_retention_score: float
    cleaned_count: int


class MemoryRetentionManager:
    """
    è®°å¿†ä¿æŒç®¡ç†å™¨

    èŒè´£:
    1. è®¡ç®—å’Œæ›´æ–°è®°å¿†çš„ä¿ç•™åˆ†æ•°
    2. æ¸…ç†ä½ä»·å€¼è®°å¿†
    3. è®°å½•è®¿é—®ï¼Œæå‡é«˜é¢‘è®°å¿†çš„æƒé‡
    """

    def __init__(
        self,
        pool: asyncpg.Pool,
        decay_rate: float = 0.1,
        cleanup_threshold: float = 0.1,
        min_age_days: int = 7,
    ):
        """
        Args:
            pool: æ•°æ®åº“è¿æ¥æ± 
            decay_rate: è¡°å‡ç³»æ•° Î» (é»˜è®¤ 0.1)
            cleanup_threshold: æ¸…ç†é˜ˆå€¼ (é»˜è®¤ 0.1)
            min_age_days: æœ€å°ä¿ç•™å¤©æ•° (é»˜è®¤ 7 å¤©)
        """
        self.pool = pool
        self.decay_rate = decay_rate
        self.cleanup_threshold = cleanup_threshold
        self.min_age_days = min_age_days

    # ========================================
    # è®¿é—®è®°å½•
    # ========================================

    async def record_access(self, memory_id: str) -> None:
        """
        è®°å½•è®°å¿†è¢«è®¿é—®ï¼Œå¢åŠ  access_count å¹¶æ›´æ–° last_accessed_at

        Args:
            memory_id: è®°å¿† ID
        """
        query = """
            UPDATE memories
            SET access_count = access_count + 1,
                last_accessed_at = NOW(),
                retention_score = calculate_retention_score(access_count + 1, NOW(), $2)
            WHERE id = $1
        """
        async with self.pool.acquire() as conn:
            await conn.execute(query, uuid.UUID(memory_id), self.decay_rate)

    async def record_batch_access(self, memory_ids: list[str]) -> None:
        """æ‰¹é‡è®°å½•è®¿é—®"""
        query = """
            UPDATE memories
            SET access_count = access_count + 1,
                last_accessed_at = NOW(),
                retention_score = calculate_retention_score(access_count + 1, NOW(), $2)
            WHERE id = ANY($1::uuid[])
        """
        async with self.pool.acquire() as conn:
            uuid_list = [uuid.UUID(mid) for mid in memory_ids]
            await conn.execute(query, uuid_list, self.decay_rate)

    # ========================================
    # ä¿ç•™åˆ†æ•°è®¡ç®—
    # ========================================

    async def update_all_retention_scores(self) -> int:
        """
        æ›´æ–°æ‰€æœ‰è®°å¿†çš„ä¿ç•™åˆ†æ•°

        Returns:
            æ›´æ–°çš„è®°å¿†æ•°é‡
        """
        query = """
            UPDATE memories
            SET retention_score = calculate_retention_score(access_count, last_accessed_at, $1)
        """
        async with self.pool.acquire() as conn:
            result = await conn.execute(query, self.decay_rate)
            # è§£æ UPDATE è¿”å›çš„è¡Œæ•°
            return int(result.split()[-1])

    async def get_retention_distribution(
        self,
        user_id: str | None = None,
        app_name: str | None = None,
    ) -> dict[str, int]:
        """
        è·å–è®°å¿†ä¿ç•™åˆ†æ•°åˆ†å¸ƒ

        Returns:
            {"high": count, "medium": count, "low": count}
        """
        conditions = ["1=1"]
        params = []
        param_idx = 1

        if user_id:
            conditions.append(f"user_id = ${param_idx}")
            params.append(user_id)
            param_idx += 1

        if app_name:
            conditions.append(f"app_name = ${param_idx}")
            params.append(app_name)
            param_idx += 1

        where_clause = " AND ".join(conditions)

        query = f"""
            SELECT
                COUNT(*) FILTER (WHERE retention_score >= 0.7) AS high,
                COUNT(*) FILTER (WHERE retention_score >= 0.3 AND retention_score < 0.7) AS medium,
                COUNT(*) FILTER (WHERE retention_score < 0.3) AS low
            FROM memories
            WHERE {where_clause}
        """
        async with self.pool.acquire() as conn:
            row = await conn.fetchrow(query, *params)
            return {
                "high": row["high"],
                "medium": row["medium"],
                "low": row["low"],
            }

    # ========================================
    # è®°å¿†æ¸…ç†
    # ========================================

    async def cleanup_low_value_memories(
        self,
        threshold: float | None = None,
        min_age_days: int | None = None,
        dry_run: bool = False,
    ) -> MemoryStats:
        """
        æ¸…ç†ä½ä»·å€¼è®°å¿†

        Args:
            threshold: ä¿ç•™åˆ†æ•°é˜ˆå€¼ (ä½äºæ­¤åˆ†æ•°çš„è®°å¿†å°†è¢«æ¸…ç†)
            min_age_days: æœ€å°ä¿ç•™å¤©æ•° (åˆ›å»ºæ—¶é—´æ—©äºæ­¤å¤©æ•°çš„è®°å¿†æ‰ä¼šè¢«æ¸…ç†)
            dry_run: å¦‚æœä¸º Trueï¼Œåªè¿”å›ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸å®é™…åˆ é™¤

        Returns:
            MemoryStats: æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
        """
        threshold = threshold or self.cleanup_threshold
        min_age_days = min_age_days or self.min_age_days

        # å…ˆæ›´æ–°æ‰€æœ‰åˆ†æ•°
        await self.update_all_retention_scores()

        # è·å–æ¸…ç†å‰ç»Ÿè®¡
        distribution = await self.get_retention_distribution()

        if dry_run:
            # åªç»Ÿè®¡å°†è¢«æ¸…ç†çš„æ•°é‡
            query = """
                SELECT COUNT(*) FROM memories
                WHERE retention_score < $1
                  AND created_at < NOW() - INTERVAL '1 day' * $2
            """
            async with self.pool.acquire() as conn:
                count = await conn.fetchval(query, threshold, min_age_days)

            return MemoryStats(
                total_memories=sum(distribution.values()),
                high_value_count=distribution["high"],
                medium_value_count=distribution["medium"],
                low_value_count=distribution["low"],
                avg_retention_score=0,  # éœ€è¦é¢å¤–è®¡ç®—
                cleaned_count=count,
            )

        # å®é™…æ¸…ç†
        query = """
            DELETE FROM memories
            WHERE retention_score < $1
              AND created_at < NOW() - INTERVAL '1 day' * $2
        """
        async with self.pool.acquire() as conn:
            result = await conn.execute(query, threshold, min_age_days)
            cleaned_count = int(result.split()[-1])

        # è·å–æ¸…ç†åç»Ÿè®¡
        distribution_after = await self.get_retention_distribution()

        # è®¡ç®—å¹³å‡ä¿ç•™åˆ†æ•°
        avg_query = "SELECT AVG(retention_score) FROM memories"
        async with self.pool.acquire() as conn:
            avg_score = await conn.fetchval(avg_query) or 0

        return MemoryStats(
            total_memories=sum(distribution_after.values()),
            high_value_count=distribution_after["high"],
            medium_value_count=distribution_after["medium"],
            low_value_count=distribution_after["low"],
            avg_retention_score=float(avg_score),
            cleaned_count=cleaned_count,
        )

    # ========================================
    # æƒ…æ™¯åˆ†å—æ£€ç´¢
    # ========================================

    async def get_episodic_memories_by_time_slice(
        self,
        user_id: str,
        app_name: str,
        start_time: datetime,
        end_time: datetime,
        limit: int = 50,
    ) -> list[dict[str, Any]]:
        """
        æŒ‰æ—¶é—´åˆ‡ç‰‡æ£€ç´¢æƒ…æ™¯è®°å¿†

        Args:
            user_id: ç”¨æˆ· ID
            app_name: åº”ç”¨åç§°
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            limit: æœ€å¤§è¿”å›æ•°é‡

        Returns:
            è®°å¿†åˆ—è¡¨
        """
        query = """
            SELECT id, content, memory_type, metadata, retention_score, created_at
            FROM memories
            WHERE user_id = $1
              AND app_name = $2
              AND created_at >= $3
              AND created_at <= $4
            ORDER BY created_at DESC
            LIMIT $5
        """
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                query, user_id, app_name, start_time, end_time, limit
            )
            return [dict(row) for row in rows]


# ========================================
# å®šæ—¶æ¸…ç†ä»»åŠ¡
# ========================================

async def scheduled_cleanup_task(
    pool: asyncpg.Pool,
    interval_hours: int = 24,
    decay_rate: float = 0.1,
    cleanup_threshold: float = 0.1,
    min_age_days: int = 7,
) -> None:
    """
    åå°å®šæ—¶æ¸…ç†ä»»åŠ¡

    Args:
        pool: æ•°æ®åº“è¿æ¥æ± 
        interval_hours: æ¸…ç†é—´éš” (å°æ—¶)
    """
    manager = MemoryRetentionManager(
        pool=pool,
        decay_rate=decay_rate,
        cleanup_threshold=cleanup_threshold,
        min_age_days=min_age_days,
    )

    while True:
        try:
            stats = await manager.cleanup_low_value_memories()
            print(
                f"Memory cleanup completed: "
                f"cleaned={stats.cleaned_count}, "
                f"remaining={stats.total_memories}, "
                f"avg_score={stats.avg_retention_score:.2f}"
            )
        except Exception as e:
            print(f"Memory cleanup failed: {e}")

        await asyncio.sleep(interval_hours * 3600)
```

#### 4.3.3 Context Window ç»„è£…å™¨å®ç°

åˆ›å»º `src/cognizes/engine/hippocampus/context_assembler.py`ï¼š

```python
"""
ContextAssembler: ä¸Šä¸‹æ–‡ç»„è£…å™¨

è´Ÿè´£æ ¹æ® Token é¢„ç®—åŠ¨æ€ç»„è£…ä¸Šä¸‹æ–‡çª—å£ï¼š
- System Prompt
- Top-K Memories (æŒ‰ç›¸å…³æ€§)
- Recent History (æœ€è¿‘å¯¹è¯)
- Facts (ç”¨æˆ·åå¥½)
"""

from __future__ import annotations

import uuid
from dataclasses import dataclass, field
from typing import Any

import asyncpg


@dataclass
class ContextItem:
    """ä¸Šä¸‹æ–‡é¡¹"""
    context_type: str  # 'system', 'memory', 'history', 'fact'
    content: str
    relevance_score: float = 1.0
    token_estimate: int = 0
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class ContextWindow:
    """ç»„è£…åçš„ä¸Šä¸‹æ–‡çª—å£"""
    items: list[ContextItem]
    total_tokens: int
    budget_used: float  # ä½¿ç”¨çš„é¢„ç®—æ¯”ä¾‹


class ContextAssembler:
    """
    ä¸Šä¸‹æ–‡ç»„è£…å™¨

    èŒè´£:
    1. æ ¹æ® Token é¢„ç®—åˆ†é…å„éƒ¨åˆ†ä¸Šä¸‹æ–‡
    2. æŒ‰ç›¸å…³æ€§å’Œé‡è¦æ€§æ’åº
    3. åŠ¨æ€æˆªæ–­ä»¥é€‚åº”é¢„ç®—
    """

    def __init__(
        self,
        pool: asyncpg.Pool,
        max_tokens: int = 8000,
        system_ratio: float = 0.1,    # System Prompt å æ¯”
        memory_ratio: float = 0.3,    # è®°å¿†å æ¯”
        history_ratio: float = 0.4,   # å†å²å æ¯”
        fact_ratio: float = 0.2,      # äº‹å®å æ¯”
    ):
        self.pool = pool
        self.max_tokens = max_tokens
        self.system_ratio = system_ratio
        self.memory_ratio = memory_ratio
        self.history_ratio = history_ratio
        self.fact_ratio = fact_ratio

    async def assemble(
        self,
        user_id: str,
        app_name: str,
        thread_id: str,
        query: str,
        query_embedding: list[float],
        system_prompt: str | None = None,
    ) -> ContextWindow:
        """
        ç»„è£…ä¸Šä¸‹æ–‡çª—å£

        Args:
            user_id: ç”¨æˆ· ID
            app_name: åº”ç”¨åç§°
            thread_id: å½“å‰ä¼šè¯ ID
            query: ç”¨æˆ·æŸ¥è¯¢
            query_embedding: æŸ¥è¯¢çš„å‘é‡åµŒå…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯

        Returns:
            ContextWindow: ç»„è£…åçš„ä¸Šä¸‹æ–‡
        """
        items: list[ContextItem] = []
        total_tokens = 0

        # 1. æ·»åŠ  System Prompt
        if system_prompt:
            system_tokens = self._estimate_tokens(system_prompt)
            system_budget = int(self.max_tokens * self.system_ratio)

            if system_tokens <= system_budget:
                items.append(ContextItem(
                    context_type="system",
                    content=system_prompt,
                    relevance_score=1.0,
                    token_estimate=system_tokens,
                ))
                total_tokens += system_tokens

        # 2. æ£€ç´¢ç›¸å…³è®°å¿†
        memory_budget = int(self.max_tokens * self.memory_ratio)
        memories = await self._retrieve_memories(
            user_id, app_name, query_embedding, memory_budget
        )
        for mem in memories:
            if total_tokens + mem.token_estimate <= self.max_tokens:
                items.append(mem)
                total_tokens += mem.token_estimate

        # 3. è·å–æœ€è¿‘å†å²
        history_budget = int(self.max_tokens * self.history_ratio)
        history = await self._retrieve_history(
            thread_id, history_budget
        )
        for hist in history:
            if total_tokens + hist.token_estimate <= self.max_tokens:
                items.append(hist)
                total_tokens += hist.token_estimate

        # 4. è·å–ç”¨æˆ· Facts
        fact_budget = int(self.max_tokens * self.fact_ratio)
        facts = await self._retrieve_facts(
            user_id, app_name, query_embedding, fact_budget
        )
        for fact in facts:
            if total_tokens + fact.token_estimate <= self.max_tokens:
                items.append(fact)
                total_tokens += fact.token_estimate

        return ContextWindow(
            items=items,
            total_tokens=total_tokens,
            budget_used=total_tokens / self.max_tokens,
        )

    async def _retrieve_memories(
        self,
        user_id: str,
        app_name: str,
        query_embedding: list[float],
        budget: int,
    ) -> list[ContextItem]:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        query = """
            SELECT
                id, content, retention_score,
                1 - (embedding <=> $3::vector) AS similarity
            FROM memories
            WHERE user_id = $1
              AND app_name = $2
              AND embedding IS NOT NULL
            ORDER BY similarity * retention_score DESC
            LIMIT 10
        """
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query, user_id, app_name, query_embedding)

        items = []
        tokens_used = 0
        for row in rows:
            token_est = self._estimate_tokens(row["content"])
            if tokens_used + token_est > budget:
                break
            items.append(ContextItem(
                context_type="memory",
                content=row["content"],
                relevance_score=float(row["similarity"]) * float(row["retention_score"]),
                token_estimate=token_est,
                metadata={"memory_id": str(row["id"])},
            ))
            tokens_used += token_est
            # æ›´æ–°è®¿é—®è®°å½•
            await self._record_memory_access(str(row["id"]))

        return items

    async def _retrieve_history(
        self,
        thread_id: str,
        budget: int,
    ) -> list[ContextItem]:
        """æ£€ç´¢æœ€è¿‘å†å²"""
        query = """
            SELECT id, author, content, created_at
            FROM events
            WHERE thread_id = $1
              AND event_type = 'message'
            ORDER BY sequence_num DESC
            LIMIT 30
        """
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query, uuid.UUID(thread_id))

        items = []
        tokens_used = 0
        # åè½¬ä»¥æŒ‰æ—¶é—´æ­£åº
        for row in reversed(rows):
            content = row["content"]
            if isinstance(content, dict):
                text = content.get("text", str(content))
            else:
                text = str(content)

            formatted = f"[{row['author']}]: {text}"
            token_est = self._estimate_tokens(formatted)

            if tokens_used + token_est > budget:
                break
            items.append(ContextItem(
                context_type="history",
                content=formatted,
                relevance_score=1.0,  # å†å²æŒ‰æ—¶é—´æ’åº
                token_estimate=token_est,
            ))
            tokens_used += token_est

        return items

    async def _retrieve_facts(
        self,
        user_id: str,
        app_name: str,
        query_embedding: list[float],
        budget: int,
    ) -> list[ContextItem]:
        """æ£€ç´¢ç”¨æˆ· Facts"""
        query = """
            SELECT
                id, fact_type, key, value, confidence,
                1 - (embedding <=> $3::vector) AS similarity
            FROM facts
            WHERE user_id = $1
              AND app_name = $2
              AND (valid_until IS NULL OR valid_until > NOW())
            ORDER BY COALESCE(1 - (embedding <=> $3::vector), confidence) DESC
            LIMIT 10
        """
        async with self.pool.acquire() as conn:
            rows = await conn.fetch(query, user_id, app_name, query_embedding)

        items = []
        tokens_used = 0
        for row in rows:
            content = f"[{row['fact_type']}] {row['key']}: {row['value']}"
            token_est = self._estimate_tokens(content)

            if tokens_used + token_est > budget:
                break
            items.append(ContextItem(
                context_type="fact",
                content=content,
                relevance_score=float(row.get("similarity") or row["confidence"]),
                token_estimate=token_est,
                metadata={"fact_id": str(row["id"])},
            ))
            tokens_used += token_est

        return items

    async def _record_memory_access(self, memory_id: str) -> None:
        """è®°å½•è®°å¿†è®¿é—®"""
        query = """
            UPDATE memories
            SET access_count = access_count + 1,
                last_accessed_at = NOW()
            WHERE id = $1
        """
        async with self.pool.acquire() as conn:
            await conn.execute(query, uuid.UUID(memory_id))

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®— Token æ•°é‡ (ç®€åŒ–: 4 å­—ç¬¦ â‰ˆ 1 token)"""
        return len(text) // 4 + 1

    def format_context(self, window: ContextWindow) -> str:
        """å°†ä¸Šä¸‹æ–‡çª—å£æ ¼å¼åŒ–ä¸º Prompt"""
        sections = {
            "system": [],
            "fact": [],
            "memory": [],
            "history": [],
        }

        for item in window.items:
            sections[item.context_type].append(item.content)

        parts = []

        if sections["system"]:
            parts.append("\n".join(sections["system"]))

        if sections["fact"]:
            parts.append("## ç”¨æˆ·åå¥½")
            parts.extend(sections["fact"])

        if sections["memory"]:
            parts.append("## ç›¸å…³è®°å¿†")
            parts.extend(sections["memory"])

        if sections["history"]:
            parts.append("## å¯¹è¯å†å²")
            parts.extend(sections["history"])

        return "\n\n".join(parts)
```

### 4.4 Step 4: OpenMemoryService å®ç° (ADK é€‚é…å™¨)

#### 4.4.1 æ¥å£è®¾è®¡

åˆ›å»º `src/cognizes/engine/hippocampus/memory_service.py`ï¼š

```python
"""
OpenMemoryService: ADK MemoryService é€‚é…å™¨

å®ç°å¯¹æ ‡ Google ADK MemoryService çš„æ¥å£å¥‘çº¦ï¼Œ
ä½¿ç”¨ PostgreSQL + PGVector ä½œä¸ºåç«¯å­˜å‚¨ã€‚
"""

from __future__ import annotations

import uuid
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

import asyncpg

from .consolidation_worker import MemoryConsolidationWorker, JobType
from .retention_manager import MemoryRetentionManager
from .context_assembler import ContextAssembler, ContextWindow


@dataclass
class SearchMemoryResult:
    """è®°å¿†æ£€ç´¢ç»“æœ"""
    memory_id: str
    content: str
    memory_type: str
    relevance_score: float
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class SearchMemoryResponse:
    """æ£€ç´¢å“åº”"""
    memories: list[SearchMemoryResult]
    total_count: int
    query: str


class OpenMemoryService:
    """
    OpenMemoryService: å¯¹æ ‡ ADK MemoryService

    æ ¸å¿ƒèƒ½åŠ›:
    1. add_session_to_memory(): å°† Session è½¬åŒ–ä¸ºå¯æ£€ç´¢çš„è®°å¿†
    2. search_memory(): åŸºäº Query æ£€ç´¢ç›¸å…³è®°å¿†
    3. list_memories(): åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰è®°å¿†
    """

    def __init__(
        self,
        pool: asyncpg.Pool,
        embedding_model: str = "text-embedding-004",
        max_search_results: int = 10,
    ):
        self.pool = pool
        self.embedding_model = embedding_model
        self.max_search_results = max_search_results

        # å†…éƒ¨ç»„ä»¶
        self._consolidation_worker = MemoryConsolidationWorker(pool)
        self._retention_manager = MemoryRetentionManager(pool)
        self._context_assembler = ContextAssembler(pool)

    # ========================================
    # æ ¸å¿ƒæ¥å£: add_session_to_memory
    # ========================================

    async def add_session_to_memory(
        self,
        session_id: str,
        consolidation_type: str = "full",
    ) -> dict[str, Any]:
        """
        å°† Session ä¸­çš„å¯¹è¯è½¬åŒ–ä¸ºå¯æœç´¢çš„è®°å¿†

        Args:
            session_id: ä¼šè¯ ID (å¯¹åº” threads.id)
            consolidation_type: å·©å›ºç±»å‹
                - "fast": ä»…å¿«é€Ÿæ‘˜è¦
                - "deep": ä»…æ·±åº¦æå–
                - "full": å®Œæ•´å·©å›º

        Returns:
            å·©å›ºç»“æœ (ç”Ÿæˆçš„è®°å¿† ID åˆ—è¡¨)
        """
        job_type = {
            "fast": JobType.FAST_REPLAY,
            "deep": JobType.DEEP_REFLECTION,
            "full": JobType.FULL_CONSOLIDATION,
        }.get(consolidation_type, JobType.FULL_CONSOLIDATION)

        job = await self._consolidation_worker.consolidate(
            thread_id=session_id,
            job_type=job_type,
        )

        return {
            "job_id": job.id,
            "status": job.status.value,
            "result": job.result,
        }

    # ========================================
    # æ ¸å¿ƒæ¥å£: search_memory
    # ========================================

    async def search_memory(
        self,
        *,
        app_name: str,
        user_id: str,
        query: str,
        limit: int | None = None,
        memory_type: str | None = None,
        min_relevance: float = 0.0,
    ) -> SearchMemoryResponse:
        """
        åŸºäº Query æ£€ç´¢ç›¸å…³è®°å¿†

        Args:
            app_name: åº”ç”¨åç§°
            user_id: ç”¨æˆ· ID
            query: æŸ¥è¯¢æ–‡æœ¬
            limit: æœ€å¤§è¿”å›æ•°é‡
            memory_type: è¿‡æ»¤è®°å¿†ç±»å‹ ('episodic', 'semantic', 'summary')
            min_relevance: æœ€å°ç›¸å…³åº¦é˜ˆå€¼

        Returns:
            SearchMemoryResponse: æ£€ç´¢ç»“æœ
        """
        import google.generativeai as genai

        limit = limit or self.max_search_results

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        embedding_result = genai.embed_content(
            model=f"models/{self.embedding_model}",
            content=query,
            task_type="retrieval_query",
        )
        query_embedding = embedding_result["embedding"]

        # æ„å»ºæŸ¥è¯¢
        conditions = ["user_id = $1", "app_name = $2", "embedding IS NOT NULL"]
        params = [user_id, app_name]
        param_idx = 3

        if memory_type:
            conditions.append(f"memory_type = ${param_idx}")
            params.append(memory_type)
            param_idx += 1

        where_clause = " AND ".join(conditions)

        sql = f"""
            SELECT
                id, content, memory_type, metadata, retention_score,
                1 - (embedding <=> ${param_idx}::vector) AS relevance
            FROM memories
            WHERE {where_clause}
              AND (1 - (embedding <=> ${param_idx}::vector)) >= ${param_idx + 1}
            ORDER BY relevance * retention_score DESC
            LIMIT ${param_idx + 2}
        """
        params.extend([query_embedding, min_relevance, limit])

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(sql, *params)

        # è®°å½•è®¿é—®
        memory_ids = [str(row["id"]) for row in rows]
        if memory_ids:
            await self._retention_manager.record_batch_access(memory_ids)

        # æ„å»ºå“åº”
        memories = [
            SearchMemoryResult(
                memory_id=str(row["id"]),
                content=row["content"],
                memory_type=row["memory_type"],
                relevance_score=float(row["relevance"]),
                metadata=row["metadata"] if isinstance(row["metadata"], dict) else {},
            )
            for row in rows
        ]

        return SearchMemoryResponse(
            memories=memories,
            total_count=len(memories),
            query=query,
        )

    # ========================================
    # è¾…åŠ©æ¥å£: list_memories
    # ========================================

    async def list_memories(
        self,
        *,
        app_name: str,
        user_id: str,
        memory_type: str | None = None,
        limit: int = 50,
        offset: int = 0,
    ) -> list[dict[str, Any]]:
        """
        åˆ—å‡ºç”¨æˆ·çš„æ‰€æœ‰è®°å¿†

        Args:
            app_name: åº”ç”¨åç§°
            user_id: ç”¨æˆ· ID
            memory_type: è¿‡æ»¤è®°å¿†ç±»å‹
            limit: æœ€å¤§è¿”å›æ•°é‡
            offset: åˆ†é¡µåç§»

        Returns:
            è®°å¿†åˆ—è¡¨
        """
        conditions = ["user_id = $1", "app_name = $2"]
        params = [user_id, app_name]
        param_idx = 3

        if memory_type:
            conditions.append(f"memory_type = ${param_idx}")
            params.append(memory_type)
            param_idx += 1

        where_clause = " AND ".join(conditions)

        sql = f"""
            SELECT id, content, memory_type, metadata, retention_score, created_at
            FROM memories
            WHERE {where_clause}
            ORDER BY retention_score DESC, created_at DESC
            LIMIT ${param_idx}
            OFFSET ${param_idx + 1}
        """
        params.extend([limit, offset])

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(sql, *params)

        return [dict(row) for row in rows]

    # ========================================
    # è¾…åŠ©æ¥å£: get_context_window
    # ========================================

    async def get_context_window(
        self,
        *,
        app_name: str,
        user_id: str,
        thread_id: str,
        query: str,
        system_prompt: str | None = None,
        max_tokens: int = 8000,
    ) -> ContextWindow:
        """
        è·å–ç»„è£…å¥½çš„ä¸Šä¸‹æ–‡çª—å£

        Args:
            app_name: åº”ç”¨åç§°
            user_id: ç”¨æˆ· ID
            thread_id: å½“å‰ä¼šè¯ ID
            query: ç”¨æˆ·æŸ¥è¯¢
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_tokens: æœ€å¤§ Token é¢„ç®—

        Returns:
            ContextWindow: ç»„è£…åçš„ä¸Šä¸‹æ–‡
        """
        import google.generativeai as genai

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        embedding_result = genai.embed_content(
            model=f"models/{self.embedding_model}",
            content=query,
            task_type="retrieval_query",
        )
        query_embedding = embedding_result["embedding"]

        # é‡æ–°é…ç½® Token é¢„ç®—
        self._context_assembler.max_tokens = max_tokens

        return await self._context_assembler.assemble(
            user_id=user_id,
            app_name=app_name,
            thread_id=thread_id,
            query=query,
            query_embedding=query_embedding,
            system_prompt=system_prompt,
        )

    # ========================================
    # ç»´æŠ¤æ¥å£
    # ========================================

    async def cleanup_memories(
        self,
        threshold: float = 0.1,
        min_age_days: int = 7,
    ) -> dict[str, Any]:
        """
        æ¸…ç†ä½ä»·å€¼è®°å¿†

        Returns:
            æ¸…ç†ç»Ÿè®¡ä¿¡æ¯
        """
        stats = await self._retention_manager.cleanup_low_value_memories(
            threshold=threshold,
            min_age_days=min_age_days,
        )
        return {
            "total_memories": stats.total_memories,
            "cleaned_count": stats.cleaned_count,
            "avg_retention_score": stats.avg_retention_score,
        }
```

---

### 4.6 Step 6: AG-UI è®°å¿†ç³»ç»Ÿå¯è§†åŒ–æ¥å£

> [!NOTE]
>
> **å¯¹æ ‡ AG-UI åè®®**ï¼šæœ¬èŠ‚å®ç° The Hippocampus ä¸ AG-UI å¯è§†åŒ–å±‚çš„é›†æˆï¼Œæä¾›è®°å¿†å·©å›ºçŠ¶æ€ã€è®°å¿†å¬å›æ¥æºå’Œè®°å¿†å¥åº·åº¦çš„å¯è§†åŒ–èƒ½åŠ›ã€‚
>
> **å‚è€ƒèµ„æº**ï¼š
>
> - [AG-UI åè®®è°ƒç ”](../research/070-ag-ui.md)
> - [AG-UI å®˜æ–¹æ–‡æ¡£](https://docs.ag-ui.com/)

#### 4.6.1 è®°å¿†å¯è§†åŒ–æ¶æ„

```mermaid
graph TB
    subgraph "Hippocampus å­˜å‚¨å±‚"
        MEM[memories è¡¨]
        FACTS[facts è¡¨]
        CONS[consolidation_jobs è¡¨]
    end

    subgraph "å¯è§†åŒ–æ¥å£å±‚"
        CS[ConsolidationStatus]
        MH[MemoryHit]
        MD[MemoryDashboard]
    end

    subgraph "AG-UI äº‹ä»¶"
        ACT[ACTIVITY_SNAPSHOT]
        CUST[CUSTOM Events]
    end

    CONS -->|å·©å›ºçŠ¶æ€| CS
    MEM -->|å¬å›æ¥æº| MH
    MEM & FACTS -->|å¥åº·åº¦| MD

    CS --> ACT
    MH --> CUST
    MD --> CUST

    style CS fill:#a78bfa,stroke:#7c3aed,color:#000
    style MH fill:#4ade80,stroke:#16a34a,color:#000
    style MD fill:#fbbf24,stroke:#d97706,color:#000
```

#### 4.6.2 AG-UI äº‹ä»¶æ˜ å°„è¡¨

| Hippocampus åŠŸèƒ½ | è§¦å‘æ¡ä»¶                  | AG-UI äº‹ä»¶ç±»å‹          | å±•ç¤ºç»„ä»¶     |
| :--------------- | :------------------------ | :---------------------- | :----------- |
| è®°å¿†å·©å›ºè¿›åº¦     | Consolidation Worker æ‰§è¡Œ | `ACTIVITY_SNAPSHOT`     | å·©å›ºè¿›åº¦æ¡   |
| è®°å¿†å¬å›         | search_memory() è¿”å›ç»“æœ  | `CUSTOM (memory_hit)`   | æ¥æºæ ‡æ³¨å¡ç‰‡ |
| é—å¿˜æ›²çº¿æ›´æ–°     | retention_score è¡°å‡      | `CUSTOM (decay_update)` | è®°å¿†çƒ­åŠ›å›¾   |
| ä¸Šä¸‹æ–‡é¢„ç®—       | Context Budgeting æ‰§è¡Œ    | `STATE_DELTA`           | Token ä»ªè¡¨ç›˜ |

#### 4.6.3 MemoryVisualizer å®ç°

åˆ›å»º `src/cognizes/engine/hippocampus/memory_visualizer.py`ï¼š

```python
"""
Hippocampus MemoryVisualizer: è®°å¿†ç³»ç»Ÿå¯è§†åŒ–æ¥å£

èŒè´£:
1. æä¾›è®°å¿†å·©å›ºçŠ¶æ€å¯è§†åŒ–
2. å®ç°è®°å¿†å¬å›æ¥æºæ ‡æ³¨
3. å±•ç¤ºè®°å¿†å¥åº·åº¦ä»ªè¡¨ç›˜æ•°æ®
"""

from __future__ import annotations

import json
from dataclasses import dataclass, field
from typing import Any, Optional
from datetime import datetime, timedelta
from enum import Enum


class MemoryEventType(str, Enum):
    """è®°å¿†ç›¸å…³ AG-UI äº‹ä»¶ç±»å‹"""
    CONSOLIDATION_PROGRESS = "memory_consolidation_progress"
    MEMORY_HIT = "memory_hit"
    DECAY_UPDATE = "memory_decay_update"
    CONTEXT_BUDGET = "memory_context_budget"


@dataclass
class ConsolidationProgress:
    """è®°å¿†å·©å›ºè¿›åº¦"""
    job_id: str
    status: str  # pending, running, completed, failed
    total_events: int
    processed_events: int
    extracted_facts: int
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None

    @property
    def progress_percent(self) -> float:
        if self.total_events == 0:
            return 0.0
        return (self.processed_events / self.total_events) * 100


@dataclass
class MemoryHit:
    """è®°å¿†å¬å›å‘½ä¸­"""
    memory_id: str
    memory_type: str  # episodic, semantic, procedural
    content_preview: str
    relevance_score: float
    source_session_id: Optional[str] = None
    created_at: Optional[datetime] = None
    retention_score: float = 1.0


@dataclass
class MemoryHealthMetrics:
    """è®°å¿†å¥åº·åº¦æŒ‡æ ‡"""
    total_memories: int
    episodic_count: int
    semantic_count: int
    procedural_count: int
    avg_retention_score: float
    low_retention_count: int  # retention_score < 0.3
    decay_rate_7d: float  # 7 å¤©å†…è¡°å‡ç‡
    top_accessed_memories: list[str]


class MemoryVisualizer:
    """è®°å¿†ç³»ç»Ÿå¯è§†åŒ–å™¨"""

    def __init__(self, pool, event_emitter=None):
        """
        Args:
            pool: asyncpg è¿æ¥æ± 
            event_emitter: AG-UI äº‹ä»¶å‘å°„å™¨ (å¯é€‰)
        """
        self._pool = pool
        self._event_emitter = event_emitter

    async def emit_consolidation_progress(
        self,
        run_id: str,
        job_id: str
    ) -> ConsolidationProgress:
        """
        å‘å°„è®°å¿†å·©å›ºè¿›åº¦äº‹ä»¶

        Args:
            run_id: å½“å‰è¿è¡Œ ID
            job_id: å·©å›ºä»»åŠ¡ ID

        Returns:
            å·©å›ºè¿›åº¦ä¿¡æ¯
        """
        async with self._pool.acquire() as conn:
            job = await conn.fetchrow("""
                SELECT
                    id, status,
                    (input_data->>'total_events')::int as total_events,
                    (input_data->>'processed_events')::int as processed_events,
                    (output_data->>'extracted_facts')::int as extracted_facts,
                    started_at, completed_at
                FROM consolidation_jobs
                WHERE id = $1
            """, job_id)

            if not job:
                return None

            progress = ConsolidationProgress(
                job_id=str(job["id"]),
                status=job["status"],
                total_events=job["total_events"] or 0,
                processed_events=job["processed_events"] or 0,
                extracted_facts=job["extracted_facts"] or 0,
                start_time=job["started_at"],
                end_time=job["completed_at"]
            )

            # å‘å°„ AG-UI äº‹ä»¶
            if self._event_emitter:
                await self._event_emitter.emit_activity_snapshot(
                    run_id=run_id,
                    activity_type="memory_consolidation",
                    data={
                        "jobId": progress.job_id,
                        "status": progress.status,
                        "progressPercent": progress.progress_percent,
                        "extractedFacts": progress.extracted_facts
                    }
                )

            return progress

    async def emit_memory_hits(
        self,
        run_id: str,
        query: str,
        hits: list[dict]
    ) -> list[MemoryHit]:
        """
        å‘å°„è®°å¿†å¬å›å‘½ä¸­äº‹ä»¶

        ç”¨äºåœ¨ Agent å“åº”ä¸­æ ‡æ³¨è®°å¿†æ¥æº

        Args:
            run_id: å½“å‰è¿è¡Œ ID
            query: æœç´¢æŸ¥è¯¢
            hits: å¬å›ç»“æœåˆ—è¡¨

        Returns:
            è®°å¿†å‘½ä¸­åˆ—è¡¨
        """
        memory_hits = []

        for hit in hits:
            memory_hit = MemoryHit(
                memory_id=hit["id"],
                memory_type=hit.get("memory_type", "episodic"),
                content_preview=hit.get("content", "")[:200],
                relevance_score=hit.get("score", 0.0),
                source_session_id=hit.get("session_id"),
                created_at=hit.get("created_at"),
                retention_score=hit.get("retention_score", 1.0)
            )
            memory_hits.append(memory_hit)

        # å‘å°„ AG-UI CUSTOM äº‹ä»¶
        if self._event_emitter:
            await self._event_emitter.emit_custom(
                run_id=run_id,
                event_name=MemoryEventType.MEMORY_HIT.value,
                data={
                    "query": query,
                    "hits": [
                        {
                            "memoryId": h.memory_id,
                            "memoryType": h.memory_type,
                            "preview": h.content_preview,
                            "score": h.relevance_score,
                            "retentionScore": h.retention_score
                        }
                        for h in memory_hits
                    ]
                }
            )

        return memory_hits

    async def get_health_metrics(
        self,
        user_id: str,
        app_name: str
    ) -> MemoryHealthMetrics:
        """
        è·å–è®°å¿†å¥åº·åº¦æŒ‡æ ‡

        ç”¨äºæ¸²æŸ“è®°å¿†å¥åº·åº¦ä»ªè¡¨ç›˜

        Args:
            user_id: ç”¨æˆ· ID
            app_name: åº”ç”¨åç§°

        Returns:
            è®°å¿†å¥åº·åº¦æŒ‡æ ‡
        """
        async with self._pool.acquire() as conn:
            # åŸºç¡€ç»Ÿè®¡
            stats = await conn.fetchrow("""
                SELECT
                    COUNT(*) as total,
                    COUNT(*) FILTER (WHERE memory_type = 'episodic') as episodic,
                    COUNT(*) FILTER (WHERE memory_type = 'semantic') as semantic,
                    COUNT(*) FILTER (WHERE memory_type = 'procedural') as procedural,
                    AVG(retention_score) as avg_retention,
                    COUNT(*) FILTER (WHERE retention_score < 0.3) as low_retention
                FROM memories
                WHERE user_id = $1 AND app_name = $2
            """, user_id, app_name)

            # 7 å¤©è¡°å‡ç‡
            decay_stats = await conn.fetchrow("""
                WITH old_scores AS (
                    SELECT AVG(retention_score) as avg_score
                    FROM memories
                    WHERE user_id = $1 AND app_name = $2
                      AND created_at < NOW() - INTERVAL '7 days'
                ),
                new_scores AS (
                    SELECT AVG(retention_score) as avg_score
                    FROM memories
                    WHERE user_id = $1 AND app_name = $2
                )
                SELECT
                    COALESCE(
                        (old_scores.avg_score - new_scores.avg_score) /
                        NULLIF(old_scores.avg_score, 0) * 100,
                        0
                    ) as decay_rate
                FROM old_scores, new_scores
            """, user_id, app_name)

            # æœ€å¸¸è®¿é—®çš„è®°å¿†
            top_accessed = await conn.fetch("""
                SELECT id
                FROM memories
                WHERE user_id = $1 AND app_name = $2
                ORDER BY access_count DESC, retention_score DESC
                LIMIT 5
            """, user_id, app_name)

            return MemoryHealthMetrics(
                total_memories=stats["total"] or 0,
                episodic_count=stats["episodic"] or 0,
                semantic_count=stats["semantic"] or 0,
                procedural_count=stats["procedural"] or 0,
                avg_retention_score=float(stats["avg_retention"] or 0),
                low_retention_count=stats["low_retention"] or 0,
                decay_rate_7d=float(decay_stats["decay_rate"] or 0),
                top_accessed_memories=[str(r["id"]) for r in top_accessed]
            )

    async def emit_context_budget_status(
        self,
        run_id: str,
        budget_info: dict
    ) -> None:
        """
        å‘å°„ä¸Šä¸‹æ–‡é¢„ç®—çŠ¶æ€äº‹ä»¶

        Args:
            run_id: å½“å‰è¿è¡Œ ID
            budget_info: é¢„ç®—ä¿¡æ¯
        """
        if self._event_emitter:
            await self._event_emitter.emit_state_delta(
                run_id=run_id,
                delta=[{
                    "op": "replace",
                    "path": "/contextBudget",
                    "value": {
                        "totalTokens": budget_info.get("total_tokens", 0),
                        "usedTokens": budget_info.get("used_tokens", 0),
                        "memoriesIncluded": budget_info.get("memories_count", 0),
                        "memoryTokens": budget_info.get("memory_tokens", 0)
                    }
                }]
            )
```

#### 4.6.4 å‰ç«¯å±•ç¤ºç»„ä»¶è§„èŒƒ

| ç»„ä»¶åç§°                   | æ•°æ®æº              | å±•ç¤ºå†…å®¹                           |
| :------------------------- | :------------------ | :--------------------------------- |
| `ConsolidationProgressBar` | ACTIVITY_SNAPSHOT   | è¿›åº¦ç™¾åˆ†æ¯”ã€æå–äº‹å®æ•°             |
| `MemorySourceCard`         | CUSTOM (memory_hit) | è®°å¿†ç±»å‹å›¾æ ‡ã€å†…å®¹é¢„è§ˆã€ç›¸å…³æ€§åˆ†æ•° |
| `MemoryHealthDashboard`    | API è½®è¯¢            | æ€»æ•°ã€ç±»å‹åˆ†å¸ƒã€è¡°å‡æ›²çº¿           |
| `TokenBudgetMeter`         | STATE_DELTA         | å·²ç”¨/æ€»é‡è¿›åº¦æ¡                    |

#### 4.6.5 ä»»åŠ¡æ¸…å•

| ä»»åŠ¡ ID | ä»»åŠ¡æè¿°                   | çŠ¶æ€      | éªŒæ”¶æ ‡å‡†         |
| :------ | :------------------------- | :-------- | :--------------- |
| P2-6-1  | å®ç° `MemoryVisualizer` ç±» | ğŸ”² å¾…å¼€å§‹ | 4 ç§äº‹ä»¶ç±»å‹æ”¯æŒ |
| P2-6-2  | å®ç°å·©å›ºè¿›åº¦äº‹ä»¶å‘å°„       | ğŸ”² å¾…å¼€å§‹ | è¿›åº¦å®æ—¶æ›´æ–°     |
| P2-6-3  | å®ç°è®°å¿†å¬å›æ¥æºæ ‡æ³¨       | ğŸ”² å¾…å¼€å§‹ | æ¥æºå¯è¿½æº¯       |
| P2-6-4  | å®ç°å¥åº·åº¦æŒ‡æ ‡æ¥å£         | ğŸ”² å¾…å¼€å§‹ | æŒ‡æ ‡è®¡ç®—æ­£ç¡®     |
| P2-6-5  | ç¼–å†™å¯è§†åŒ–æ¥å£æµ‹è¯•         | ğŸ”² å¾…å¼€å§‹ | è¦†ç›–ç‡ > 80%     |

#### 4.6.6 éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹     | éªŒæ”¶æ ‡å‡†                    | éªŒè¯æ–¹æ³• |
| :--------- | :-------------------------- | :------- |
| å·©å›ºè¿›åº¦   | å®æ—¶å±•ç¤ºå·©å›ºè¿›åº¦ç™¾åˆ†æ¯”      | é›†æˆæµ‹è¯• |
| æ¥æºæ ‡æ³¨   | å¬å›çš„è®°å¿†æ˜¾ç¤ºæ¥æºä¼šè¯      | E2E æµ‹è¯• |
| å¥åº·åº¦     | æ­£ç¡®è®¡ç®—è¡°å‡ç‡å’Œåˆ†ç±»ç»Ÿè®¡    | å•å…ƒæµ‹è¯• |
| Token é¢„ç®— | å®æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ Token ä½¿ç”¨é‡ | é›†æˆæµ‹è¯• |

---

## 5. éªŒè¯ SOP (Phase 2)

> [!IMPORTANT]
>
> æœ¬èŠ‚æä¾› Phase 2: The Hippocampus å®Œæ•´éªŒæ”¶æµç¨‹ï¼Œè¯·æŒ‰é¡ºåºé€æ­¥æ‰§è¡Œã€‚

### 5.1 Step 1: Schema éƒ¨ç½²éªŒè¯

```bash
# 1.1 ç¡®ä¿ Phase 1 Schema å·²éƒ¨ç½²
psql -d 'cognizes-engine' -c "\dt threads"
# åº”æ˜¾ç¤º threads è¡¨

# 1.2 éƒ¨ç½² Hippocampus Schema
psql -d 'cognizes-engine' -f src/cognizes/engine/schema/hippocampus_schema.sql

# 1.3 éªŒè¯è¡¨åˆ›å»º
psql -d 'cognizes-engine' -c "\dt"
# åº”æ˜¾ç¤º: memories, facts, consolidation_jobs, instructions

# 1.4 éªŒè¯ç´¢å¼•
psql -d 'cognizes-engine' -c "\di" | grep -E "(memories|facts)"

# 1.5 éªŒè¯å‡½æ•°
psql -d 'cognizes-engine' -c "\df calculate_retention_score"
psql -d 'cognizes-engine' -c "\df cleanup_low_value_memories"

# 1.6 æµ‹è¯•è¡°å‡å‡½æ•°
psql -d 'cognizes-engine' -c "SELECT calculate_retention_score(5, NOW() - INTERVAL '3 days', 0.1);"
# åº”è¿”å› 0.x çš„æµ®ç‚¹æ•°
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] `memories`, `facts`, `consolidation_jobs`, `instructions` è¡¨å­˜åœ¨
- [ ] HNSW å‘é‡ç´¢å¼•å·²åˆ›å»º
- [ ] `calculate_retention_score` å‡½æ•°å¯æ­£å¸¸è°ƒç”¨
- [ ] `cleanup_low_value_memories` å‡½æ•°å­˜åœ¨

---

#### 5.1.1 Step 1.1: pg_cron å®šæ—¶ä»»åŠ¡é…ç½® (P2-2-8, P2-3-4)

> [!IMPORTANT]
>
> pg_cron å®šæ—¶ä»»åŠ¡ç”¨äºè‡ªåŠ¨è§¦å‘è®°å¿†å·©å›ºå’Œä½ä»·å€¼è®°å¿†æ¸…ç†ï¼Œéœ€é…ç½®å Phase 2 éªŒæ”¶æ‰èƒ½å®Œæ•´é€šè¿‡ã€‚

```bash
# 1.1 æ£€æŸ¥ pg_cron æ‰©å±•æ˜¯å¦å·²å®‰è£… (Phase 1 å·²å®Œæˆ)
psql -d 'cognizes-engine' -c "SELECT * FROM pg_extension WHERE extname = 'pg_cron';"
# åº”è¿”å› 1 è¡Œè®°å½•

# 1.2 é…ç½®å®šæ—¶ä»»åŠ¡ - æ¯å¤©å‡Œæ™¨ 2 ç‚¹æ¸…ç†ä½ä»·å€¼è®°å¿† (P2-3-4)
psql -d 'cognizes-engine' -c "
SELECT cron.schedule(
    'cleanup_memories',
    '0 2 * * *',
    \$\$SELECT cleanup_low_value_memories(0.1, 7)\$\$
);
"
# åº”è¿”å›ä»»åŠ¡ ID (å¦‚ 1)

# 1.3 é…ç½®å®šæ—¶ä»»åŠ¡ - æ¯å°æ—¶è§¦å‘è®°å¿†å·©å›ºæ£€æŸ¥ (P2-2-8)
psql -d 'cognizes-engine' -c "
SELECT cron.schedule(
    'trigger_consolidation',
    '0 * * * *',
    \$\$
    INSERT INTO consolidation_jobs (thread_id, job_type, status)
    SELECT id, 'full_consolidation', 'pending'
    FROM threads
    WHERE updated_at > NOW() - INTERVAL '1 hour'
      AND id NOT IN (
          SELECT thread_id FROM consolidation_jobs
          WHERE created_at > NOW() - INTERVAL '1 hour'
      )
    \$\$
);
"
# åº”è¿”å›ä»»åŠ¡ ID (å¦‚ 2)

# 1.4 éªŒè¯å®šæ—¶ä»»åŠ¡åˆ›å»ºæˆåŠŸ
psql -d 'cognizes-engine' -c "SELECT jobid, jobname, schedule, command FROM cron.job;"
# åº”æ˜¾ç¤º cleanup_memories å’Œ trigger_consolidation ä¸¤ä¸ªä»»åŠ¡

# 1.5 æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œæ—¥å¿— (é¦–æ¬¡é…ç½®åå¯èƒ½ä¸ºç©º)
psql -d 'cognizes-engine' -c "SELECT * FROM cron.job_run_details ORDER BY start_time DESC LIMIT 5;"

# 1.6 æ‰‹åŠ¨æµ‹è¯•æ¸…ç†å‡½æ•° (å¯é€‰)
psql -d 'cognizes-engine' -c "SELECT cleanup_low_value_memories(0.1, 7);"
# åº”è¿”å›æ¸…ç†çš„è®°å½•æ•° (å¯èƒ½ä¸º 0)
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] pg_cron æ‰©å±•å·²å®‰è£…
- [ ] `cleanup_memories` å®šæ—¶ä»»åŠ¡å·²åˆ›å»º (æ¯å¤© 02:00)
- [ ] `trigger_consolidation` å®šæ—¶ä»»åŠ¡å·²åˆ›å»º (æ¯å°æ—¶)
- [ ] `cron.job` è¡¨æ˜¾ç¤º 2 ä¸ªä»»åŠ¡

**åˆ é™¤ä»»åŠ¡ (å¦‚éœ€é‡æ–°é…ç½®)**ï¼š

```bash
# åˆ é™¤æŒ‡å®šä»»åŠ¡
psql -d 'cognizes-engine' -c "SELECT cron.unschedule('cleanup_memories');"
psql -d 'cognizes-engine' -c "SELECT cron.unschedule('trigger_consolidation');"
```

---

### 5.2 Step 2: å•å…ƒæµ‹è¯•éªŒè¯

```bash
# 2.1 è¿è¡Œ Hippocampus å•å…ƒæµ‹è¯•
uv run pytest tests/unittests/hippocampus/ -v --tb=short

# 2.2 æŸ¥çœ‹æµ‹è¯•è¦†ç›–ç‡ (å¯é€‰ï¼Œéœ€å…ˆå®‰è£… pytest-cov)
# uv add pytest-cov --dev
uv run pytest tests/unittests/hippocampus/ -v --cov=src/cognizes/engine/hippocampus --cov-report=term-missing
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] 35 ä¸ªå•å…ƒæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] è¦†ç›–ä»¥ä¸‹æ¨¡å—:
  - `consolidation_worker.py` (æ•°æ®ç±»ã€æšä¸¾ã€æ ¼å¼åŒ–é€»è¾‘)
  - `retention_manager.py` (ä¿ç•™åˆ†æ•°åˆ†å¸ƒ)
  - `context_assembler.py` (Token ä¼°ç®—ã€ä¸Šä¸‹æ–‡æ ¼å¼åŒ–)
  - `memory_service.py` (æœåŠ¡å‚æ•°éªŒè¯)
  - `memory_visualizer.py` (äº‹ä»¶ç±»å‹ã€è¿›åº¦è®¡ç®—)

---

### 5.3 Step 3: é›†æˆæµ‹è¯•éªŒè¯

```bash
# 3.1 è¿è¡Œ Hippocampus é›†æˆæµ‹è¯•
uv run pytest tests/integration/hippocampus/ -v -s --tb=short

# 3.2 æŸ¥çœ‹è¯¦ç»†è¾“å‡º (å«æ€§èƒ½æŒ‡æ ‡)
uv run pytest tests/integration/hippocampus/ -v -s
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] 16 ä¸ªé›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] Schema æµ‹è¯•é€šè¿‡: è¡¨ç»“æ„ã€ç´¢å¼•ã€å‡½æ•°ã€çº¦æŸ
- [ ] Read-Your-Writes å»¶è¿Ÿ < 100ms
- [ ] æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½ P99 < 50ms (1K è§„æ¨¡)
- [ ] ä¿ç•™åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡æ­£ç¡®
- [ ] è®¿é—®è®¡æ•°é€’å¢æ­£ç¡®
- [ ] Fact Upsert çº¦æŸç”Ÿæ•ˆ

---

### 5.4 Step 4: æ€§èƒ½æµ‹è¯• (å¯é€‰, 10 ä¸‡è§„æ¨¡)

```bash
# 4.0 æ¸…ç†å†å²æ€§èƒ½æµ‹è¯•æ•°æ® (é¿å…å­˜é‡æ•°æ®å½±å“ç»“æœ)
uv run python -c "
import asyncio
import asyncpg

async def cleanup():
    pool = await asyncpg.create_pool('postgresql://aigc:@localhost/cognizes-engine')
    user_id = 'perf_test_user'

    async with pool.acquire() as conn:
        # ç»Ÿè®¡ç°æœ‰æ•°æ®
        count = await conn.fetchval(
            'SELECT COUNT(*) FROM memories WHERE user_id = \$1', user_id
        )
        if count == 0:
            print('âœ“ æ— å†å²æµ‹è¯•æ•°æ®ï¼Œæ— éœ€æ¸…ç†')
            return

        # æ¸…ç†æ€§èƒ½æµ‹è¯•æ•°æ®
        deleted = await conn.fetchval('''
            DELETE FROM memories WHERE user_id = \$1 RETURNING COUNT(*)
        ''', user_id)
        print(f'âœ“ å·²æ¸…ç† {count} æ¡å†å²æµ‹è¯•æ•°æ®')
    await pool.close()

asyncio.run(cleanup())
"

# 4.1 ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
uv run python -c "
import asyncio
import asyncpg
import uuid
import random
from datetime import datetime, timedelta

async def seed():
    pool = await asyncpg.create_pool('postgresql://aigc:@localhost/cognizes-engine')
    user_id = 'perf_test_user'
    app_name = 'perf_test_app'

    async with pool.acquire() as conn:
        count = await conn.fetchval(
            'SELECT COUNT(*) FROM memories WHERE user_id = \$1', user_id
        )
        if count >= 100000:
            print(f'å·²æœ‰ {count} æ¡æ•°æ®ï¼Œè·³è¿‡')
            return

        print('å¼€å§‹ç”Ÿæˆ 100K æµ‹è¯•æ•°æ®...')
        batch_size = 1000
        base_time = datetime.now() - timedelta(days=365)

        for batch in range(100):
            rows = []
            for i in range(batch_size):
                created_at = base_time + timedelta(minutes=random.randint(0, 525600))
                rows.append((
                    uuid.uuid4(), user_id, app_name, 'episodic',
                    f'æµ‹è¯•è®°å¿† {batch * batch_size + i}',
                    random.random(), random.randint(0, 100), created_at
                ))
            await conn.executemany('''
                INSERT INTO memories (id, user_id, app_name, memory_type, content,
                                     retention_score, access_count, created_at)
                VALUES (\$1, \$2, \$3, \$4, \$5, \$6, \$7, \$8)
            ''', rows)
            if (batch + 1) % 10 == 0:
                print(f'  å·²æ’å…¥ {(batch + 1) * batch_size}')
    await pool.close()

asyncio.run(seed())
"

# 4.2 è¿è¡Œæ€§èƒ½æµ‹è¯•
uv run pytest tests/integration/hippocampus/test_episodic_performance.py -v -s -k "full"
# === å®Œæ•´æ€§èƒ½æµ‹è¯• (100,000 æ¡) ===
# å¹³å‡å»¶è¿Ÿ: 1.01 ms
# P99 å»¶è¿Ÿ: 2.38 ms
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] 10 ä¸‡è§„æ¨¡æ—¶é—´åˆ‡ç‰‡æŸ¥è¯¢ P99 < 100ms
- [ ] æŸ¥è¯¢ä½¿ç”¨ç´¢å¼•æ‰«æ (éå…¨è¡¨æ‰«æ)

---

### 5.5 Step 5: æ¨¡å—å¯¼å…¥éªŒè¯

```bash
# 5.1 éªŒè¯æ¨¡å—å¯å¯¼å…¥
uv run python -c "
from cognizes.engine.hippocampus.consolidation_worker import (
    MemoryConsolidationWorker, JobType, JobStatus
)
from cognizes.engine.hippocampus.retention_manager import MemoryRetentionManager
from cognizes.engine.hippocampus.context_assembler import ContextAssembler
from cognizes.engine.hippocampus.memory_service import OpenMemoryService
from cognizes.engine.hippocampus.memory_visualizer import MemoryVisualizer

print('âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ')
"
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] æ‰€æœ‰ 5 ä¸ªæ¨¡å—å¯æ­£å¸¸å¯¼å…¥
- [ ] æ— å¾ªç¯ä¾èµ–é”™è¯¯

---

### 5.6 Step 6: å…¨é‡æµ‹è¯•éªŒè¯

```bash
# 6.1 è¿è¡Œæ‰€æœ‰æµ‹è¯• (åŒ…æ‹¬ Phase 1)
uv run pytest tests/ -v --tb=line

# 6.2 æŸ¥çœ‹æµ‹è¯•ç»Ÿè®¡
uv run pytest tests/ -v --tb=line 2>&1 | tail -5
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] Phase 1 æµ‹è¯•: 61 passed
- [ ] Phase 2 å•å…ƒæµ‹è¯•: 35 passed
- [ ] Phase 2 é›†æˆæµ‹è¯•: 16 passed
- [ ] **æ€»è®¡: 112+ passed**

---

### 5.7 éªŒæ”¶æ€»ç»“æ¸…å•

| éªŒæ”¶é¡¹           | çŠ¶æ€ | è¯´æ˜                          |
| :--------------- | :--: | :---------------------------- |
| Schema éƒ¨ç½²      |  â¬œ  | 4 å¼ è¡¨ + 2 ä¸ªå‡½æ•° + HNSW ç´¢å¼• |
| pg_cron å®šæ—¶ä»»åŠ¡ |  â¬œ  | 2 ä¸ªä»»åŠ¡ (æ¸…ç† + å·©å›º)        |
| å•å…ƒæµ‹è¯•         |  â¬œ  | 35 tests passed               |
| é›†æˆæµ‹è¯•         |  â¬œ  | 17 tests passed               |
| Read-Your-Writes |  â¬œ  | P99 < 100ms                   |
| æ¨¡å—å¯¼å…¥         |  â¬œ  | 5 æ¨¡å—æ— é”™è¯¯                  |
| å…¨é‡å›å½’         |  â¬œ  | 113+ tests passed             |

> [!TIP]
>
> å®Œæˆä¸Šè¿°æ‰€æœ‰éªŒæ”¶é¡¹åï¼Œå‹¾é€‰çŠ¶æ€ä¸º âœ…ï¼ŒPhase 2: The Hippocampus éªŒæ”¶é€šè¿‡ï¼Œå¯è¿›å…¥ Phase 3: The Perceptionã€‚

---

## 6. éªŒæ”¶åŸºå‡†

### 6.1 åŠŸèƒ½éªŒæ”¶çŸ©é˜µ

| éªŒæ”¶é¡¹                | ä»»åŠ¡ ID           | éªŒæ”¶æ ‡å‡†                                                 | éªŒè¯æ–¹æ³•                |
| :-------------------- | :---------------- | :------------------------------------------------------- | :---------------------- |
| **Schema éƒ¨ç½²**       | P2-2-1 ~ P2-2-2   | `memories`, `facts`, `consolidation_jobs` è¡¨åˆ›å»ºæˆåŠŸ     | `\dt` æŸ¥çœ‹è¡¨åˆ—è¡¨        |
| **Fast Replay**       | P2-2-5 ~ P2-2-8   | å¯¹è¯æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œå­˜å…¥ `memories` è¡¨                     | å•å…ƒæµ‹è¯•                |
| **Deep Reflection**   | P2-2-9 ~ P2-2-12  | Facts æå–æˆåŠŸï¼Œå­˜å…¥ `facts` è¡¨ (Upsert é€»è¾‘æ­£ç¡®)        | å•å…ƒæµ‹è¯• + é‡å¤æ’å…¥æµ‹è¯• |
| **Read-Your-Writes**  | P2-2-13 ~ P2-2-14 | æ–°è®°å¿†åœ¨ä¸‹ä¸€ Turn ç«‹å³å¯æ£€ç´¢                             | å»¶è¿Ÿæµ‹è¯• (< 100ms)      |
| **è‰¾å®¾æµ©æ–¯è¡°å‡**      | P2-3-1 ~ P2-3-4   | `retention_score` éšæ—¶é—´è¡°å‡ï¼Œé«˜é¢‘è®¿é—®æå‡åˆ†æ•°           | è¡°å‡æ›²çº¿éªŒè¯            |
| **æƒ…æ™¯åˆ†å—**          | P2-3-5 ~ P2-3-7   | æŒ‰æ—¶é—´åˆ‡ç‰‡æ£€ç´¢ P99 < 100ms (10 ä¸‡è®°å¿†è§„æ¨¡)               | æ€§èƒ½æµ‹è¯•                |
| **Context Window**    | P2-3-8 ~ P2-3-11  | åŠ¨æ€ç»„è£… Context ä¸è¶…å‡º Token é¢„ç®—ï¼Œè¶…é™æ—¶è‡ªåŠ¨æˆªæ–­       | Token ç»Ÿè®¡æµ‹è¯•          |
| **OpenMemoryService** | Phase 2 ç»¼åˆ      | å®ç° `add_session_to_memory()` å’Œ `search_memory()` æ¥å£ | æ¥å£å…¼å®¹æ€§æµ‹è¯•          |

### 6.2 æ€§èƒ½éªŒæ”¶æŒ‡æ ‡

| æŒ‡æ ‡                 | ç›®æ ‡å€¼    | æµ‹è¯•æ¡ä»¶                      |
| :------------------- | :-------- | :---------------------------- |
| **è®°å¿†å†™å…¥å»¶è¿Ÿ**     | < 500ms   | å•æ¬¡ `consolidate()` è°ƒç”¨     |
| **è®°å¿†æ£€ç´¢å»¶è¿Ÿ**     | < 50ms    | `search_memory()` Top-10 ç»“æœ |
| **å‘é‡ç´¢å¼• QPS**     | > 100 QPS | 10 ä¸‡å‘é‡è§„æ¨¡                 |
| **Read-Your-Writes** | < 100ms   | æ–°è®°å¿†å¯è§å»¶è¿Ÿ                |
| **Context ç»„è£…å»¶è¿Ÿ** | < 100ms   | 8000 Token é¢„ç®—               |

### 6.3 å…¼å®¹æ€§éªŒæ”¶

| éªŒæ”¶é¡¹                     | éªŒæ”¶æ ‡å‡†                                                |
| :------------------------- | :------------------------------------------------------ |
| **ADK MemoryService å…¼å®¹** | `OpenMemoryService` å¯ä½œä¸º ADK `MemoryService` æ›¿ä»£ä½¿ç”¨ |
| **Phase 1 å…¼å®¹**           | ä¸ `threads`/`events` è¡¨æ— ç¼å…³è”                        |
| **å‘é‡æ ¼å¼å…¼å®¹**           | ä½¿ç”¨ä¸ Phase 1 ç›¸åŒçš„ 1536 ç»´å‘é‡ (Gemini embedding)    |

### 6.4 éªŒè¯æµ‹è¯•ä»£ç 

> [!NOTE]
>
> æœ¬èŠ‚æä¾›å…³é”®éªŒè¯æµ‹è¯•çš„ä»£ç å®ç°ï¼Œå¯¹åº”ä»»åŠ¡ P2-2-13~14, P2-3-7, P2-4-3ã€‚

#### 6.4.1 Read-Your-Writes å»¶è¿Ÿæµ‹è¯• (P2-2-13, P2-2-14)

åˆ›å»º `tests/hippocampus/test_read_your_writes.py`ï¼š

```python
"""
Read-Your-Writes å»¶è¿Ÿæµ‹è¯•

éªŒè¯æ–°å†™å…¥çš„è®°å¿†èƒ½å¦åœ¨ä¸‹ä¸€ä¸ª Turn ç«‹å³å¯è§ï¼Œ
ç¡®ä¿æˆ‘ä»¬çš„ Zero-ETL æ¶æ„æ¯” Google æ–¹æ¡ˆæ›´å¿«ã€‚

éªŒæ”¶æ ‡å‡†: å»¶è¿Ÿ < 100ms
"""

import asyncio
import time
import uuid
from statistics import mean, stdev

import asyncpg
import pytest

from hippocampus.consolidation_worker import MemoryConsolidationWorker, JobType
from hippocampus.memory_service import OpenMemoryService


class TestReadYourWrites:
    """Read-Your-Writes å»¶è¿Ÿæµ‹è¯•å¥—ä»¶"""

    @pytest.fixture
    async def pool(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥æ± """
        pool = await asyncpg.create_pool(
            "postgresql://user:pass@localhost/agent_db_test"
        )
        yield pool
        await pool.close()

    @pytest.fixture
    async def memory_service(self, pool):
        """åˆ›å»º MemoryService å®ä¾‹"""
        return OpenMemoryService(pool)

    @pytest.fixture
    async def setup_test_thread(self, pool):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„ Thread å’Œ Events"""
        thread_id = str(uuid.uuid4())
        user_id = "test_user"
        app_name = "test_app"

        async with pool.acquire() as conn:
            # åˆ›å»º Thread
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, $2, $3, '{}')
            """, uuid.UUID(thread_id), user_id, app_name)

            # åˆ›å»ºæµ‹è¯• Events
            for i in range(5):
                await conn.execute("""
                    INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                    VALUES ($1, $2, 'message', $3, $4)
                """, uuid.UUID(thread_id),
                    'user' if i % 2 == 0 else 'agent',
                    f'{{"text": "æµ‹è¯•æ¶ˆæ¯ {i}"}}',
                    i)

        yield {"thread_id": thread_id, "user_id": user_id, "app_name": app_name}

        # æ¸…ç†
        async with pool.acquire() as conn:
            await conn.execute("DELETE FROM threads WHERE id = $1", uuid.UUID(thread_id))

    async def test_read_your_writes_latency(
        self, pool, memory_service, setup_test_thread
    ):
        """
        éªŒè¯ Read-Your-Writes å»¶è¿Ÿ < 100ms

        æµç¨‹:
        1. æ‰§è¡Œè®°å¿†å·©å›º (å†™å…¥)
        2. ç«‹å³æ‰§è¡Œè®°å¿†æ£€ç´¢ (è¯»å–)
        3. æµ‹é‡ä»å†™å…¥å®Œæˆåˆ°è¯»å–æˆåŠŸçš„å»¶è¿Ÿ
        """
        thread_info = setup_test_thread
        latencies = []

        for _ in range(10):  # æ‰§è¡Œ 10 æ¬¡æµ‹é‡
            # Step 1: æ‰§è¡Œå·©å›º (å†™å…¥)
            result = await memory_service.add_session_to_memory(
                session_id=thread_info["thread_id"],
                consolidation_type="fast"
            )
            assert result["status"] == "completed"

            # Step 2: ç«‹å³æ£€ç´¢ (è¯»å–) å¹¶æµ‹é‡å»¶è¿Ÿ
            start = time.perf_counter()
            search_result = await memory_service.search_memory(
                app_name=thread_info["app_name"],
                user_id=thread_info["user_id"],
                query="æµ‹è¯•æ¶ˆæ¯",
            )
            end = time.perf_counter()

            latency_ms = (end - start) * 1000
            latencies.append(latency_ms)

            # éªŒè¯è®°å¿†å¯è§
            assert search_result.total_count > 0, "æ–°è®°å¿†åº”ç«‹å³å¯è§"

        # ç»Ÿè®¡ç»“æœ
        avg_latency = mean(latencies)
        p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]

        print(f"\n=== Read-Your-Writes å»¶è¿Ÿæµ‹è¯•ç»“æœ ===")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ms")
        print(f"P99 å»¶è¿Ÿ: {p99_latency:.2f} ms")
        print(f"æ ‡å‡†å·®: {stdev(latencies):.2f} ms")

        # éªŒæ”¶æ ‡å‡†: P99 < 100ms
        assert p99_latency < 100, f"P99 å»¶è¿Ÿ {p99_latency:.2f}ms è¶…è¿‡ 100ms é˜ˆå€¼"


# è¿è¡Œ: pytest -v tests/hippocampus/test_read_your_writes.py
```

#### 6.4.2 æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½æµ‹è¯• (P2-3-7)

åˆ›å»º `tests/hippocampus/test_episodic_performance.py`ï¼š

```python
"""
æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½æµ‹è¯•

éªŒè¯åœ¨ 10 ä¸‡è®°å¿†è§„æ¨¡ä¸‹ï¼ŒæŒ‰æ—¶é—´åˆ‡ç‰‡æ£€ç´¢çš„ P99 < 100msã€‚
"""

import asyncio
import random
import time
import uuid
from datetime import datetime, timedelta
from statistics import mean

import asyncpg
import pytest


class TestEpisodicPerformance:
    """æƒ…æ™¯åˆ†å—æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    MEMORY_COUNT = 100_000  # 10 ä¸‡è®°å¿†
    TEST_RUNS = 50

    @pytest.fixture(scope="class")
    async def pool(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥æ± """
        pool = await asyncpg.create_pool(
            "postgresql://user:pass@localhost/agent_db_test",
            min_size=5,
            max_size=20,
        )
        yield pool
        await pool.close()

    @pytest.fixture(scope="class")
    async def seed_memories(self, pool):
        """
        é¢„å…ˆå¡«å…… 10 ä¸‡æ¡æµ‹è¯•è®°å¿†

        æ³¨æ„: æ­¤ fixture ä»…åœ¨æµ‹è¯•ç±»é¦–æ¬¡è¿è¡Œæ—¶æ‰§è¡Œ
        """
        user_id = "perf_test_user"
        app_name = "perf_test_app"

        async with pool.acquire() as conn:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æµ‹è¯•æ•°æ®
            count = await conn.fetchval(
                "SELECT COUNT(*) FROM memories WHERE user_id = $1",
                user_id
            )
            if count >= self.MEMORY_COUNT:
                print(f"\nå·²å­˜åœ¨ {count} æ¡æµ‹è¯•è®°å¿†ï¼Œè·³è¿‡ç§å­æ•°æ®ç”Ÿæˆ")
                return {"user_id": user_id, "app_name": app_name}

            print(f"\nå¼€å§‹ç”Ÿæˆ {self.MEMORY_COUNT} æ¡æµ‹è¯•è®°å¿†...")

            # æ‰¹é‡æ’å…¥ (æ¯æ‰¹ 1000 æ¡)
            batch_size = 1000
            base_time = datetime.now() - timedelta(days=365)

            for batch in range(self.MEMORY_COUNT // batch_size):
                rows = []
                for i in range(batch_size):
                    created_at = base_time + timedelta(
                        minutes=random.randint(0, 525600)  # ä¸€å¹´å†…éšæœº
                    )
                    rows.append((
                        uuid.uuid4(),
                        user_id,
                        app_name,
                        'episodic',
                        f'æµ‹è¯•è®°å¿†å†…å®¹ {batch * batch_size + i}',
                        random.random(),  # retention_score
                        random.randint(0, 100),  # access_count
                        created_at,
                    ))

                await conn.executemany("""
                    INSERT INTO memories (id, user_id, app_name, memory_type, content,
                                         retention_score, access_count, created_at)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                """, rows)

                if (batch + 1) % 10 == 0:
                    print(f"  å·²æ’å…¥ {(batch + 1) * batch_size} æ¡è®°å¿†")

            print(f"æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")

        return {"user_id": user_id, "app_name": app_name}

    async def test_time_slice_query_performance(self, pool, seed_memories):
        """
        æµ‹è¯•æŒ‰æ—¶é—´åˆ‡ç‰‡æŸ¥è¯¢æ€§èƒ½

        éªŒæ”¶æ ‡å‡†: P99 < 100ms
        """
        user_id = seed_memories["user_id"]
        app_name = seed_memories["app_name"]

        latencies = []

        for _ in range(self.TEST_RUNS):
            # éšæœºé€‰æ‹©ä¸€ä¸ª 7 å¤©çš„æ—¶é—´çª—å£
            start_offset = random.randint(0, 358)
            start_time = datetime.now() - timedelta(days=365 - start_offset)
            end_time = start_time + timedelta(days=7)

            # æ‰§è¡Œæ—¶é—´åˆ‡ç‰‡æŸ¥è¯¢
            start = time.perf_counter()

            async with pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT id, content, retention_score, created_at
                    FROM memories
                    WHERE user_id = $1
                      AND app_name = $2
                      AND created_at >= $3
                      AND created_at <= $4
                    ORDER BY created_at DESC
                    LIMIT 50
                """, user_id, app_name, start_time, end_time)

            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            latencies.append(latency_ms)

        # ç»Ÿè®¡ç»“æœ
        avg_latency = mean(latencies)
        p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
        max_latency = max(latencies)

        print(f"\n=== æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½æµ‹è¯•ç»“æœ ({self.MEMORY_COUNT:,} æ¡è®°å¿†) ===")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ms")
        print(f"P99 å»¶è¿Ÿ: {p99_latency:.2f} ms")
        print(f"æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f} ms")
        print(f"æµ‹è¯•æ¬¡æ•°: {self.TEST_RUNS}")

        # éªŒæ”¶æ ‡å‡†: P99 < 100ms
        assert p99_latency < 100, f"P99 å»¶è¿Ÿ {p99_latency:.2f}ms è¶…è¿‡ 100ms é˜ˆå€¼"

    async def test_composite_index_usage(self, pool, seed_memories):
        """éªŒè¯å¤åˆç´¢å¼• (user_id, app_name, created_at) è¢«æ­£ç¡®ä½¿ç”¨"""
        user_id = seed_memories["user_id"]
        app_name = seed_memories["app_name"]
        start_time = datetime.now() - timedelta(days=30)
        end_time = datetime.now()

        async with pool.acquire() as conn:
            # ä½¿ç”¨ EXPLAIN ANALYZE æ£€æŸ¥æŸ¥è¯¢è®¡åˆ’
            plan = await conn.fetch("""
                EXPLAIN ANALYZE
                SELECT id, content, retention_score, created_at
                FROM memories
                WHERE user_id = $1
                  AND app_name = $2
                  AND created_at >= $3
                  AND created_at <= $4
                ORDER BY created_at DESC
                LIMIT 50
            """, user_id, app_name, start_time, end_time)

            plan_text = "\n".join(row[0] for row in plan)
            print(f"\n=== æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ ===\n{plan_text}")

            # éªŒè¯ä½¿ç”¨äº†ç´¢å¼•æ‰«æ
            assert "Index" in plan_text, "æŸ¥è¯¢åº”ä½¿ç”¨ç´¢å¼•æ‰«æ"
            assert "Seq Scan" not in plan_text, "ä¸åº”ä½¿ç”¨å…¨è¡¨æ‰«æ"


# è¿è¡Œ: pytest -v tests/hippocampus/test_episodic_performance.py
```

#### 6.4.3 å•å…ƒæµ‹è¯•æ¡†æ¶ (P2-4-3)

åˆ›å»º `tests/hippocampus/conftest.py` (pytest é…ç½®):

```python
"""
Hippocampus æµ‹è¯•é…ç½®

æä¾›æµ‹è¯• fixtures å’Œå…±äº«é…ç½®
"""

import asyncio
import os

import asyncpg
import pytest


@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="session")
async def test_db_pool():
    """
    åˆ›å»ºæµ‹è¯•æ•°æ®åº“è¿æ¥æ± 

    ç¯å¢ƒå˜é‡:
    - TEST_DATABASE_URL: æµ‹è¯•æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
    """
    database_url = os.getenv(
        "TEST_DATABASE_URL",
        "postgresql://user:pass@localhost/agent_db_test"
    )
    pool = await asyncpg.create_pool(database_url, min_size=2, max_size=10)
    yield pool
    await pool.close()


@pytest.fixture
async def clean_test_data(test_db_pool):
    """
    æµ‹è¯•åæ¸…ç†æ•°æ®

    åœ¨æ¯ä¸ªæµ‹è¯•ç»“æŸååˆ é™¤æµ‹è¯•æœŸé—´åˆ›å»ºçš„æ•°æ®
    """
    created_ids = {"threads": [], "memories": [], "facts": []}

    yield created_ids

    # æ¸…ç†
    async with test_db_pool.acquire() as conn:
        if created_ids["facts"]:
            await conn.execute(
                "DELETE FROM facts WHERE id = ANY($1::uuid[])",
                created_ids["facts"]
            )
        if created_ids["memories"]:
            await conn.execute(
                "DELETE FROM memories WHERE id = ANY($1::uuid[])",
                created_ids["memories"]
            )
        if created_ids["threads"]:
            await conn.execute(
                "DELETE FROM threads WHERE id = ANY($1::uuid[])",
                created_ids["threads"]
            )
```

åˆ›å»º `tests/hippocampus/test_consolidation_worker.py`:

```python
"""
MemoryConsolidationWorker å•å…ƒæµ‹è¯•
"""

import uuid
import pytest

from hippocampus.consolidation_worker import (
    MemoryConsolidationWorker,
    JobType,
    JobStatus,
)


class TestConsolidationWorker:
    """Consolidation Worker å•å…ƒæµ‹è¯•"""

    @pytest.fixture
    async def worker(self, test_db_pool):
        return MemoryConsolidationWorker(test_db_pool)

    async def test_fast_replay_generates_summary(self, worker, test_db_pool, clean_test_data):
        """Fast Replay åº”ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        # Setup: åˆ›å»ºæµ‹è¯• Thread å’Œ Events
        thread_id = str(uuid.uuid4())
        async with test_db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, 'test_user', 'test_app', '{}')
            """, uuid.UUID(thread_id))
            clean_test_data["threads"].append(uuid.UUID(thread_id))

            for i in range(3):
                await conn.execute("""
                    INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                    VALUES ($1, $2, 'message', $3, $4)
                """, uuid.UUID(thread_id),
                    'user' if i % 2 == 0 else 'agent',
                    f'{{"text": "å¯¹è¯å†…å®¹ {i}"}}', i)

        # Act
        job = await worker.consolidate(thread_id, JobType.FAST_REPLAY)

        # Assert
        assert job.status == JobStatus.COMPLETED
        assert "summary" in job.result
        assert job.result["summary"]["memory_id"]

    async def test_deep_reflection_extracts_facts(self, worker, test_db_pool, clean_test_data):
        """Deep Reflection åº”æå– Facts"""
        thread_id = str(uuid.uuid4())
        async with test_db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, 'test_user', 'test_app', '{}')
            """, uuid.UUID(thread_id))
            clean_test_data["threads"].append(uuid.UUID(thread_id))

            # åˆ›å»ºåŒ…å«åå¥½ä¿¡æ¯çš„å¯¹è¯
            await conn.execute("""
                INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                VALUES ($1, 'user', 'message', '{"text": "æˆ‘å–œæ¬¢åƒå¯¿å¸å’Œæ„å¤§åˆ©é¢"}', 0)
            """, uuid.UUID(thread_id))

        # Act
        job = await worker.consolidate(thread_id, JobType.DEEP_REFLECTION)

        # Assert
        assert job.status == JobStatus.COMPLETED
        # Facts å¯èƒ½ä¸ºç©º (å–å†³äº LLM æå–ç»“æœ)ï¼Œä½†ä»»åŠ¡åº”æˆåŠŸå®Œæˆ

    async def test_full_consolidation_runs_both_phases(self, worker, test_db_pool, clean_test_data):
        """Full Consolidation åº”æ‰§è¡Œä¸¤ä¸ªé˜¶æ®µ"""
        thread_id = str(uuid.uuid4())
        async with test_db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, 'test_user', 'test_app', '{}')
            """, uuid.UUID(thread_id))
            clean_test_data["threads"].append(uuid.UUID(thread_id))

            await conn.execute("""
                INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                VALUES ($1, 'user', 'message', '{"text": "æµ‹è¯•æ¶ˆæ¯"}', 0)
            """, uuid.UUID(thread_id))

        # Act
        job = await worker.consolidate(thread_id, JobType.FULL_CONSOLIDATION)

        # Assert
        assert job.status == JobStatus.COMPLETED
        assert "summary" in job.result  # Fast Replay ç»“æœ
        assert "facts" in job.result or "insights" in job.result  # Deep Reflection ç»“æœ
```

---

### 6.5. äº¤ä»˜ç‰©æ¸…å•

#### 6.5.1 Schema æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„                                            | æè¿°                    | çŠ¶æ€      |
| :-------------------------------------------------- | :---------------------- | :-------- |
| `src/cognizes/engine/schema/hippocampus_schema.sql` | Hippocampus æ‰©å±• Schema | ğŸ”² å¾…å¼€å§‹ |

#### 6.5.2 ä»£ç æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„                                                  | æè¿°            | çŠ¶æ€      |
| :-------------------------------------------------------- | :-------------- | :-------- |
| `src/cognizes/engine/hippocampus/__init__.py`             | æ¨¡å—åˆå§‹åŒ–      | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/consolidation_worker.py` | è®°å¿†å·©å›º Worker | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/retention_manager.py`    | è®°å¿†ä¿æŒç®¡ç†å™¨  | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/context_assembler.py`    | ä¸Šä¸‹æ–‡ç»„è£…å™¨    | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/memory_service.py`       | ADK é€‚é…å™¨      | ğŸ”² å¾…å¼€å§‹ |

#### 6.5.3 æµ‹è¯•æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„                                                     | æè¿°                       | çŠ¶æ€      |
| :----------------------------------------------------------- | :------------------------- | :-------- |
| `tests/integration/hippocampus/test_consolidation_worker.py` | Worker å•å…ƒæµ‹è¯•            | ğŸ”² å¾…å¼€å§‹ |
| `tests/integration/hippocampus/test_retention_manager.py`    | ä¿æŒç®¡ç†å™¨å•å…ƒæµ‹è¯•         | ğŸ”² å¾…å¼€å§‹ |
| `tests/integration/hippocampus/test_context_assembler.py`    | ä¸Šä¸‹æ–‡ç»„è£…å™¨å•å…ƒæµ‹è¯•       | ğŸ”² å¾…å¼€å§‹ |
| `tests/integration/hippocampus/test_memory_service.py`       | OpenMemoryService é›†æˆæµ‹è¯• | ğŸ”² å¾…å¼€å§‹ |

#### 6.5.4 ç›®å½•ç»“æ„

```
src/cognizes/engine/
â”œâ”€â”€ schema/
â”‚   â”œâ”€â”€ agent_schema.sql           # Phase 1 åŸºç¡€ Schema
â”‚   â””â”€â”€ hippocampus_schema.sql     # Phase 2 æ‰©å±• Schema
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ pulse/                     # Phase 1: The Pulse
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state_manager.py
â”‚   â”‚   â””â”€â”€ pg_notify_listener.py
â”‚   â””â”€â”€ hippocampus/               # Phase 2: The Hippocampus
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ consolidation_worker.py
â”‚       â”œâ”€â”€ retention_manager.py
â”‚       â”œâ”€â”€ context_assembler.py
â”‚       â””â”€â”€ memory_service.py
tests/
â”œâ”€â”€ pulse/
â”‚   â””â”€â”€ test_state_manager.py
â””â”€â”€ hippocampus/
    â”œâ”€â”€ test_consolidation_worker.py
    â”œâ”€â”€ test_retention_manager.py
    â”œâ”€â”€ test_context_assembler.py
    â””â”€â”€ test_memory_service.py
```

---

## 7. é£é™©ä¸ç¼“è§£ç­–ç•¥

### 7.1 æŠ€æœ¯é£é™©

| é£é™©                        | å½±å“ | æ¦‚ç‡ | ç¼“è§£ç­–ç•¥                                |
| :-------------------------- | :--- | :--- | :-------------------------------------- |
| **LLM æå–ä¸ç¨³å®š**          | ä¸­   | ä¸­   | è®¾è®¡å¥å£®çš„ JSON è§£æé€»è¾‘ï¼Œå®¹é”™å¤„ç†      |
| **å‘é‡æ£€ç´¢ç²¾åº¦ä¸è¶³**        | é«˜   | ä½   | å¼•å…¥ Reranker (Phase 3)ï¼Œè°ƒä¼˜ HNSW å‚æ•° |
| **è‰¾å®¾æµ©æ–¯è¡°å‡å‚æ•°ä¸åˆç†**  | ä¸­   | ä¸­   | æä¾›å¯é…ç½®å‚æ•°ï¼Œé€šè¿‡ A/B æµ‹è¯•è°ƒä¼˜       |
| **Context Window ç»„è£…åå·®** | ä¸­   | ä½   | å®ç°ç²¾ç¡® Token ç»Ÿè®¡ (ä½¿ç”¨ tiktoken)     |

### 7.2 å·¥ç¨‹é£é™©

| é£é™©                        | å½±å“ | æ¦‚ç‡ | ç¼“è§£ç­–ç•¥                               |
| :-------------------------- | :--- | :--- | :------------------------------------- |
| **Gemini API é™æµ**         | é«˜   | ä¸­   | å®ç°æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæ‰¹é‡å¤„ç†å‡å°‘è°ƒç”¨æ¬¡æ•° |
| **å¤§è§„æ¨¡è®°å¿†æ¸…ç†é˜»å¡**      | ä¸­   | ä½   | ä½¿ç”¨ `pg_cron` å®šæ—¶ä»»åŠ¡ï¼Œåˆ†æ‰¹åˆ é™¤      |
| **Phase 1 Schema å˜æ›´å½±å“** | ä½   | ä½   | ä½¿ç”¨å¤–é”®çº¦æŸï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§           |

---

## 8. é™„å½•

### 8.1 Prompt æ¨¡æ¿å‚è€ƒ

#### Fast Replay Prompt

```
ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸ºä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚

å¯¹è¯å†å²:
{conversation}

è¦æ±‚:
1. æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡ 200 å­—
2. ä¿ç•™ç”¨æˆ·çš„å…³é”®é—®é¢˜å’Œ Agent çš„æ ¸å¿ƒå›ç­”
3. ä¿ç•™ä»»ä½•é‡è¦çš„å†³ç­–æˆ–ç»“è®º
4. ä½¿ç”¨ç¬¬ä¸‰äººç§°æè¿°

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€æˆ–è§£é‡Šã€‚
```

#### Deep Reflection Prompt

```
ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·ç”»åƒåˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–ç”¨æˆ·çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬åå¥½ã€è§„åˆ™å’Œäº‹å®ã€‚

å¯¹è¯å†å²:
{conversation}

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹:
{
    "facts": [
        {
            "type": "preference|rule|profile",
            "key": "åå¥½/è§„åˆ™çš„å”¯ä¸€æ ‡è¯†ï¼Œå¦‚ food_preference",
            "value": {"å…·ä½“çš„åå¥½å†…å®¹"},
            "confidence": 0.0-1.0 çš„ç½®ä¿¡åº¦åˆ†æ•°
        }
    ],
    "insights": [
        {
            "content": "ä»å¯¹è¯ä¸­æç‚¼çš„æ·±å±‚æ´å¯Ÿ",
            "importance": "high|medium|low"
        }
    ]
}

è¦æ±‚:
1. åªæå–æ˜ç¡®è¡¨è¾¾æˆ–å¯é æ¨æ–­çš„ä¿¡æ¯
2. preference: ç”¨æˆ·çš„å–œå¥½ï¼ˆå¦‚é¥®é£Ÿã€é£æ ¼åå¥½ï¼‰
3. rule: ç”¨æˆ·è®¾å®šçš„è§„åˆ™ï¼ˆå¦‚"æ¯å‘¨äº”ä¸å¼€ä¼š"ï¼‰
4. profile: ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ï¼ˆå¦‚èŒä¸šã€ä½ç½®ï¼‰
5. å¦‚æœæ²¡æœ‰å¯æå–çš„ä¿¡æ¯ï¼Œè¿”å›ç©ºæ•°ç»„

è¯·åªè¾“å‡º JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚
```

### 8.2 è¡°å‡ç®—æ³•å‚æ•°è°ƒä¼˜æŒ‡å—

| åœºæ™¯               | æ¨è Î» (decay_rate) | æ¨èé˜ˆå€¼ (threshold) | è¯´æ˜                       |
| :----------------- | :------------------ | :------------------- | :------------------------- |
| **é«˜äº¤äº’é¢‘ç‡ App** | 0.15                | 0.15                 | åŠ é€Ÿé—å¿˜ï¼Œä¿æŒè®°å¿†æ–°é²œåº¦   |
| **ä½äº¤äº’é¢‘ç‡ App** | 0.05                | 0.05                 | å‡ç¼“é—å¿˜ï¼Œä¿ç•™æ›´å¤šå†å²è®°å¿† |
| **æ•æ„Ÿä¿¡æ¯åœºæ™¯**   | 0.3                 | 0.2                  | å¿«é€Ÿæ¸…ç†ï¼Œå‡å°‘éšç§é£é™©     |
| **çŸ¥è¯†ç§¯ç´¯åœºæ™¯**   | 0.02                | 0.02                 | é•¿æœŸä¿ç•™ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±     |

---

## 9. å‚è€ƒæ–‡çŒ®

<a id="ref1"></a>[1] Psychology Today, "Types of Memory," _Psychology Today_, 2024. [Online]. Available: https://www.psychologytoday.com/us/basics/memory/types-of-memory

<a id="ref2"></a>[2] LangChain, "LangGraph Memory Overview," _LangChain Documentation_, 2025. [Online]. Available: https://docs.langchain.com/oss/python/langgraph/memory

<a id="ref3"></a>[3] Google, "ADK Memory Documentation," _Google ADK Docs_, 2025. [Online]. Available: https://google.github.io/adk-docs/sessions/memory/

<a id="ref4"></a>[4] Google, "ADK Sessions Documentation," _Google ADK Docs_, 2025. [Online]. Available: https://google.github.io/adk-docs/sessions/

<a id="ref5"></a>[5] LangChain, "LangGraph Memory Agent," _GitHub Repository_, 2024. [Online]. Available: https://github.com/langchain-ai/memory-agent

<a id="ref6"></a>[6] LangChain, "LangGraph Memory Template," _GitHub Repository_, 2024. [Online]. Available: https://github.com/langchain-ai/memory-template

<a id="ref7"></a>[7] SII-GAIR, "Context Engineering 2.0: The Context of Context Engineering," _SII-GAIR Technical Report_, 2025.

<a id="ref8"></a>[8] H. Ebbinghaus, "Memory: A Contribution to Experimental Psychology," _Teachers College, Columbia University_, 1913.
