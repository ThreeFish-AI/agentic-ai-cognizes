---
id: the-hippocampus-implementation
sidebar_position: 2.0
title: Phase 2ï¼šThe Hippocampus éªŒè¯å®æ–½æ–¹æ¡ˆ
last_update:
  author: Aurelius Huang
  created_at: 2026-01-08
  updated_at: 2026-01-12
  version: 1.1
  status: Pending Review
tags:
  - The Hippocampus
  - Memory System
  - Implementation Plan
  - PostgreSQL
  - Zero-ETL
---

> [!NOTE]
>
> **æ–‡æ¡£å®šä½**ï¼šæœ¬æ–‡æ¡£æ˜¯ [000-roadmap.md](./000-roadmap.md) Phase 2 çš„è¯¦ç»†å·¥ç¨‹å®æ–½æ–¹æ¡ˆï¼Œç”¨äºæŒ‡å¯¼ã€Œ**The Hippocampus (ä»¿ç”Ÿè®°å¿†)**ã€çš„å®Œæ•´è½åœ°éªŒè¯å·¥ä½œã€‚æ¶µç›–æŠ€æœ¯è°ƒç ”ã€æ¶æ„è®¾è®¡ã€ä»£ç å®ç°ã€æµ‹è¯•éªŒè¯ç­‰å…¨æµç¨‹ã€‚
>
> **å‰ç½®ä¾èµ–**ï¼šæœ¬é˜¶æ®µä¾èµ– [010-the-pulse.md](./010-the-pulse.md) Phase 1 çš„å®Œæˆï¼Œéœ€å¤ç”¨å…¶ç»Ÿä¸€å­˜å‚¨åŸºåº§ (Unified Schema) å’Œä¼šè¯ç®¡ç†èƒ½åŠ›ã€‚

---

## 1. æ‰§è¡Œæ‘˜è¦

### 1.1 å®šä½ä¸ç›®æ ‡ (Phase 2)

**Phase 2: The Hippocampus** æ˜¯æ•´ä¸ªéªŒè¯è®¡åˆ’çš„è®°å¿†æ ¸å¿ƒé˜¶æ®µï¼Œå¯¹æ ‡äººç±»å¤§è„‘çš„**æµ·é©¬ä½“ (Hippocampus)** â€”â€” è´Ÿè´£å°†çŸ­æœŸè®°å¿†è½¬åŒ–ä¸ºé•¿æœŸè®°å¿†çš„å…³é”®è„‘åŒºã€‚æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

1. **å®ç° Zero-ETL è®°å¿†æ¶æ„**ï¼šæ‘’å¼ƒä¼ ç»Ÿ `Redis (App)` + `VectorDB (Mem)` çš„å‰²è£‚æ¶æ„ï¼ŒSession Log ä¸ Semantic Memory åŒåº“å­˜å‚¨
2. **éªŒè¯è®°å¿†å·©å›ºæœºåˆ¶**ï¼šå®ç°ä» Short-term åˆ° Long-term çš„æ— ç¼æµè½¬ï¼ˆFast Replay + Deep Reflectionï¼‰
3. **éªŒè¯ç”Ÿç‰©é—å¿˜æœºåˆ¶**ï¼šå®ç°è‰¾å®¾æµ©æ–¯è¡°å‡ç®—æ³•ï¼Œè‡ªåŠ¨æ¸…ç†ä½ä»·å€¼è®°å¿†
4. **éªŒè¯ Context Budgeting**ï¼šå®ç°åŠ¨æ€ä¸Šä¸‹æ–‡ç»„è£…ï¼Œç²¾å‡†æ§åˆ¶ Token é¢„ç®—

```mermaid
graph LR
    subgraph "Phase 2: The Hippocampus"
        F[Phase 1 åŸºåº§<br>Session Engine] --> H1[Memory Consolidation<br>è®°å¿†å·©å›º]
        F --> H2[Biological Retention<br>é—å¿˜ä¸ä¿æŒ]
        F --> H3[Context Budgeting<br>ä¸Šä¸‹æ–‡é¢„ç®—]
    end

    H1 & H2 & H3 --> V[Verification<br>éªŒæ”¶é€šè¿‡]
    V --> Phase3[Phase 3: Perception]

    style F fill:#065f46,stroke:#34d399,color:#fff
    style H1 fill:#7c2d12,stroke:#fb923c,color:#fff
    style H2 fill:#7c2d12,stroke:#fb923c,color:#fff
    style H3 fill:#7c2d12,stroke:#fb923c,color:#fff
```

### 1.2 æ ¸å¿ƒè®¤çŸ¥æ¶æ„ (Core Cognitive Architecture)

ä¸ºäº†æ„å»ºå…·å¤‡"é•¿æœŸå¿ƒæ™º"çš„ Agentï¼Œæˆ‘ä»¬å‚ç…§è®¤çŸ¥å¿ƒç†å­¦æ¨¡å‹ï¼Œè®¾è®¡äº†æ›´åŠ ä½“ç³»åŒ–çš„è®°å¿†ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿä¸ä»…æ˜¯æ•°æ®çš„å­˜å‚¨åº“ï¼Œæ›´æ˜¯ä¿¡æ¯æµè½¬ä¸å‡ç»´çš„åŠ å·¥å‚ã€‚

#### 1.2.1 è®°å¿†æ¨¡å‹ï¼šæ­£äº¤çš„ä¸‰ç»´è§†å›¾ (Static View)

æˆ‘ä»¬å°†é•¿æœŸè®°å¿†è§£è€¦ä¸ºä¸‰ä¸ªæ­£äº¤ç»´åº¦ï¼Œåˆ†åˆ«è§£å†³"ç»å†"ã€"çŸ¥è¯†"ä¸"æŠ€èƒ½"çš„æŒä¹…åŒ–é—®é¢˜ï¼š

| è®°å¿†ç»´åº¦ (Dimension)                  | è®¤çŸ¥éšå–» (Metaphor) | æ•°æ®å½¢æ€ (Schema)                               | æ ¸å¿ƒèŒèƒ½ (Function)                                            | å­˜å‚¨å®ä½“       |
| :------------------------------------ | :------------------ | :---------------------------------------------- | :------------------------------------------------------------- | :------------- |
| **Episodic Memory**<br>(æƒ…æ™¯è®°å¿†)     | **"è‡ªä¼ ä½“æµ"**      | **æ—¶åºç‰‡æ®µ** + å‘é‡åµŒå…¥<br>(Time-Series Chunks) | è®°å½•"å‘ç”Ÿäº†ä»€ä¹ˆ"ã€‚æä¾›è¿ç»­çš„äº¤äº’ä¸Šä¸‹æ–‡ï¼Œç»´æŠ¤å¯¹è¯çš„å†å²è¿è´¯æ€§ã€‚ | `memories`     |
| **Semantic Memory**<br>(è¯­ä¹‰è®°å¿†)     | **"æ¦‚å¿µç½‘ç»œ"**      | **ç»“æ„åŒ–äº‹å®** + å…³ç³»<br>(Structured Facts)     | è®°å½•"æ˜¯ä»€ä¹ˆ"ã€‚æ²‰æ·€ç”¨æˆ·åå¥½ã€ç”»åƒä¸ä¸–ç•ŒçŸ¥è¯†ï¼Œè·¨ä¼šè¯å¤ç”¨ã€‚       | `facts`        |
| **Procedural Memory**<br>(ç¨‹åºæ€§è®°å¿†) | **"è‚Œè‚‰è®°å¿†"**      | **æŒ‡ä»¤é›†** + ç‰ˆæœ¬æ§åˆ¶<br>(Instructions)         | è®°å½•"æ€ä¹ˆåš"ã€‚å›ºåŒ– Agent çš„è¡Œä¸ºæ¨¡å¼ã€SOP ä¸å·¥å…·ä½¿ç”¨ç­–ç•¥ã€‚      | `instructions` |

#### 1.2.2 åŠ¨æ€æœºåˆ¶ï¼šæµ·é©¬ä½“å¾ªç¯ (Dynamic View)

æ¨¡ä»¿äººè„‘çš„æµ·é©¬ä½“ (Hippocampus) åŠŸèƒ½ï¼Œæˆ‘ä»¬åœ¨ç³»ç»Ÿä¸­å¼•å…¥äº†**è®°å¿†å·©å›º (Consolidation)** ä¸**å†æ¿€æ´» (Reactivation)** çš„åŠ¨æ€å¾ªç¯ï¼š

```mermaid
graph TB
    subgraph WM_Scope ["Working Memory (å·¥ä½œè®°å¿†)"]
        direction TB
        WM[Context Window<br>å½“å‰ä¸Šä¸‹æ–‡]
    end

    subgraph Processes ["Cognitive Processes (è®¤çŸ¥)"]
        direction TB
        Encoding(Encoding<br>åŒé‡ç¼–ç )
        Retrieval(Retrieval<br>è”æƒ³æ£€ç´¢)
        Consolidation(Consolidation<br>åå°å·©å›º)
        Decay(Decay<br>ç”Ÿç‰©è¡°å‡)
    end

    subgraph Hippocampus ["Hippocampus (LTM)"]
        direction TB
        EM[(Episodic<br>æƒ…æ™¯è®°å¿†)]
        SM[(Semantic<br>è¯­ä¹‰è®°å¿†)]
        PM[(Procedural<br>ç¨‹åºæ€§è®°å¿†)]
    end

    %% Flows
    Input(User Input) --> WM
    WM -->|å®æ—¶å†™å…¥| Encoding
    Encoding -->|å­˜å‚¨| Hippocampus

    WM -.->|å¼‚æ­¥æç‚¼| Consolidation
    Consolidation -->|æå–äº‹å®| Hippocampus
    Consolidation -->|å‹ç¼©å½’æ¡£| Hippocampus

    Retrieval -->|æ³¨å…¥| WM
    Hippocampus -.->|å‘é‡ç´¢å¼•| Retrieval
    Hippocampus -.->|å®šæœŸæ¸…ç†| Decay

    style WM_Scope fill:#065f46,stroke:#34d399,color:#fff
    style Hippocampus fill:#1e3a5f,stroke:#60a5fa,color:#fff
    style Processes fill:#7c2d12,stroke:#fb923c,color:#fff
```

1. **Working Memory (å·¥ä½œè®°å¿†)**ï¼šä½œä¸ºç³»ç»Ÿçš„"å‰é¢å¶"ï¼Œæ¥æ”¶ç”¨æˆ·è¾“å…¥å¹¶ç»´æŠ¤å½“å‰çš„ä¸Šä¸‹æ–‡çª—å£ (`Context Window`)ã€‚
2. **Hippocampus (æµ·é©¬ä½“/LTM)**ï¼šé•¿æ—¶è®°å¿†çš„å­˜å‚¨ä¸­å¿ƒï¼Œç”±æƒ…æ™¯ (`Episodic`)ã€è¯­ä¹‰ (`Semantic`) å’Œç¨‹åºæ€§ (`Procedural`) ä¸‰ä¸ªæ­£äº¤çš„è®°å¿†åŒºç»„æˆã€‚
3. **Cognitive Processes (è®¤çŸ¥è¿‡ç¨‹)**ï¼šè¿æ¥ WM ä¸ LTM çš„åŠ¨æ€æœºåˆ¶ï¼Œé€šè¿‡ä»¥ä¸‹å››ä¸ªå…³é”®è¿‡ç¨‹ç»´æŒç³»ç»Ÿçš„"æ–°é™ˆä»£è°¢"ï¼š
   - **Encoding (ç¼–ç )**ï¼šå°†å®æ—¶çš„çŸ­æœŸäº¤äº’è½¬åŒ–ä¸ºå¯å­˜å‚¨çš„è®°å¿†ç—•è¿¹ã€‚
   - **Consolidation (å·©å›º)**ï¼šåœ¨åå°å¼‚æ­¥è¿è¡Œï¼Œå°†ç¢ç‰‡åŒ–çš„å¯¹è¯å†å²æç‚¼ä¸ºç»“æ„åŒ–çš„äº‹å®ä¸çŸ¥è¯†ã€‚
   - **Retrieval (æ£€ç´¢)**ï¼šåŸºäºè¯­ä¹‰ç›¸å…³æ€§ï¼Œåœ¨éœ€è¦æ—¶å°†æ²‰ç¡çš„é•¿æœŸè®°å¿†"å†æ¿€æ´»"å¹¶åŠ è½½å›å·¥ä½œè®°å¿†ã€‚
   - **Decay (è¡°å‡)**ï¼šæ¨¡æ‹Ÿç”Ÿç‰©é—å¿˜æœºåˆ¶ï¼Œå®šæœŸæ¸…ç†ä½ä»·å€¼æˆ–é•¿æœŸæœªè¢«è®¿é—®çš„è®°å¿†ï¼Œé˜²æ­¢è®°å¿†åº“è‡ƒè‚¿ã€‚

#### 1.2.3 å…³é”®ç‰¹æ€§ (Key Features)

1. **åŒé‡è·¯å¾„ (Dual Pathways)**:
   - **å¿«è·¯å¾„ (Fast Path)**: å®æ—¶å¯¹è¯æµç›´æ¥è¿›å…¥å·¥ä½œè®°å¿†ï¼Œä¿è¯å“åº”é€Ÿåº¦ã€‚
   - **æ…¢è·¯å¾„ (Slow Path)**: å¼‚æ­¥è¿›ç¨‹åœ¨åå°è¿›è¡Œ"åæ€"ä¸"å·©å›º"ï¼Œå°†ç¢ç‰‡åŒ–å¯¹è¯è½¬åŒ–ä¸ºç»“æ„åŒ–çŸ¥è¯†ã€‚

2. **è”æƒ³å¬å› (Associative Recall)**:
   - æ‘’å¼ƒå•çº¯çš„å…³é”®è¯åŒ¹é…ï¼Œåˆ©ç”¨ **Embedding Vector** å®ç°åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ¨¡ç³Šå¬å›ï¼Œæ¨¡æ‹Ÿ"è§¦æ™¯ç”Ÿæƒ…"çš„è®¤çŸ¥ä½“éªŒã€‚

3. **ç”Ÿç‰©æ€§é—å¿˜ (Biological Decay)**:
   - å¼•å…¥åŸºäº Ebbinghaus é—å¿˜æ›²çº¿çš„ `Retention Score`ï¼Œè®©ä½ä»·å€¼è®°å¿†éšæ—¶é—´è‡ªç„¶æ¶ˆé€€ï¼Œä¿æŒè®°å¿†åº“çš„"ä¿¡å™ªæ¯”"ä¸é²œæ´»æ€§ã€‚

### 1.3 æ‰§è¡Œå¯¼å›¾ (Execution Map)

ä¸ºäº†ç¡®ä¿ç³»ç»Ÿçš„**æ­£äº¤æ€§ (Orthogonality)** ä¸**è‡ªæ´½æ€§ (Self-consistency)**ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œè®¡åˆ’é‡æ„ä¸ºåˆ†å±‚é€’è¿›çš„å®æ–½è·¯å¾„ï¼Œç¡®ä¿æ¯ä¸€å±‚éƒ½åœ¨åšå®çš„åŸºç¡€ä¸Šæ„å»ºã€‚

#### 1.3.1 ä»»åŠ¡-æ–‡æ¡£é”šå®š

æˆ‘ä»¬å°†å·¥ç¨‹ä»»åŠ¡æ˜ å°„åˆ°æ¶æ„çš„ä¸‰ä¸ªæ­£äº¤åˆ‡é¢ï¼š**åŸºç¡€æ¶æ„ (Infra)**ã€**è®¤çŸ¥è¿‡ç¨‹ (Process)** ä¸ **æœåŠ¡é›†æˆ (Service)**ã€‚

> [!NOTE]
>
> **ç‰ˆæœ¬å¯¹ç…§**ï¼šæœ¬è®¡åˆ’å±äº **Engine Roadmap (Phase 2)**ï¼Œå¯¹åº” **Project Roadmap ([002-task-checklist](../002-task-checklist.md))** ä¸­çš„ **Phase 3 (T3.3 è®°å¿†æŒä¹…åŒ–)**ã€‚

| æ¶æ„åˆ‡é¢ (Layer)                  | æ ¸å¿ƒç»„ä»¶ (Component)               | å…³é”®èŒè´£ (Responsibility)                                                           | å¯¹åº”ä»»åŠ¡é›† (Engine)                                                           | å¯¹åº”ä»»åŠ¡é›† (Project)                                 |
| :-------------------------------- | :--------------------------------- | :---------------------------------------------------------------------------------- | :---------------------------------------------------------------------------- | :--------------------------------------------------- |
| **L0: Foundation**<br>é™æ€å­˜å‚¨å±‚  | **Unified Schema**<br>Repositories | å®šä¹‰è®°å¿†çš„ä¸‰ç»´å½¢æ€ (`Episodic`, `Semantic`, `Procedural`) åŠå…¶æŒä¹…åŒ–æ¥å£ã€‚          | **P2-2 (Part)**<br>- Schema Definition<br>- Repository Implementation         | **T3.3.1 - T3.3.3**<br>- çŸ­æœŸ/é•¿æœŸ/æƒ…æ™¯è®°å¿†å­˜å‚¨      |
| **L1: Inflow**<br>åŠ¨æ€ç”Ÿæˆå±‚      | **Consolidation Worker**           | å®ç°è®°å¿†çš„**åŒé‡ç¼–ç **ï¼š<br>- Fast Replay (æ‘˜è¦)<br>- Deep Reflection (äº‹å®æå–)    | **P2-2 (Main)**<br>- Worker Skeleton<br>- Prompt Engineering<br>- Async Queue | **T3.3.7**<br>- è®°å¿†å›ºåŒ–æœºåˆ¶                         |
| **L2: Lifecycle**<br>åŠ¨æ€ç»´æŠ¤å±‚   | **Retention Manager**              | å®ç°è®°å¿†çš„**ç”Ÿç‰©å‘¨æœŸ**ï¼š<br>- Ebbinghaus Decay (é—å¿˜)<br>- Context Budgeting (ç»„è£…) | **P2-3**<br>- Scoring Algorithm<br>- Window Assembly                          | **T3.3.7**<br>- è‡ªåŠ¨ç»´æŠ¤ä¸æ¸…ç†                       |
| **L3: Integration**<br>æœåŠ¡é€‚é…å±‚ | **Memory Service**                 | å®ç°ä¸ ADK çš„**æ ‡å‡†å¥‘çº¦**ï¼š<br>- Interface Adapter<br>- Hybrid Search               | **P2-4**<br>- ADK Integration<br>- E2E Verification                           | **T3.3.5, T3.3.6**<br>- è®°å¿†ç®¡ç†å™¨<br>- è®°å¿†æ£€ç´¢åŠŸèƒ½ |

#### 1.3.2 å·¥æœŸå®‰æ’ (2.5 Days)

| é˜¶æ®µ          | é‡Œç¨‹ç¢‘å®šä¹‰ (Milestone)                   | å…³é”®äº¤ä»˜ç‰© (Deliverables)                                             | é¢„ä¼°å·¥æœŸ |
| :------------ | :--------------------------------------- | :-------------------------------------------------------------------- | :------- |
| **Phase 2.1** | **Cognitive Alignment**<br>(è®¤çŸ¥å¯¹é½)    | âœ… è®°å¿†æœºåˆ¶è°ƒç ”æŠ¥å‘Š<br>âœ… æŠ€æœ¯é€‰å‹å¯¹æ¯”è¡¨                              | 0.25 Day |
| **Phase 2.2** | **Memory Formation**<br>(è®°å¿†ç”Ÿæˆæœºåˆ¶)   | âœ… Hippocampus Schema DDL<br>âœ… `ConsolidationWorker` (Alpha)         | 1.0 Day  |
| **Phase 2.3** | **Memory Dynamics**<br>(è®°å¿†åŠ¨åŠ›å­¦)      | âœ… `retention_score` ç®—æ³•å®ç°<br>âœ… `get_context_window` å­˜å‚¨è¿‡ç¨‹     | 0.5 Day  |
| **Phase 2.4** | **Cortex Integration**<br>(å…¨è„‘é›†æˆéªŒæ”¶) | âœ… `PostgresMemoryService` (ADK compliant)<br>âœ… è®°å¿†ç³»ç»ŸéªŒæ”¶æµ‹è¯•æŠ¥å‘Š | 0.25 Day |
| **Phase 2.5** | æµ‹è¯•                                     | æµ‹è¯•ä»£ç  + éªŒè¯æ–‡æ¡£                                                   | 0.5 Day  |

---

## 2. æ ¸å¿ƒå‚è€ƒæ¨¡å‹ï¼šä»¿ç”Ÿè®°å¿†æœºåˆ¶

### 2.1 Google ADK

#### 2.1.1 å¯¹æ ‡åˆ†æï¼šGoogle ADK MemoryService

åŸºäº Google ADK å®˜æ–¹æ–‡æ¡£<sup>[[3]](#ref3)</sup>ï¼Œæˆ‘ä»¬å°†å¤åˆ»å…¶æ ¸å¿ƒèƒ½åŠ›ï¼Œå¹¶æ˜ å°„åˆ° PostgreSQL ç”Ÿæ€ï¼š

| ADK æ ¸å¿ƒæ¦‚å¿µ      | å®šä¹‰                          | æˆ‘ä»¬çš„å¤åˆ»å®ç° (PostgreSQL)                 | é”šå®šä»£ç                                                                                    |
| :---------------- | :---------------------------- | :------------------------------------------ | :----------------------------------------------------------------------------------------- |
| **MemoryService** | è·¨ä¼šè¯çš„å¯æœç´¢çŸ¥è¯†åº“ç®¡ç†æ¥å£  | `PostgresMemoryService`                     | [memory_service.py](file:///src/cognizes/adapters/postgres/memory_service.py)              |
| **Memory**        | ä»å¯¹è¯ä¸­æå–çš„ç»“æ„åŒ–çŸ¥è¯†ç‰‡æ®µ  | `memories` è¡¨ (å‘é‡)<br>`facts` è¡¨ (ç»“æ„åŒ–) | [schema/hippocampus_schema.sql](file:///src/cognizes/engine/schema/hippocampus_schema.sql) |
| **add_session**   | å°† Session è½¬åŒ–ä¸ºå¯æœç´¢çš„è®°å¿† | `ConsolidationWorker` (å¼‚æ­¥)                | [consolidation_worker.py](file:///src/cognizes/engine/hippocampus/consolidation_worker.py) |
| **search_memory** | åŸºäº Query æ£€ç´¢ç›¸å…³è®°å¿†       | æ··åˆæ£€ç´¢ (Vector + JSONB)                   | `search_memory()`                                                                          |

#### 2.1.2 æ¥å£å¥‘çº¦ (Interface Contract)

æˆ‘ä»¬éµå¾ª ADK çš„ `BaseMemoryService` æ ‡å‡†æ¥å£ï¼Œç¡®ä¿ **Drop-in Compatible**ï¼š

```python
class BaseMemoryService(ABC):
    @abstractmethod
    async def add_session_to_memory(self, session: Session) -> None:
        """Trigger: å¼‚æ­¥è§¦å‘è®°å¿†å·©å›º (Inflow)"""
        ...

    @abstractmethod
    async def search_memory(self, *, app_name: str, user_id: str, query: str) -> SearchMemoryResponse:
        """Trigger: å®æ—¶æ£€ç´¢ç›¸å…³è®°å¿† (Retrieval)"""
        ...
```

#### 2.1.3 å·¥ä½œæµå‚è€ƒ (Workflow Reference)

Memory Bank çš„æ ¸å¿ƒä»·å€¼åœ¨äºå°† **å†™å…¥ (Consolidation)** ä¸ **è¯»å– (Retrieval)** è§£è€¦ï¼š

```mermaid
sequenceDiagram
    participant User
    participant Agent
    participant Session as SessionService
    participant Memory as MemoryService
    participant Worker as ConsolidationWorker

    Note over Agent, Memory: Hot Path (å®æ—¶å“åº”)
    User->>Agent: ç”¨æˆ·æ¶ˆæ¯
    Agent->>Session: append_event()
    Agent->>Memory: search_memory(query)
    Memory-->>Agent: ç›¸å…³è®°å¿† (Context)
    Agent->>User: è¿”å›å›å¤

    Note over Session, Worker: Background Path (å¼‚æ­¥å·©å›º)
    Session-)Memory: add_session_to_memory()
    Memory-)Worker: dispatch_job()
    Worker->>Worker: LLM Extraction (Facts/Insights)
    Worker->>Memory: Persist (Vector + Struct)
```

**å…³é”®æ´å¯Ÿ**ï¼š

1. **æ­£äº¤æ€§**: è®°å¿†ç”Ÿæˆ (Worker) ä¸ è®°å¿†ä½¿ç”¨ (Agent) äº’ä¸é˜»å¡ã€‚
2. **åŒå‘æµ**: Session æ•°æ®æµå…¥ Memoryï¼ŒMemory çŸ¥è¯†æµå› Agentã€‚
3. **ç™½ç›’åŒ–**: æˆ‘ä»¬å°†åŸç‰ˆé»‘ç›’çš„ Vertex AI é€»è¾‘æ›¿æ¢ä¸ºå¯è§‚æµ‹çš„ `ConsolidationWorker`ã€‚

#### 2.1.4 å†™å…¥ç­–ç•¥ (Writing Strategy)

ç»“åˆ LangGraph çš„è®¾è®¡ç†å¿µ<sup>[[2]](#ref2)</sup>ï¼Œæˆ‘ä»¬åœ¨æ—¶åºå›¾ä¸­æ˜ç¡®åŒºåˆ†äº†ä¸¤ç§å†™å…¥è·¯å¾„ï¼š

| è·¯å¾„ (Path)    | æ¨¡å¼ (Mode)  | å¯¹åº”æœºåˆ¶                       | ä¼˜åŠ¿ (Pros)                   | åŠ£åŠ¿ (Cons)                |
| :------------- | :----------- | :----------------------------- | :---------------------------- | :------------------------- |
| **Hot Path**   | åŒæ­¥ (Sync)  | `append_event()` (Session)     | ç«‹å³ä¸€è‡´æ€§ (Read-Your-Writes) | å¢åŠ ç”¨æˆ·ç­‰å¾…å»¶è¿Ÿ           |
| **Background** | å¼‚æ­¥ (Async) | `ConsolidationWorker` (Memory) | é«˜ååï¼Œä¸é˜»å¡ç”¨æˆ·ä½“éªŒ        | å­˜åœ¨çŸ­æš‚çš„"è®°å¿†ä¸ä¸€è‡´çª—å£" |

**æˆ‘ä»¬çš„å†³ç­–**ï¼š

- **Fast Replay**: ä½œä¸ºçƒ­è·¯å¾„çš„è¡¥å……ï¼Œé€šè¿‡ Session å¿«é€Ÿå›æº¯ã€‚
- **Deep Reflection**: **å¿…é¡»å¼‚æ­¥**ã€‚å› ä¸º Fact Extraction éœ€è¦æ˜‚è´µçš„ LLM æ¨ç†ï¼Œç»ä¸èƒ½é˜»å¡ç”¨æˆ·å¯¹è¯ã€‚

### 2.2 LangGraph Memory è®¾è®¡æ¨¡å¼

LangGraph çš„ Memory è®¾è®¡ä¸ºæˆ‘ä»¬æä¾›äº†é‡è¦çš„**å®ç°å‚è€ƒ**<sup>[[2]](#ref2)</sup>ã€‚

#### 2.2.1 æŒä¹…åŒ–æœºåˆ¶å¯¹ç…§

LangGraph æä¾›ä¸¤å¥—äº’è¡¥çš„æŒä¹…åŒ–æœºåˆ¶ï¼Œä¸æˆ‘ä»¬çš„å®ç°å½¢æˆæ¸…æ™°æ˜ å°„ï¼š

| LangGraph æœºåˆ¶   | å­˜å‚¨èŒƒå›´    | å¯¹åº”æˆ‘ä»¬çš„å®ç°                               | é”šå®šè¡¨/æ¨¡å—                         |
| :--------------- | :---------- | :------------------------------------------- | :---------------------------------- |
| **Checkpointer** | å•ä¸ª Thread | Phase 1 çŸ­æœŸè®°å¿† (Session State)             | `threads`, `events`                 |
| **Store**        | è·¨ Thread   | Phase 2 é•¿æœŸè®°å¿† (Consolidated Memory/Facts) | `memories`, `facts`, `instructions` |

#### 2.2.2 ä¸‰ç±»è®°å¿†çš„å®ç°å‚è€ƒ

LangGraph çš„ä¸‰ç±»è®°å¿†åœ¨æˆ‘ä»¬çš„æ–¹æ¡ˆä¸­é€šè¿‡**ç»Ÿä¸€çš„ Repository æ¥å£**å®ç°ï¼š

| è®°å¿†ç±»å‹                  | LangGraph ç”¨é€”       | æˆ‘ä»¬çš„å­˜å‚¨è¡¨   | Repository æ¥å£                                                                  |
| :------------------------ | :------------------- | :------------- | :------------------------------------------------------------------------------- |
| **Semantic** (è¯­ä¹‰è®°å¿†)   | ç”¨æˆ·åå¥½ã€Profile    | `facts`        | [FactsRepository](file:///src/cognizes/core/repositories/facts.py)               |
| **Episodic** (æƒ…æ™¯è®°å¿†)   | å¯¹è¯åˆ‡ç‰‡ã€Few-shot   | `memories`     | [MemoryRepository](file:///src/cognizes/core/repositories/memory.py)             |
| **Procedural** (ç¨‹åºè®°å¿†) | Agent æŒ‡ä»¤ã€è¡Œä¸ºè§„åˆ™ | `instructions` | [InstructionsRepository](file:///src/cognizes/core/repositories/instructions.py) |

<details>
<summary>ğŸ“– LangGraph åŸå§‹ä»£ç å‚è€ƒ (ç‚¹å‡»å±•å¼€)</summary>

```python
# Semantic Memory: ç”¨æˆ·åå¥½å­˜å‚¨
store.put(namespace=(user_id, "preferences"), key="food", value={"likes": ["pizza"], "dislikes": ["spicy"]})

# Episodic Memory: æƒ…æ™¯è®°å¿†æ£€ç´¢
memories = store.search(namespace=(user_id, "episodes"), query="similar task")

# Procedural Memory: Agent è‡ªæˆ‘è¿›åŒ–
store.put(("agent_instructions",), "main", {"instructions": new_instructions})
```

</details>

### 2.3 ç»¼åˆå¯¹æ¯”åˆ†æ (Comparative Analysis)

åŸºäºä¸Šè¿°è°ƒç ”ï¼Œæˆ‘ä»¬å°†å–é•¿è¡¥çŸ­ï¼Œæ„å»º **The Hippocampus** å¼•æ“ï¼š

| ç»´åº¦         | Google ADK MemoryService       | LangGraph Store                  | Open Memory Engine (æˆ‘ä»¬)        |
| :----------- | :----------------------------- | :------------------------------- | :------------------------------- |
| **å­˜å‚¨åç«¯** | Vertex AI Vector Search        | InMemory / Postgres / Redis      | PostgreSQL + PGVector            |
| **è®°å¿†ç±»å‹** | å•ä¸€ Memory ç±»å‹               | Semantic / Episodic / Procedural | ä¸‰ç§è®°å¿†ç±»å‹ + ç»Ÿä¸€å­˜å‚¨          |
| **å†™å…¥æœºåˆ¶** | å¼‚æ­¥ `add_session_to_memory()` | Hot Path / Background å¯é€‰       | Fast Replay + Async Worker       |
| **æ£€ç´¢æ–¹å¼** | `search_memory()` å‘é‡æ£€ç´¢     | `store.search()` è¯­ä¹‰æ£€ç´¢        | æ··åˆæ£€ç´¢ (Vector + JSONB + Time) |
| **å·©å›ºç­–ç•¥** | LLM æå– â†’ è‡ªåŠ¨å‘é‡åŒ–          | åº”ç”¨å±‚æ§åˆ¶                       | ä¸¤é˜¶æ®µå·©å›º + è‰¾å®¾æµ©æ–¯è¡°å‡        |
| **å¼€æ”¾ç¨‹åº¦** | é»‘ç›’ (ä¾èµ– Vertex AI)          | ç™½ç›’ (å®Œå…¨å¯æ§)                  | ç™½ç›’ (PostgreSQL åŸç”Ÿ)           |

### 2.4 è°ƒç ”äº¤ä»˜ç‰©æ‘˜è¦

> [!NOTE]
> æœ¬èŠ‚æ±‡æ€»ä»»åŠ¡ **P2-1-1 ~ P2-1-5** çš„è°ƒç ”æˆæœã€‚è¯¦ç»†çš„æŠ€æœ¯åˆ†æå·²åœ¨å‰æ–‡å±•å¼€ï¼Œæ­¤å¤„ä»…åšç´¢å¼•ç´¢å¼•ä¸äº¤ä»˜ç¡®è®¤ã€‚

#### 2.4.1 æ ¸å¿ƒäº¤ä»˜ç‰©ç´¢å¼•

| ä»»åŠ¡ ID    | ä»»åŠ¡æè¿°                         | äº¤ä»˜å†…å®¹ç´¢å¼•                                                 |
| :--------- | :------------------------------- | :----------------------------------------------------------- |
| **P2-1-1** | ADK `MemoryService` æ¥å£åˆ†æ     | è§ [2.1.2 æ¥å£å¥‘çº¦](#212-æ¥å£å¥‘çº¦-interface-contract)        |
| **P2-1-2** | Memory Bank å·¥ä½œæµåˆ†æ           | è§ [2.1.3 å·¥ä½œæµå‚è€ƒ](#213-å·¥ä½œæµå‚è€ƒ-workflow-reference)    |
| **P2-1-3** | LangGraph `Checkpointer` åˆ†æ    | è§ [2.2.1 æŒä¹…åŒ–æœºåˆ¶å¯¹ç…§](#221-æŒä¹…åŒ–æœºåˆ¶å¯¹ç…§)               |
| **P2-1-4** | LangGraph `Store` è·¨ Thread åˆ†æ | è§ [2.2.2 ä¸‰ç±»è®°å¿†çš„å®ç°å‚è€ƒ](#222-ä¸‰ç±»è®°å¿†çš„å®ç°å‚è€ƒ)       |
| **P2-1-5** | ç»¼åˆå¯¹æ¯”åˆ†æè¡¨                   | è§ [2.3 ç»¼åˆå¯¹æ¯”åˆ†æ](#23-ç»¼åˆå¯¹æ¯”åˆ†æ-comparative-analysis) |

#### 2.4.2 å…³é”®æŠ€æœ¯é€‰å‹ç¡®è®¤

åŸºäºä¸Šè¿°è°ƒç ”ï¼Œæˆ‘ä»¬ç¡®è®¤ä»¥ä¸‹æ ¸å¿ƒæŠ€æœ¯æ ˆæ˜ å°„ï¼š

| ç»„ä»¶å±‚çº§         | Google ADK (åŸç‰ˆ)       | The Hippocampus (æˆ‘ä»¬)          | é€‰å‹ä¾æ®                                     |
| :--------------- | :---------------------- | :------------------------------ | :------------------------------------------- |
| **Vector Store** | Vertex AI Vector Search | **PostgreSQL + PGVector**       | ç»Ÿä¸€æŠ€æœ¯æ ˆï¼Œå‡å°‘è¿ç»´ç†µå¢ (Entropy Reduction) |
| **Embedding**    | `textembedding-gecko`   | **Gemini `text-embedding-005`** | é«˜æ€§èƒ½ä¸”æˆæœ¬å¯æ§                             |
| **Extraction**   | Gemini Pro              | **Gemini 3.0 Flash**            | æ›´å¿«çš„æ¨ç†é€Ÿåº¦ï¼Œé€‚åˆåå°æ‰¹å¤„ç†               |
| **Index Algo**   | ScaNN                   | **HNSW**                        | PGVector æ ‡é…ï¼Œå…¼é¡¾å¬å›ç‡ä¸æ€§èƒ½              |

---

## 3. æ¶æ„è®¾è®¡ï¼šHippocampus Schema æ‰©å±•

### 3.1 Schema æ‰©å±•è®¾è®¡

åœ¨ Phase 1 çš„ Unified Schema åŸºç¡€ä¸Šï¼Œæ–°å¢ä»¥ä¸‹è®°å¿†ç›¸å…³è¡¨ï¼š

```mermaid
erDiagram
    %% Core Relationships
    threads ||--o{ events : contains
    threads ||--o{ consolidation_jobs : triggers

    %% Process Flow: Inflow
    consolidation_jobs ||--o{ memories : generates
    consolidation_jobs ||--o{ facts : extracts
    consolidation_jobs }o..o| instructions : "updates (implicit)"

    %% Data Ownership (FKs)
    threads ||--o{ memories : "source of"
    threads ||--o{ facts : "source of"

    memories {
        uuid id PK "è®°å¿†å”¯ä¸€æ ‡è¯†"
        uuid thread_id FK "æ¥æºä¼šè¯"
        varchar user_id "ç”¨æˆ·æ ‡è¯†"
        varchar app_name "åº”ç”¨åç§°"
        varchar memory_type "è®°å¿†ç±»å‹: episodic/semantic"
        text content "è®°å¿†å†…å®¹"
        vector embedding "å‘é‡åµŒå…¥ (1536ç»´)"
        jsonb metadata "å…ƒæ•°æ®"
        float retention_score "ä¿ç•™åˆ†æ•°"
        integer access_count "è®¿é—®æ¬¡æ•°"
        timestamp last_accessed_at "æœ€åè®¿é—®æ—¶é—´"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }

    facts {
        uuid id PK "äº‹å®å”¯ä¸€æ ‡è¯†"
        uuid thread_id FK "æ¥æºä¼šè¯"
        varchar user_id "ç”¨æˆ·æ ‡è¯†"
        varchar app_name "åº”ç”¨åç§°"
        varchar fact_type "äº‹å®ç±»å‹: preference/rule/profile"
        varchar key "äº‹å®é”®"
        jsonb value "äº‹å®å€¼"
        vector embedding "å‘é‡åµŒå…¥"
        float confidence "ç½®ä¿¡åº¦"
        timestamp valid_from "ç”Ÿæ•ˆæ—¶é—´"
        timestamp valid_until "å¤±æ•ˆæ—¶é—´"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }

    consolidation_jobs {
        uuid id PK "ä»»åŠ¡å”¯ä¸€æ ‡è¯†"
        uuid thread_id FK "ç›®æ ‡ä¼šè¯"
        varchar status "çŠ¶æ€: pending/running/completed/failed"
        varchar job_type "ä»»åŠ¡ç±»å‹: fast_replay/deep_reflection"
        jsonb result "å¤„ç†ç»“æœ"
        text error "é”™è¯¯ä¿¡æ¯"
        timestamp started_at "å¼€å§‹æ—¶é—´"
        timestamp completed_at "å®Œæˆæ—¶é—´"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }

    instructions {
        uuid id PK "æŒ‡ä»¤å”¯ä¸€æ ‡è¯†"
        varchar app_name "åº”ç”¨åç§°"
        varchar instruction_key "æŒ‡ä»¤é”®"
        text content "æŒ‡ä»¤å†…å®¹"
        integer version "ç‰ˆæœ¬å·"
        jsonb metadata "å…ƒæ•°æ®"
        timestamp created_at "åˆ›å»ºæ—¶é—´"
    }
```

### 3.2 è®°å¿†æ¨¡å‹èŒè´£è¾¹ç•Œ (Memory Responsibilities)

éµå¾ª **AGENTS.md** çš„ **Orthogonal Decomposition (æ­£äº¤åˆ†è§£)** åŸåˆ™ï¼Œæˆ‘ä»¬å°†ä¸‰ç§è®°å¿†ä¸¥æ ¼æ˜ å°„åˆ°ä¸‰å¼ è¡¨ä¸­ï¼Œç¡®ä¿èŒè´£äº’ä¸é‡å ä¸”è‡ªæ´½ã€‚

#### 3.2.1 èŒè´£æ­£äº¤çŸ©é˜µ

| ç»´åº¦         | **memories** (æƒ…æ™¯æµ)            | **facts** (äº‹å®æ€)                        | **instructions** (è¡Œä¸ºè§„)      |
| :----------- | :------------------------------- | :---------------------------------------- | :----------------------------- |
| **æ ¸å¿ƒèŒè´£** | **Store Experience** (ç»å†)      | **Store Knowledge** (çŸ¥è¯†)                | **Store Behavior** (è¡Œä¸º)      |
| **æ•°æ®å½¢æ€** | **Unstructured Text** (éç»“æ„åŒ–) | **Structured KV** (ç»“æ„åŒ–)                | **System Prompt** (æŒ‡ä»¤æ–‡æœ¬)   |
| **æ—¶åºç‰¹å¾** | **Time-Series** (æµå¼è¿½åŠ )       | **Current State** (çŠ¶æ€è¦†ç›–)              | **Versioned** (ç‰ˆæœ¬æ§åˆ¶)       |
| **å…¸å‹å†…å®¹** | å¯¹è¯åˆ‡ç‰‡ã€é˜¶æ®µæ€§æ€»ç»“ (`summary`) | ç”¨æˆ·ç”»åƒ (`profile`)ã€åå¥½ (`preference`) | Agent äººè®¾ã€äº¤äº’å‡†åˆ™           |
| **æ£€ç´¢æ¨¡å¼** | è¯­ä¹‰ç›¸ä¼¼åº¦ (`search_vector`)     | ç²¾ç¡®é”®å€¼åŒ¹é… + è¯­ä¹‰ (`get` + `search`)    | é”®å€¼åŠ è½½ (`load_instructions`) |

#### 3.2.2 å…³äº `memory_type='semantic'` çš„æ¶ˆæ­§

åœ¨ `memories` è¡¨çš„å®šä¹‰ä¸­ï¼Œ`memory_type` åŒ…å« `semantic` æšä¸¾ï¼Œè¿™ä¸ `facts` è¡¨çœ‹ä¼¼é‡å ã€‚ä¸ºäº†æ¶ˆé™¤æ­§ä¹‰ (Entropy Reduction)ï¼Œæˆ‘ä»¬åšå‡ºä»¥ä¸‹ **æ˜ç¡®ç•Œå®š**ï¼š

1. **`facts` è¡¨ (Primary Semantic)**:
   - **å®šä¹‰**: ç»è¿‡**æ·±åº¦å›ºåŒ–**ã€**å»é‡**ä¸”**ç»“æ„åŒ–**çš„ç¡®åˆ‡çŸ¥è¯†ã€‚
   - **åœºæ™¯**: "ç”¨æˆ·ä¸å–œæ¬¢åƒè¾£", "ç”¨æˆ·çš„èŒä¸šæ˜¯å·¥ç¨‹å¸ˆ"ã€‚è¿™æ˜¯ç³»ç»Ÿè®¤ä¸º"ä¸ºçœŸ"çš„äº‹å®ã€‚

2. **`memories` è¡¨ä¸­çš„ `semantic` ç±»å‹ (Secondary/Transient)**:
   - **å®šä¹‰**: å°šæœªå®Œå…¨ç»“æ„åŒ–ï¼Œæˆ–éš¾ä»¥ç”¨ KV è¡¨è¾¾çš„**æ³›åŒ–çŸ¥è¯†ç‰‡æ®µ**ã€‚ä¹Ÿå¯ä»¥ç†è§£ä¸º"å…³äºæŸä¸ªçŸ¥è¯†ç‚¹çš„éç»“æ„åŒ–æè¿°"ã€‚
   - **åœºæ™¯**: "ç”¨æˆ·è¯¦ç»†é˜è¿°äº†ä»–å¯¹äººå·¥æ™ºèƒ½æœªæ¥çš„çœ‹æ³•"ï¼ˆä¸€æ®µ 500 å­—çš„è§‚ç‚¹ï¼‰ã€‚è¿™ä¸é€‚åˆå­˜ä¸º KV Factï¼Œä½†å®ƒæ˜¯ä¸€æ®µå…·å¤‡"è¯­ä¹‰ä»·å€¼"çš„è®°å¿†ï¼Œæ¯”å•çº¯çš„"å¯¹è¯åˆ‡ç‰‡ (`episodic`)"æ›´æŠ½è±¡ã€‚
   - **æ¨èç­–ç•¥**: åˆæœŸ **ä¼˜å…ˆä½¿ç”¨ `episodic` å’Œ `summary`**ã€‚ä»…å½“éœ€è¦å­˜å‚¨å¤§æ®µéç»“æ„åŒ–çŸ¥è¯†ï¼ˆå¦‚æ–‡æ¡£ç‰‡æ®µ RAGï¼‰æ—¶ä½¿ç”¨ `semantic` ç±»å‹ã€‚æ­¤æ—¶ `memories` å……å½“äº†è½»é‡çº§çš„ Vector DBã€‚

> [!TIP]
>
> **è®¾è®¡å¿ƒæ³•**:
>
> - **memories** æ˜¯ Agent çš„ **"æ—¥è®°æœ¬"** (å™äº‹)ã€‚
> - **facts** æ˜¯ Agent çš„ **"æ¡£æ¡ˆåº“"** (ç”»åƒ)ã€‚
> - **instructions** æ˜¯ Agent çš„ **"å‘˜å·¥æ‰‹å†Œ"** (è§„åˆ™)ã€‚

### 3.3 æ ¸å¿ƒ Schema å®šä¹‰ (Single Source of Truth)

ä¸ºäº†éµå¾ª **Entropy Reduction (ç†µå‡)** åŸåˆ™ï¼Œé¿å…æ–‡æ¡£ä¸ä»£ç çš„ driftï¼Œæ‰€æœ‰çš„ DDL å’Œ SQL å‡½æ•°å®šä¹‰å·²æ”¶æ•›è‡³ç»Ÿä¸€çš„ Schema æ–‡ä»¶ç»´æŠ¤ã€‚

> [!IMPORTANT]
>
> **Source of Truth**: [src/cognizes/engine/schema/hippocampus_schema.sql](file:///src/cognizes/engine/schema/hippocampus_schema.sql)
>
> è¯¥æ–‡ä»¶åŒ…å«ï¼š
>
> 1. **Tables**: `memories`, `facts`, `consolidation_jobs`, `instructions`
> 2. **Indexes**: PGVector HNSW ç´¢å¼•ä¸ B-Tree è¾…åŠ©ç´¢å¼•
> 3. **Functions**: `calculate_retention_score` (è‰¾å®¾æµ©æ–¯è¡°å‡), `cleanup_low_value_memories` (è‡ªåŠ¨æ¸…ç†), `get_context_window` (ä¸Šä¸‹æ–‡ç»„è£…)

---

## 4. å®æ–½æŒ‡å—

### 4.1 Step 1: è®°å¿† Schema æ‰©å±•éƒ¨ç½²

#### 4.1.1 éƒ¨ç½²æ‰§è¡Œ (Deployment Execution)

æœ¬ Schema è®¾è®¡å…·å¤‡ **å¹‚ç­‰æ€§ (Idempotency)**ï¼Œå¯é‡å¤æ‰§è¡Œã€‚

```bash
# ç¡®ä¿ä½äºé¡¹ç›®æ ¹ç›®å½•ï¼Œå¹¶æ­£ç¡®é…ç½® PSQL ç¯å¢ƒå˜é‡
# export PGPASSWORD=your_password

# æ‰§è¡Œéƒ¨ç½² (åŒ…å« Tables, Indexes, Functions)
psql -d 'cognizes-engine' -f src/cognizes/engine/schema/hippocampus_schema.sql
```

#### 4.1.2 éªŒæ”¶éªŒè¯ (Verification SOP)

æ‰§è¡Œä»¥ä¸‹ SOP ç¡®ä¿å¯¹è±¡åˆ›å»ºæ­£ç¡®ï¼š

```bash
# 1. éªŒè¯æ ¸å¿ƒè¡¨ç»“æ„ (4 Tables)
psql -d 'cognizes-engine' -c "\dt" | grep -E 'memories|facts|consolidation_jobs|instructions'

# 2. éªŒè¯å‘é‡ç´¢å¼• (HNSW)
psql -d 'cognizes-engine' -c "SELECT indexname, indexdef FROM pg_indexes WHERE indexname = 'idx_memories_embedding';"

# 3. éªŒè¯åŠŸèƒ½å‡½æ•° (3 Functions)
psql -d 'cognizes-engine' -c "\df calculate_retention_score"
psql -d 'cognizes-engine' -c "\df cleanup_low_value_memories"
psql -d 'cognizes-engine' -c "\df get_context_window"

# 4. åŠŸèƒ½å†’çƒŸæµ‹è¯• (Function Smoke Test)
psql -d 'cognizes-engine' -c "SELECT calculate_retention_score(5, NOW() - INTERVAL '3 days') AS score;"
# é¢„æœŸç»“æœ: score < 1.0 (e.g., ~0.95)
```

#### 4.1.3 å®šæ—¶ä»»åŠ¡é…ç½® (pg_cron) - P2-2-8

æˆ‘ä»¬é€šè¿‡ PostgreSQL å†…å»ºçš„å®šæ—¶ä»»åŠ¡æ¥å®ç°è®°å¿†ç³»ç»Ÿçš„**è‡ªç»´æŠ¤ (Self-Maintenance)**ã€‚

**å‰ææ¡ä»¶**: éœ€å…ˆå®‰è£…å¹¶å¯ç”¨ `pg_cron` æ‰©å±•ï¼ˆè¯¦è§ [010-the-pulse.md](../engine/010-the-pulse.md#pg_cron)ï¼‰ã€‚

**Step 1: æ³¨å†Œå®šæ—¶ä»»åŠ¡ (Execution)**

```sql
-- 1. è®°å¿†æ¸…ç† (æ¯æ—¥å‡Œæ™¨ 02:00)
-- æ¸…ç†è®¿é—®ç‡ä½ä¸”é™ˆæ—§çš„è®°å¿†ï¼Œä¿æŒ Context æ¸…çˆ½
SELECT cron.schedule(
    'cleanup_memories',
    '0 2 * * *',
    $$SELECT cleanup_low_value_memories(0.1, 7)$$
);

-- 2. å‘¨æœŸæ€§å·©å›º (æ¯å°æ—¶)
-- æ‰«ææœ€è¿‘æ´»è·ƒçš„ä¼šè¯ï¼Œç”Ÿæˆ consolidate ä»»åŠ¡
SELECT cron.schedule(
    'trigger_consolidation',
    '0 * * * *',
    $$SELECT trigger_maintenance_consolidation('1 hour'::interval)$$
);
```

**Step 2: ä»»åŠ¡éªŒè¯ (Verification)**

```bash
# 1. éªŒè¯ä»»åŠ¡æ˜¯å¦æ³¨å†Œ
psql -d 'cognizes-engine' -c "SELECT jobid, schedule, command FROM cron.job;"

# 2. æ‰‹åŠ¨è§¦å‘æµ‹è¯• (éªŒè¯å‡½æ•°é€»è¾‘)
psql -d 'cognizes-engine' -c "SELECT trigger_maintenance_consolidation('1 day'::interval);"
# é¢„æœŸ: è¿”å›ç”Ÿæˆçš„ job æ•°é‡
```

### 4.2 Step 2: Memory Consolidation Worker å®ç°

#### 4.2.1 æ ¸å¿ƒæ¶æ„è®¾è®¡

Memory Consolidation Worker é‡‡ç”¨**ä¸¤é˜¶æ®µå·©å›º**ç­–ç•¥ï¼Œæ¨¡æ‹Ÿäººç±»å¤§è„‘çš„è®°å¿†å·©å›ºè¿‡ç¨‹ï¼š

```mermaid
graph TB
    subgraph "é˜¶æ®µ 1: Fast Replay (å¿«å›æ”¾)"
        E[Events äº‹ä»¶æµ] --> ER[extract_recent_events]
        ER --> GS[generate_summary]
        GS --> SS[store_summary]
    end

    subgraph "é˜¶æ®µ 2: Deep Reflection (æ·±åæ€)"
        E --> EF[extract_facts]
        EF --> VI[vectorize_insights]
        VI --> SM[store_to_memories]
    end

    SS --> M[(memories è¡¨)]
    SM --> F[(facts è¡¨)]

    style E fill:#065f46,stroke:#34d399,color:#fff
    style M fill:#1e3a5f,stroke:#60a5fa,color:#fff
    style F fill:#7c2d12,stroke:#fb923c,color:#fff
```

#### 4.2.2 æ ¸å¿ƒä»£ç å®ç° (Source of Truth)

ä¸ºäº†éµå¾ª **Entropy Reduction (ç†µå‡)** åŸåˆ™ï¼Œå…·ä½“çš„ä¸šåŠ¡é€»è¾‘ä»£ç å·²æ”¶æ•›è‡³æºæ–‡ä»¶ç»´æŠ¤ã€‚

> [!IMPORTANT]
>
> **Source of Truth**: [src/cognizes/engine/hippocampus/consolidation_worker.py](file:///src/cognizes/engine/hippocampus/consolidation_worker.py)
>
> è¯¥æ¨¡å—å®ç°äº† `MemoryConsolidationWorker` ç±»ï¼Œè´Ÿè´£ï¼š
>
> 1. **Fast Replay**: ä½¿ç”¨ `_generate_summary` å¿«é€Ÿç”Ÿæˆå¯¹è¯æ‘˜è¦ã€‚
> 2. **Deep Reflection**: ä½¿ç”¨ `_extract_facts` æ·±åº¦æå–ç»“æ„åŒ–äº‹å® (Facts) å’Œæ´å¯Ÿ (Insights)ã€‚
> 3. **Vectorization**: è°ƒç”¨ `_generate_embedding` (Gemini `text-embedding-004`) ç”Ÿæˆå‘é‡ã€‚
> 4. **Storage**: å°†å¤„ç†ç»“æœåˆ†åˆ«å­˜å…¥ `memories` (Summary/Insight) å’Œ `facts` (Preference/Proflie) è¡¨ã€‚

#### 4.2.3 ä½¿ç”¨ç¤ºä¾‹ (SDK)

```python
# ä½¿ç”¨ç¤ºä¾‹: æ‰‹åŠ¨è§¦å‘è®°å¿†å·©å›º
import asyncio
import asyncpg
from cognizes.engine.hippocampus.consolidation_worker import MemoryConsolidationWorker, JobType

async def main():
    # 1. åˆ›å»ºæ•°æ®åº“è¿æ¥æ±  (é€šå¸¸ç”± DatabaseManager ç®¡ç†)
    pool = await asyncpg.create_pool("postgresql://aigc:@localhost/cognizes-engine")

    # 2. åˆå§‹åŒ– Worker
    worker = MemoryConsolidationWorker(pool)

    # 3. æ‰§è¡Œå®Œæ•´å·©å›º (Full Consolidation)
    # åŒ…å«: Fast Replay (æ‘˜è¦) + Deep Reflection (äº‹å®æå–)
    job = await worker.consolidate(
        thread_id="your-thread-id",
        job_type=JobType.FULL_CONSOLIDATION
    )

    print(f"Job completed: {job.result}")
    await pool.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### 4.3 Step 3: Biological Retention å®ç°

#### 4.3.1 è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿åŸç†

è‰¾å®¾æµ©æ–¯é—å¿˜æ›²çº¿æè¿°äº†è®°å¿†éšæ—¶é—´è¡°å‡çš„è§„å¾‹ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äº Agent è®°å¿†ç³»ç»Ÿï¼š

```mermaid
graph LR
    subgraph "é—å¿˜æ›²çº¿æ¨¡å‹"
        T[Time æ—¶é—´] --> D[Decay è¡°å‡]
        F[Frequency è®¿é—®é¢‘ç‡] --> B[Boost åŠ æˆ]
        D & B --> R[Retention Score ä¿ç•™åˆ†æ•°]
    end

    R --> C{Score é˜ˆå€¼}
    C -->|>= 0.1| K[ä¿ç•™]
    C -->|< 0.1| DEL[æ¸…ç†]

    style R fill:#7c2d12,stroke:#fb923c,color:#fff
```

**å…¬å¼**ï¼š

$$
    \text{retention\_score} = \min(1.0, \frac{\text{time\_decay} \times \text{frequency\_boost}}{5.0})
$$

å…¶ä¸­ï¼š

- $\text{time\_decay} = e^{-\lambda \times \text{days\_elapsed}}$ (æŒ‡æ•°è¡°å‡)
- $\text{frequency\_boost} = 1 + \ln(1 + \text{access\_count})$ (å¯¹æ•°åŠ æˆ)
- $\lambda = 0.1$ (é»˜è®¤è¡°å‡ç³»æ•°)

#### 4.3.2 Memory Retention Manager å®ç°

åˆ›å»º `src/cognizes/engine/hippocampus/retention_manager.py`ï¼š

#### 4.3.3 Context Window ç»„è£…å™¨å®ç°

åˆ›å»º `src/cognizes/engine/hippocampus/context_assembler.py`ï¼š

### 4.4 Step 4: OpenMemoryService å®ç° (ADK é€‚é…å™¨)

æœ¬æœåŠ¡ä½œä¸º ADK MemoryService çš„ **PostgreSQL é€‚é…å™¨**ï¼Œå¯¹å¤–æä¾›ç»Ÿä¸€çš„è®°å¿†è¯»å†™æ¥å£ã€‚

> [!IMPORTANT]
>
> **Source of Truth**: [src/cognizes/adapters/postgres/memory_service.py](file:///src/cognizes/adapters/postgres/memory_service.py)

#### 4.4.1 æ¥å£è®¾è®¡

åˆ›å»º `src/cognizes/engine/hippocampus/memory_service.py`ï¼š

#### 4.4.1 æ ¸å¿ƒèƒ½åŠ› (Capabilities)

è¯¥æœåŠ¡å°è£…äº†åº•å±‚çš„ Worker å’Œ Managerï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒ APIï¼š

1. **`add_session_to_memory(session_id, consolidation_type)`**:
   - **åŠŸèƒ½**: è§¦å‘è®°å¿†å·©å›ºæµç¨‹ã€‚
   - **å®ç°**: å§”æ‰˜ç»™ `MemoryConsolidationWorker` å¼‚æ­¥æ‰§è¡Œã€‚
   - **ADK æ˜ å°„**: å¯¹åº” ADK `MemoryService.add_memory()`.

2. **`search_memory(query, user_id, app_name)`**:
   - **åŠŸèƒ½**: è¯­ä¹‰æ£€ç´¢ç›¸å…³è®°å¿†ã€‚
   - **å®ç°**: è°ƒç”¨ Gemini `retrieval_query` æ¨¡å‹ç”Ÿæˆå‘é‡ï¼Œåœ¨ `memories` è¡¨æ‰§è¡Œ HNSW ç›¸ä¼¼åº¦æœç´¢ã€‚
   - **ç»“æœ**: è¿”å› `SearchMemoryResponse` å¯¹è±¡ï¼ŒåŒ…å«åŒ¹é…çš„è®°å¿†ç‰‡æ®µåŠå…¶ç›¸å…³åº¦åˆ†æ•°ã€‚

3. **`get_context_window(user_id, app_name, query)`**:
   - **åŠŸèƒ½**: æ„å»º LLM ä¸Šä¸‹æ–‡çª—å£ã€‚
   - **å®ç°**: è°ƒç”¨ SQL å‡½æ•° `get_context_window`ï¼ŒåŠ¨æ€ç»„è£… System Prompt (Instructions) + Facts + Memories + Historyã€‚

#### 4.4.2 æ¥å£å¥‘çº¦éªŒè¯

ä¸ºäº†ç¡®ä¿é€‚é…å™¨ç¬¦åˆ ADK æ ‡å‡†ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹é›†æˆæµ‹è¯•ï¼š

```bash
# è¿è¡Œ Memory Service é›†æˆæµ‹è¯•
pytest tests/integration/engine/test_memory_service.py
```

---

### 4.5 Step 5: AG-UI è®°å¿†ç³»ç»Ÿå¯è§†åŒ–æ¥å£

> [!NOTE]
>
> **å¯¹æ ‡ AG-UI åè®®**ï¼šæœ¬èŠ‚å®ç° The Hippocampus ä¸ AG-UI å¯è§†åŒ–å±‚çš„é›†æˆï¼Œæä¾›è®°å¿†å·©å›ºçŠ¶æ€ã€è®°å¿†å¬å›æ¥æºå’Œè®°å¿†å¥åº·åº¦çš„å¯è§†åŒ–èƒ½åŠ›ã€‚
>
> **å‚è€ƒèµ„æº**ï¼š
>
> - [AG-UI åè®®è°ƒç ”](../research/070-ag-ui.md)
> - [AG-UI å®˜æ–¹æ–‡æ¡£](https://docs.ag-ui.com/)

#### 4.5.1 è®°å¿†å¯è§†åŒ–æ¶æ„

```mermaid
graph TB
    subgraph "Hippocampus å­˜å‚¨å±‚"
        MEM[memories è¡¨]
        FACTS[facts è¡¨]
        CONS[consolidation_jobs è¡¨]
    end

    subgraph "å¯è§†åŒ–æ¥å£å±‚"
        CS[ConsolidationStatus]
        MH[MemoryHit]
        MD[MemoryDashboard]
    end

    subgraph "AG-UI äº‹ä»¶"
        ACT[ACTIVITY_SNAPSHOT]
        CUST[CUSTOM Events]
    end

    CONS -->|å·©å›ºçŠ¶æ€| CS
    MEM -->|å¬å›æ¥æº| MH
    MEM & FACTS -->|å¥åº·åº¦| MD

    CS --> ACT
    MH --> CUST
    MD --> CUST

    style CS fill:#a78bfa,stroke:#7c3aed,color:#000
    style MH fill:#4ade80,stroke:#16a34a,color:#000
    style MD fill:#fbbf24,stroke:#d97706,color:#000
```

#### 4.5.2 AG-UI äº‹ä»¶æ˜ å°„è¡¨

| Hippocampus åŠŸèƒ½ | è§¦å‘æ¡ä»¶                  | AG-UI äº‹ä»¶ç±»å‹          | å±•ç¤ºç»„ä»¶     |
| :--------------- | :------------------------ | :---------------------- | :----------- |
| è®°å¿†å·©å›ºè¿›åº¦     | Consolidation Worker æ‰§è¡Œ | `ACTIVITY_SNAPSHOT`     | å·©å›ºè¿›åº¦æ¡   |
| è®°å¿†å¬å›         | search_memory() è¿”å›ç»“æœ  | `CUSTOM (memory_hit)`   | æ¥æºæ ‡æ³¨å¡ç‰‡ |
| é—å¿˜æ›²çº¿æ›´æ–°     | retention_score è¡°å‡      | `CUSTOM (decay_update)` | è®°å¿†çƒ­åŠ›å›¾   |
| ä¸Šä¸‹æ–‡é¢„ç®—       | Context Budgeting æ‰§è¡Œ    | `STATE_DELTA`           | Token ä»ªè¡¨ç›˜ |

#### 4.5.3 MemoryVisualizer å®ç°

åˆ›å»º `src/cognizes/engine/hippocampus/memory_visualizer.py`ï¼š

#### 4.5.4 å‰ç«¯å±•ç¤ºç»„ä»¶è§„èŒƒ

| ç»„ä»¶åç§°                   | æ•°æ®æº              | å±•ç¤ºå†…å®¹                           |
| :------------------------- | :------------------ | :--------------------------------- |
| `ConsolidationProgressBar` | ACTIVITY_SNAPSHOT   | è¿›åº¦ç™¾åˆ†æ¯”ã€æå–äº‹å®æ•°             |
| `MemorySourceCard`         | CUSTOM (memory_hit) | è®°å¿†ç±»å‹å›¾æ ‡ã€å†…å®¹é¢„è§ˆã€ç›¸å…³æ€§åˆ†æ•° |
| `MemoryHealthDashboard`    | API è½®è¯¢            | æ€»æ•°ã€ç±»å‹åˆ†å¸ƒã€è¡°å‡æ›²çº¿           |
| `TokenBudgetMeter`         | STATE_DELTA         | å·²ç”¨/æ€»é‡è¿›åº¦æ¡                    |

#### 4.5.5 ä»»åŠ¡æ¸…å•

| ä»»åŠ¡ ID | ä»»åŠ¡æè¿°                   | çŠ¶æ€      | éªŒæ”¶æ ‡å‡†         |
| :------ | :------------------------- | :-------- | :--------------- |
| P2-6-1  | å®ç° `MemoryVisualizer` ç±» | ğŸ”² å¾…å¼€å§‹ | 4 ç§äº‹ä»¶ç±»å‹æ”¯æŒ |
| P2-6-2  | å®ç°å·©å›ºè¿›åº¦äº‹ä»¶å‘å°„       | ğŸ”² å¾…å¼€å§‹ | è¿›åº¦å®æ—¶æ›´æ–°     |
| P2-6-3  | å®ç°è®°å¿†å¬å›æ¥æºæ ‡æ³¨       | ğŸ”² å¾…å¼€å§‹ | æ¥æºå¯è¿½æº¯       |
| P2-6-4  | å®ç°å¥åº·åº¦æŒ‡æ ‡æ¥å£         | ğŸ”² å¾…å¼€å§‹ | æŒ‡æ ‡è®¡ç®—æ­£ç¡®     |
| P2-6-5  | ç¼–å†™å¯è§†åŒ–æ¥å£æµ‹è¯•         | ğŸ”² å¾…å¼€å§‹ | è¦†ç›–ç‡ > 80%     |

#### 4.5.6 éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹     | éªŒæ”¶æ ‡å‡†                    | éªŒè¯æ–¹æ³• |
| :--------- | :-------------------------- | :------- |
| å·©å›ºè¿›åº¦   | å®æ—¶å±•ç¤ºå·©å›ºè¿›åº¦ç™¾åˆ†æ¯”      | é›†æˆæµ‹è¯• |
| æ¥æºæ ‡æ³¨   | å¬å›çš„è®°å¿†æ˜¾ç¤ºæ¥æºä¼šè¯      | E2E æµ‹è¯• |
| å¥åº·åº¦     | æ­£ç¡®è®¡ç®—è¡°å‡ç‡å’Œåˆ†ç±»ç»Ÿè®¡    | å•å…ƒæµ‹è¯• |
| Token é¢„ç®— | å®æ—¶æ›´æ–°ä¸Šä¸‹æ–‡ Token ä½¿ç”¨é‡ | é›†æˆæµ‹è¯• |

---

## 5. éªŒè¯ SOP (Phase 2)

> [!IMPORTANT]
>
> æœ¬èŠ‚æä¾› Phase 2: The Hippocampus å®Œæ•´éªŒæ”¶æµç¨‹ï¼Œè¯·æŒ‰é¡ºåºé€æ­¥æ‰§è¡Œã€‚

### 5.1 Step 1: Schema éƒ¨ç½²éªŒè¯

```bash
# 1.1 ç¡®ä¿ Phase 1 Schema å·²éƒ¨ç½²
psql -d 'cognizes-engine' -c "\dt threads"
# åº”æ˜¾ç¤º threads è¡¨

# 1.2 éƒ¨ç½² Hippocampus Schema
psql -d 'cognizes-engine' -f src/cognizes/engine/schema/hippocampus_schema.sql

# 1.3 éªŒè¯è¡¨åˆ›å»º
psql -d 'cognizes-engine' -c "\dt"
# åº”æ˜¾ç¤º: memories, facts, consolidation_jobs, instructions

# 1.4 éªŒè¯ç´¢å¼•
psql -d 'cognizes-engine' -c "\di" | grep -E "(memories|facts)"

# 1.5 éªŒè¯å‡½æ•°
psql -d 'cognizes-engine' -c "\df calculate_retention_score"
psql -d 'cognizes-engine' -c "\df cleanup_low_value_memories"

# 1.6 æµ‹è¯•è¡°å‡å‡½æ•°
psql -d 'cognizes-engine' -c "SELECT calculate_retention_score(5, NOW() - INTERVAL '3 days', 0.1);"
# åº”è¿”å› 0.x çš„æµ®ç‚¹æ•°
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] `memories`, `facts`, `consolidation_jobs`, `instructions` è¡¨å­˜åœ¨
- [ ] HNSW å‘é‡ç´¢å¼•å·²åˆ›å»º
- [ ] `calculate_retention_score` å‡½æ•°å¯æ­£å¸¸è°ƒç”¨
- [ ] `cleanup_low_value_memories` å‡½æ•°å­˜åœ¨

---

#### 5.1.1 Step 1.1: pg_cron å®šæ—¶ä»»åŠ¡é…ç½® (P2-2-8, P2-3-4)

> [!IMPORTANT]
>
> pg_cron å®šæ—¶ä»»åŠ¡ç”¨äºè‡ªåŠ¨è§¦å‘è®°å¿†å·©å›ºå’Œä½ä»·å€¼è®°å¿†æ¸…ç†ï¼Œéœ€é…ç½®å Phase 2 éªŒæ”¶æ‰èƒ½å®Œæ•´é€šè¿‡ã€‚

```bash
# 1.1 æ£€æŸ¥ pg_cron æ‰©å±•æ˜¯å¦å·²å®‰è£… (Phase 1 å·²å®Œæˆ)
psql -d 'cognizes-engine' -c "SELECT * FROM pg_extension WHERE extname = 'pg_cron';"
# åº”è¿”å› 1 è¡Œè®°å½•

# 1.2 é…ç½®å®šæ—¶ä»»åŠ¡ - æ¯å¤©å‡Œæ™¨ 2 ç‚¹æ¸…ç†ä½ä»·å€¼è®°å¿† (P2-3-4)
psql -d 'cognizes-engine' -c "
SELECT cron.schedule(
    'cleanup_memories',
    '0 2 * * *',
    \$\$SELECT cleanup_low_value_memories(0.1, 7)\$\$
);
"
# åº”è¿”å›ä»»åŠ¡ ID (å¦‚ 1)

# 1.3 é…ç½®å®šæ—¶ä»»åŠ¡ - æ¯å°æ—¶è§¦å‘è®°å¿†å·©å›ºæ£€æŸ¥ (P2-2-8)
psql -d 'cognizes-engine' -c "
SELECT cron.schedule(
    'trigger_consolidation',
    '0 * * * *',
    \$\$
    INSERT INTO consolidation_jobs (thread_id, job_type, status)
    SELECT id, 'full_consolidation', 'pending'
    FROM threads
    WHERE updated_at > NOW() - INTERVAL '1 hour'
      AND id NOT IN (
          SELECT thread_id FROM consolidation_jobs
          WHERE created_at > NOW() - INTERVAL '1 hour'
      )
    \$\$
);
"
# åº”è¿”å›ä»»åŠ¡ ID (å¦‚ 2)

# 1.4 éªŒè¯å®šæ—¶ä»»åŠ¡åˆ›å»ºæˆåŠŸ
psql -d 'cognizes-engine' -c "SELECT jobid, jobname, schedule, command FROM cron.job;"
# åº”æ˜¾ç¤º cleanup_memories å’Œ trigger_consolidation ä¸¤ä¸ªä»»åŠ¡

# 1.5 æŸ¥çœ‹ä»»åŠ¡æ‰§è¡Œæ—¥å¿— (é¦–æ¬¡é…ç½®åå¯èƒ½ä¸ºç©º)
psql -d 'cognizes-engine' -c "SELECT * FROM cron.job_run_details ORDER BY start_time DESC LIMIT 5;"

# 1.6 æ‰‹åŠ¨æµ‹è¯•æ¸…ç†å‡½æ•° (å¯é€‰)
psql -d 'cognizes-engine' -c "SELECT cleanup_low_value_memories(0.1, 7);"
# åº”è¿”å›æ¸…ç†çš„è®°å½•æ•° (å¯èƒ½ä¸º 0)
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] pg_cron æ‰©å±•å·²å®‰è£…
- [ ] `cleanup_memories` å®šæ—¶ä»»åŠ¡å·²åˆ›å»º (æ¯å¤© 02:00)
- [ ] `trigger_consolidation` å®šæ—¶ä»»åŠ¡å·²åˆ›å»º (æ¯å°æ—¶)
- [ ] `cron.job` è¡¨æ˜¾ç¤º 2 ä¸ªä»»åŠ¡

**åˆ é™¤ä»»åŠ¡ (å¦‚éœ€é‡æ–°é…ç½®)**ï¼š

```bash
# åˆ é™¤æŒ‡å®šä»»åŠ¡
psql -d 'cognizes-engine' -c "SELECT cron.unschedule('cleanup_memories');"
psql -d 'cognizes-engine' -c "SELECT cron.unschedule('trigger_consolidation');"
```

---

### 5.2 Step 2: å•å…ƒæµ‹è¯•éªŒè¯

```bash
# 2.1 è¿è¡Œ Hippocampus å•å…ƒæµ‹è¯•
uv run pytest tests/unittests/hippocampus/ -v --tb=short

# 2.2 æŸ¥çœ‹æµ‹è¯•è¦†ç›–ç‡ (å¯é€‰ï¼Œéœ€å…ˆå®‰è£… pytest-cov)
# uv add pytest-cov --dev
uv run pytest tests/unittests/hippocampus/ -v --cov=src/cognizes/engine/hippocampus --cov-report=term-missing
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] 35 ä¸ªå•å…ƒæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] è¦†ç›–ä»¥ä¸‹æ¨¡å—:
  - `consolidation_worker.py` (æ•°æ®ç±»ã€æšä¸¾ã€æ ¼å¼åŒ–é€»è¾‘)
  - `retention_manager.py` (ä¿ç•™åˆ†æ•°åˆ†å¸ƒ)
  - `context_assembler.py` (Token ä¼°ç®—ã€ä¸Šä¸‹æ–‡æ ¼å¼åŒ–)
  - `memory_service.py` (æœåŠ¡å‚æ•°éªŒè¯)
  - `memory_visualizer.py` (äº‹ä»¶ç±»å‹ã€è¿›åº¦è®¡ç®—)

---

### 5.3 Step 3: é›†æˆæµ‹è¯•éªŒè¯

```bash
# 3.1 è¿è¡Œ Hippocampus é›†æˆæµ‹è¯•
uv run pytest tests/integration/hippocampus/ -v -s --tb=short

# 3.2 æŸ¥çœ‹è¯¦ç»†è¾“å‡º (å«æ€§èƒ½æŒ‡æ ‡)
uv run pytest tests/integration/hippocampus/ -v -s
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] 16 ä¸ªé›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [ ] Schema æµ‹è¯•é€šè¿‡: è¡¨ç»“æ„ã€ç´¢å¼•ã€å‡½æ•°ã€çº¦æŸ
- [ ] Read-Your-Writes å»¶è¿Ÿ < 100ms
- [ ] æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½ P99 < 50ms (1K è§„æ¨¡)
- [ ] ä¿ç•™åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡æ­£ç¡®
- [ ] è®¿é—®è®¡æ•°é€’å¢æ­£ç¡®
- [ ] Fact Upsert çº¦æŸç”Ÿæ•ˆ

---

### 5.4 Step 4: æ€§èƒ½æµ‹è¯• (å¯é€‰, 10 ä¸‡è§„æ¨¡)

```bash
# 4.0 æ¸…ç†å†å²æ€§èƒ½æµ‹è¯•æ•°æ® (é¿å…å­˜é‡æ•°æ®å½±å“ç»“æœ)
uv run python -c "
import asyncio
import asyncpg

async def cleanup():
    pool = await asyncpg.create_pool('postgresql://aigc:@localhost/cognizes-engine')
    user_id = 'perf_test_user'

    async with pool.acquire() as conn:
        # ç»Ÿè®¡ç°æœ‰æ•°æ®
        count = await conn.fetchval(
            'SELECT COUNT(*) FROM memories WHERE user_id = \$1', user_id
        )
        if count == 0:
            print('âœ“ æ— å†å²æµ‹è¯•æ•°æ®ï¼Œæ— éœ€æ¸…ç†')
            return

        # æ¸…ç†æ€§èƒ½æµ‹è¯•æ•°æ®
        deleted = await conn.fetchval('''
            DELETE FROM memories WHERE user_id = \$1 RETURNING COUNT(*)
        ''', user_id)
        print(f'âœ“ å·²æ¸…ç† {count} æ¡å†å²æµ‹è¯•æ•°æ®')
    await pool.close()

asyncio.run(cleanup())
"

# 4.1 ç”Ÿæˆå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
uv run python -c "
import asyncio
import asyncpg
import uuid
import random
from datetime import datetime, timedelta

async def seed():
    pool = await asyncpg.create_pool('postgresql://aigc:@localhost/cognizes-engine')
    user_id = 'perf_test_user'
    app_name = 'perf_test_app'

    async with pool.acquire() as conn:
        count = await conn.fetchval(
            'SELECT COUNT(*) FROM memories WHERE user_id = \$1', user_id
        )
        if count >= 100000:
            print(f'å·²æœ‰ {count} æ¡æ•°æ®ï¼Œè·³è¿‡')
            return

        print('å¼€å§‹ç”Ÿæˆ 100K æµ‹è¯•æ•°æ®...')
        batch_size = 1000
        base_time = datetime.now() - timedelta(days=365)

        for batch in range(100):
            rows = []
            for i in range(batch_size):
                created_at = base_time + timedelta(minutes=random.randint(0, 525600))
                rows.append((
                    uuid.uuid4(), user_id, app_name, 'episodic',
                    f'æµ‹è¯•è®°å¿† {batch * batch_size + i}',
                    random.random(), random.randint(0, 100), created_at
                ))
            await conn.executemany('''
                INSERT INTO memories (id, user_id, app_name, memory_type, content,
                                     retention_score, access_count, created_at)
                VALUES (\$1, \$2, \$3, \$4, \$5, \$6, \$7, \$8)
            ''', rows)
            if (batch + 1) % 10 == 0:
                print(f'  å·²æ’å…¥ {(batch + 1) * batch_size}')
    await pool.close()

asyncio.run(seed())
"

# 4.2 è¿è¡Œæ€§èƒ½æµ‹è¯•
uv run pytest tests/integration/hippocampus/test_episodic_performance.py -v -s -k "full"
# === å®Œæ•´æ€§èƒ½æµ‹è¯• (100,000 æ¡) ===
# å¹³å‡å»¶è¿Ÿ: 1.01 ms
# P99 å»¶è¿Ÿ: 2.38 ms
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] 10 ä¸‡è§„æ¨¡æ—¶é—´åˆ‡ç‰‡æŸ¥è¯¢ P99 < 100ms
- [ ] æŸ¥è¯¢ä½¿ç”¨ç´¢å¼•æ‰«æ (éå…¨è¡¨æ‰«æ)

---

### 5.5 Step 5: æ¨¡å—å¯¼å…¥éªŒè¯

```bash
# 5.1 éªŒè¯æ¨¡å—å¯å¯¼å…¥
uv run python -c "
from cognizes.engine.hippocampus.consolidation_worker import (
    MemoryConsolidationWorker, JobType, JobStatus
)
from cognizes.engine.hippocampus.retention_manager import MemoryRetentionManager
from cognizes.engine.hippocampus.context_assembler import ContextAssembler
from cognizes.engine.hippocampus.memory_service import OpenMemoryService
from cognizes.engine.hippocampus.memory_visualizer import MemoryVisualizer

print('âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ')
"
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] æ‰€æœ‰ 5 ä¸ªæ¨¡å—å¯æ­£å¸¸å¯¼å…¥
- [ ] æ— å¾ªç¯ä¾èµ–é”™è¯¯

---

### 5.6 Step 6: å…¨é‡æµ‹è¯•éªŒè¯

```bash
# 6.1 è¿è¡Œæ‰€æœ‰æµ‹è¯• (åŒ…æ‹¬ Phase 1)
uv run pytest tests/ -v --tb=line

# 6.2 æŸ¥çœ‹æµ‹è¯•ç»Ÿè®¡
uv run pytest tests/ -v --tb=line 2>&1 | tail -5
```

**éªŒæ”¶æ ‡å‡†**ï¼š

- [ ] Phase 1 æµ‹è¯•: 61 passed
- [ ] Phase 2 å•å…ƒæµ‹è¯•: 35 passed
- [ ] Phase 2 é›†æˆæµ‹è¯•: 16 passed
- [ ] **æ€»è®¡: 112+ passed**

---

### 5.7 éªŒæ”¶æ€»ç»“æ¸…å•

| éªŒæ”¶é¡¹           | çŠ¶æ€ | è¯´æ˜                          |
| :--------------- | :--: | :---------------------------- |
| Schema éƒ¨ç½²      |  â¬œ  | 4 å¼ è¡¨ + 2 ä¸ªå‡½æ•° + HNSW ç´¢å¼• |
| pg_cron å®šæ—¶ä»»åŠ¡ |  â¬œ  | 2 ä¸ªä»»åŠ¡ (æ¸…ç† + å·©å›º)        |
| å•å…ƒæµ‹è¯•         |  â¬œ  | 35 tests passed               |
| é›†æˆæµ‹è¯•         |  â¬œ  | 17 tests passed               |
| Read-Your-Writes |  â¬œ  | P99 < 100ms                   |
| æ¨¡å—å¯¼å…¥         |  â¬œ  | 5 æ¨¡å—æ— é”™è¯¯                  |
| å…¨é‡å›å½’         |  â¬œ  | 113+ tests passed             |

> [!TIP]
>
> å®Œæˆä¸Šè¿°æ‰€æœ‰éªŒæ”¶é¡¹åï¼Œå‹¾é€‰çŠ¶æ€ä¸º âœ…ï¼ŒPhase 2: The Hippocampus éªŒæ”¶é€šè¿‡ï¼Œå¯è¿›å…¥ Phase 3: The Perceptionã€‚

---

## 6. éªŒæ”¶åŸºå‡†

### 6.1 åŠŸèƒ½éªŒæ”¶çŸ©é˜µ

| éªŒæ”¶é¡¹                | ä»»åŠ¡ ID           | éªŒæ”¶æ ‡å‡†                                                 | éªŒè¯æ–¹æ³•                |
| :-------------------- | :---------------- | :------------------------------------------------------- | :---------------------- |
| **Schema éƒ¨ç½²**       | P2-2-1 ~ P2-2-2   | `memories`, `facts`, `consolidation_jobs` è¡¨åˆ›å»ºæˆåŠŸ     | `\dt` æŸ¥çœ‹è¡¨åˆ—è¡¨        |
| **Fast Replay**       | P2-2-5 ~ P2-2-8   | å¯¹è¯æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œå­˜å…¥ `memories` è¡¨                     | å•å…ƒæµ‹è¯•                |
| **Deep Reflection**   | P2-2-9 ~ P2-2-12  | Facts æå–æˆåŠŸï¼Œå­˜å…¥ `facts` è¡¨ (Upsert é€»è¾‘æ­£ç¡®)        | å•å…ƒæµ‹è¯• + é‡å¤æ’å…¥æµ‹è¯• |
| **Read-Your-Writes**  | P2-2-13 ~ P2-2-14 | æ–°è®°å¿†åœ¨ä¸‹ä¸€ Turn ç«‹å³å¯æ£€ç´¢                             | å»¶è¿Ÿæµ‹è¯• (< 100ms)      |
| **è‰¾å®¾æµ©æ–¯è¡°å‡**      | P2-3-1 ~ P2-3-4   | `retention_score` éšæ—¶é—´è¡°å‡ï¼Œé«˜é¢‘è®¿é—®æå‡åˆ†æ•°           | è¡°å‡æ›²çº¿éªŒè¯            |
| **æƒ…æ™¯åˆ†å—**          | P2-3-5 ~ P2-3-7   | æŒ‰æ—¶é—´åˆ‡ç‰‡æ£€ç´¢ P99 < 100ms (10 ä¸‡è®°å¿†è§„æ¨¡)               | æ€§èƒ½æµ‹è¯•                |
| **Context Window**    | P2-3-8 ~ P2-3-11  | åŠ¨æ€ç»„è£… Context ä¸è¶…å‡º Token é¢„ç®—ï¼Œè¶…é™æ—¶è‡ªåŠ¨æˆªæ–­       | Token ç»Ÿè®¡æµ‹è¯•          |
| **OpenMemoryService** | Phase 2 ç»¼åˆ      | å®ç° `add_session_to_memory()` å’Œ `search_memory()` æ¥å£ | æ¥å£å…¼å®¹æ€§æµ‹è¯•          |

### 6.2 æ€§èƒ½éªŒæ”¶æŒ‡æ ‡

| æŒ‡æ ‡                 | ç›®æ ‡å€¼    | æµ‹è¯•æ¡ä»¶                      |
| :------------------- | :-------- | :---------------------------- |
| **è®°å¿†å†™å…¥å»¶è¿Ÿ**     | < 500ms   | å•æ¬¡ `consolidate()` è°ƒç”¨     |
| **è®°å¿†æ£€ç´¢å»¶è¿Ÿ**     | < 50ms    | `search_memory()` Top-10 ç»“æœ |
| **å‘é‡ç´¢å¼• QPS**     | > 100 QPS | 10 ä¸‡å‘é‡è§„æ¨¡                 |
| **Read-Your-Writes** | < 100ms   | æ–°è®°å¿†å¯è§å»¶è¿Ÿ                |
| **Context ç»„è£…å»¶è¿Ÿ** | < 100ms   | 8000 Token é¢„ç®—               |

### 6.3 å…¼å®¹æ€§éªŒæ”¶

| éªŒæ”¶é¡¹                     | éªŒæ”¶æ ‡å‡†                                                |
| :------------------------- | :------------------------------------------------------ |
| **ADK MemoryService å…¼å®¹** | `OpenMemoryService` å¯ä½œä¸º ADK `MemoryService` æ›¿ä»£ä½¿ç”¨ |
| **Phase 1 å…¼å®¹**           | ä¸ `threads`/`events` è¡¨æ— ç¼å…³è”                        |
| **å‘é‡æ ¼å¼å…¼å®¹**           | ä½¿ç”¨ä¸ Phase 1 ç›¸åŒçš„ 1536 ç»´å‘é‡ (Gemini embedding)    |

### 6.4 éªŒè¯æµ‹è¯•ä»£ç 

> [!NOTE]
>
> æœ¬èŠ‚æä¾›å…³é”®éªŒè¯æµ‹è¯•çš„ä»£ç å®ç°ï¼Œå¯¹åº”ä»»åŠ¡ P2-2-13~14, P2-3-7, P2-4-3ã€‚

#### 6.4.1 Read-Your-Writes å»¶è¿Ÿæµ‹è¯• (P2-2-13, P2-2-14)

åˆ›å»º `tests/hippocampus/test_read_your_writes.py`ï¼š

```python
"""
Read-Your-Writes å»¶è¿Ÿæµ‹è¯•

éªŒè¯æ–°å†™å…¥çš„è®°å¿†èƒ½å¦åœ¨ä¸‹ä¸€ä¸ª Turn ç«‹å³å¯è§ï¼Œ
ç¡®ä¿æˆ‘ä»¬çš„ Zero-ETL æ¶æ„æ¯” Google æ–¹æ¡ˆæ›´å¿«ã€‚

éªŒæ”¶æ ‡å‡†: å»¶è¿Ÿ < 100ms
"""

import asyncio
import time
import uuid
from statistics import mean, stdev

import asyncpg
import pytest

from hippocampus.consolidation_worker import MemoryConsolidationWorker, JobType
from hippocampus.memory_service import OpenMemoryService


class TestReadYourWrites:
    """Read-Your-Writes å»¶è¿Ÿæµ‹è¯•å¥—ä»¶"""

    @pytest.fixture
    async def pool(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥æ± """
        pool = await asyncpg.create_pool(
            "postgresql://user:pass@localhost/agent_db_test"
        )
        yield pool
        await pool.close()

    @pytest.fixture
    async def memory_service(self, pool):
        """åˆ›å»º MemoryService å®ä¾‹"""
        return OpenMemoryService(pool)

    @pytest.fixture
    async def setup_test_thread(self, pool):
        """åˆ›å»ºæµ‹è¯•ç”¨çš„ Thread å’Œ Events"""
        thread_id = str(uuid.uuid4())
        user_id = "test_user"
        app_name = "test_app"

        async with pool.acquire() as conn:
            # åˆ›å»º Thread
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, $2, $3, '{}')
            """, uuid.UUID(thread_id), user_id, app_name)

            # åˆ›å»ºæµ‹è¯• Events
            for i in range(5):
                await conn.execute("""
                    INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                    VALUES ($1, $2, 'message', $3, $4)
                """, uuid.UUID(thread_id),
                    'user' if i % 2 == 0 else 'agent',
                    f'{{"text": "æµ‹è¯•æ¶ˆæ¯ {i}"}}',
                    i)

        yield {"thread_id": thread_id, "user_id": user_id, "app_name": app_name}

        # æ¸…ç†
        async with pool.acquire() as conn:
            await conn.execute("DELETE FROM threads WHERE id = $1", uuid.UUID(thread_id))

    async def test_read_your_writes_latency(
        self, pool, memory_service, setup_test_thread
    ):
        """
        éªŒè¯ Read-Your-Writes å»¶è¿Ÿ < 100ms

        æµç¨‹:
        1. æ‰§è¡Œè®°å¿†å·©å›º (å†™å…¥)
        2. ç«‹å³æ‰§è¡Œè®°å¿†æ£€ç´¢ (è¯»å–)
        3. æµ‹é‡ä»å†™å…¥å®Œæˆåˆ°è¯»å–æˆåŠŸçš„å»¶è¿Ÿ
        """
        thread_info = setup_test_thread
        latencies = []

        for _ in range(10):  # æ‰§è¡Œ 10 æ¬¡æµ‹é‡
            # Step 1: æ‰§è¡Œå·©å›º (å†™å…¥)
            result = await memory_service.add_session_to_memory(
                session_id=thread_info["thread_id"],
                consolidation_type="fast"
            )
            assert result["status"] == "completed"

            # Step 2: ç«‹å³æ£€ç´¢ (è¯»å–) å¹¶æµ‹é‡å»¶è¿Ÿ
            start = time.perf_counter()
            search_result = await memory_service.search_memory(
                app_name=thread_info["app_name"],
                user_id=thread_info["user_id"],
                query="æµ‹è¯•æ¶ˆæ¯",
            )
            end = time.perf_counter()

            latency_ms = (end - start) * 1000
            latencies.append(latency_ms)

            # éªŒè¯è®°å¿†å¯è§
            assert search_result.total_count > 0, "æ–°è®°å¿†åº”ç«‹å³å¯è§"

        # ç»Ÿè®¡ç»“æœ
        avg_latency = mean(latencies)
        p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]

        print(f"\n=== Read-Your-Writes å»¶è¿Ÿæµ‹è¯•ç»“æœ ===")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ms")
        print(f"P99 å»¶è¿Ÿ: {p99_latency:.2f} ms")
        print(f"æ ‡å‡†å·®: {stdev(latencies):.2f} ms")

        # éªŒæ”¶æ ‡å‡†: P99 < 100ms
        assert p99_latency < 100, f"P99 å»¶è¿Ÿ {p99_latency:.2f}ms è¶…è¿‡ 100ms é˜ˆå€¼"


# è¿è¡Œ: pytest -v tests/hippocampus/test_read_your_writes.py
```

#### 6.4.2 æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½æµ‹è¯• (P2-3-7)

åˆ›å»º `tests/hippocampus/test_episodic_performance.py`ï¼š

```python
"""
æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½æµ‹è¯•

éªŒè¯åœ¨ 10 ä¸‡è®°å¿†è§„æ¨¡ä¸‹ï¼ŒæŒ‰æ—¶é—´åˆ‡ç‰‡æ£€ç´¢çš„ P99 < 100msã€‚
"""

import asyncio
import random
import time
import uuid
from datetime import datetime, timedelta
from statistics import mean

import asyncpg
import pytest


class TestEpisodicPerformance:
    """æƒ…æ™¯åˆ†å—æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    MEMORY_COUNT = 100_000  # 10 ä¸‡è®°å¿†
    TEST_RUNS = 50

    @pytest.fixture(scope="class")
    async def pool(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥æ± """
        pool = await asyncpg.create_pool(
            "postgresql://user:pass@localhost/agent_db_test",
            min_size=5,
            max_size=20,
        )
        yield pool
        await pool.close()

    @pytest.fixture(scope="class")
    async def seed_memories(self, pool):
        """
        é¢„å…ˆå¡«å…… 10 ä¸‡æ¡æµ‹è¯•è®°å¿†

        æ³¨æ„: æ­¤ fixture ä»…åœ¨æµ‹è¯•ç±»é¦–æ¬¡è¿è¡Œæ—¶æ‰§è¡Œ
        """
        user_id = "perf_test_user"
        app_name = "perf_test_app"

        async with pool.acquire() as conn:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æµ‹è¯•æ•°æ®
            count = await conn.fetchval(
                "SELECT COUNT(*) FROM memories WHERE user_id = $1",
                user_id
            )
            if count >= self.MEMORY_COUNT:
                print(f"\nå·²å­˜åœ¨ {count} æ¡æµ‹è¯•è®°å¿†ï¼Œè·³è¿‡ç§å­æ•°æ®ç”Ÿæˆ")
                return {"user_id": user_id, "app_name": app_name}

            print(f"\nå¼€å§‹ç”Ÿæˆ {self.MEMORY_COUNT} æ¡æµ‹è¯•è®°å¿†...")

            # æ‰¹é‡æ’å…¥ (æ¯æ‰¹ 1000 æ¡)
            batch_size = 1000
            base_time = datetime.now() - timedelta(days=365)

            for batch in range(self.MEMORY_COUNT // batch_size):
                rows = []
                for i in range(batch_size):
                    created_at = base_time + timedelta(
                        minutes=random.randint(0, 525600)  # ä¸€å¹´å†…éšæœº
                    )
                    rows.append((
                        uuid.uuid4(),
                        user_id,
                        app_name,
                        'episodic',
                        f'æµ‹è¯•è®°å¿†å†…å®¹ {batch * batch_size + i}',
                        random.random(),  # retention_score
                        random.randint(0, 100),  # access_count
                        created_at,
                    ))

                await conn.executemany("""
                    INSERT INTO memories (id, user_id, app_name, memory_type, content,
                                         retention_score, access_count, created_at)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                """, rows)

                if (batch + 1) % 10 == 0:
                    print(f"  å·²æ’å…¥ {(batch + 1) * batch_size} æ¡è®°å¿†")

            print(f"æµ‹è¯•æ•°æ®ç”Ÿæˆå®Œæˆ")

        return {"user_id": user_id, "app_name": app_name}

    async def test_time_slice_query_performance(self, pool, seed_memories):
        """
        æµ‹è¯•æŒ‰æ—¶é—´åˆ‡ç‰‡æŸ¥è¯¢æ€§èƒ½

        éªŒæ”¶æ ‡å‡†: P99 < 100ms
        """
        user_id = seed_memories["user_id"]
        app_name = seed_memories["app_name"]

        latencies = []

        for _ in range(self.TEST_RUNS):
            # éšæœºé€‰æ‹©ä¸€ä¸ª 7 å¤©çš„æ—¶é—´çª—å£
            start_offset = random.randint(0, 358)
            start_time = datetime.now() - timedelta(days=365 - start_offset)
            end_time = start_time + timedelta(days=7)

            # æ‰§è¡Œæ—¶é—´åˆ‡ç‰‡æŸ¥è¯¢
            start = time.perf_counter()

            async with pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT id, content, retention_score, created_at
                    FROM memories
                    WHERE user_id = $1
                      AND app_name = $2
                      AND created_at >= $3
                      AND created_at <= $4
                    ORDER BY created_at DESC
                    LIMIT 50
                """, user_id, app_name, start_time, end_time)

            end = time.perf_counter()
            latency_ms = (end - start) * 1000
            latencies.append(latency_ms)

        # ç»Ÿè®¡ç»“æœ
        avg_latency = mean(latencies)
        p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
        max_latency = max(latencies)

        print(f"\n=== æƒ…æ™¯åˆ†å—æ£€ç´¢æ€§èƒ½æµ‹è¯•ç»“æœ ({self.MEMORY_COUNT:,} æ¡è®°å¿†) ===")
        print(f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f} ms")
        print(f"P99 å»¶è¿Ÿ: {p99_latency:.2f} ms")
        print(f"æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f} ms")
        print(f"æµ‹è¯•æ¬¡æ•°: {self.TEST_RUNS}")

        # éªŒæ”¶æ ‡å‡†: P99 < 100ms
        assert p99_latency < 100, f"P99 å»¶è¿Ÿ {p99_latency:.2f}ms è¶…è¿‡ 100ms é˜ˆå€¼"

    async def test_composite_index_usage(self, pool, seed_memories):
        """éªŒè¯å¤åˆç´¢å¼• (user_id, app_name, created_at) è¢«æ­£ç¡®ä½¿ç”¨"""
        user_id = seed_memories["user_id"]
        app_name = seed_memories["app_name"]
        start_time = datetime.now() - timedelta(days=30)
        end_time = datetime.now()

        async with pool.acquire() as conn:
            # ä½¿ç”¨ EXPLAIN ANALYZE æ£€æŸ¥æŸ¥è¯¢è®¡åˆ’
            plan = await conn.fetch("""
                EXPLAIN ANALYZE
                SELECT id, content, retention_score, created_at
                FROM memories
                WHERE user_id = $1
                  AND app_name = $2
                  AND created_at >= $3
                  AND created_at <= $4
                ORDER BY created_at DESC
                LIMIT 50
            """, user_id, app_name, start_time, end_time)

            plan_text = "\n".join(row[0] for row in plan)
            print(f"\n=== æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’ ===\n{plan_text}")

            # éªŒè¯ä½¿ç”¨äº†ç´¢å¼•æ‰«æ
            assert "Index" in plan_text, "æŸ¥è¯¢åº”ä½¿ç”¨ç´¢å¼•æ‰«æ"
            assert "Seq Scan" not in plan_text, "ä¸åº”ä½¿ç”¨å…¨è¡¨æ‰«æ"


# è¿è¡Œ: pytest -v tests/hippocampus/test_episodic_performance.py
```

#### 6.4.3 å•å…ƒæµ‹è¯•æ¡†æ¶ (P2-4-3)

åˆ›å»º `tests/hippocampus/conftest.py` (pytest é…ç½®):

```python
"""
Hippocampus æµ‹è¯•é…ç½®

æä¾›æµ‹è¯• fixtures å’Œå…±äº«é…ç½®
"""

import asyncio
import os

import asyncpg
import pytest


@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="session")
async def test_db_pool():
    """
    åˆ›å»ºæµ‹è¯•æ•°æ®åº“è¿æ¥æ± 

    ç¯å¢ƒå˜é‡:
    - TEST_DATABASE_URL: æµ‹è¯•æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
    """
    database_url = os.getenv(
        "TEST_DATABASE_URL",
        "postgresql://user:pass@localhost/agent_db_test"
    )
    pool = await asyncpg.create_pool(database_url, min_size=2, max_size=10)
    yield pool
    await pool.close()


@pytest.fixture
async def clean_test_data(test_db_pool):
    """
    æµ‹è¯•åæ¸…ç†æ•°æ®

    åœ¨æ¯ä¸ªæµ‹è¯•ç»“æŸååˆ é™¤æµ‹è¯•æœŸé—´åˆ›å»ºçš„æ•°æ®
    """
    created_ids = {"threads": [], "memories": [], "facts": []}

    yield created_ids

    # æ¸…ç†
    async with test_db_pool.acquire() as conn:
        if created_ids["facts"]:
            await conn.execute(
                "DELETE FROM facts WHERE id = ANY($1::uuid[])",
                created_ids["facts"]
            )
        if created_ids["memories"]:
            await conn.execute(
                "DELETE FROM memories WHERE id = ANY($1::uuid[])",
                created_ids["memories"]
            )
        if created_ids["threads"]:
            await conn.execute(
                "DELETE FROM threads WHERE id = ANY($1::uuid[])",
                created_ids["threads"]
            )
```

åˆ›å»º `tests/hippocampus/test_consolidation_worker.py`:

```python
"""
MemoryConsolidationWorker å•å…ƒæµ‹è¯•
"""

import uuid
import pytest

from hippocampus.consolidation_worker import (
    MemoryConsolidationWorker,
    JobType,
    JobStatus,
)


class TestConsolidationWorker:
    """Consolidation Worker å•å…ƒæµ‹è¯•"""

    @pytest.fixture
    async def worker(self, test_db_pool):
        return MemoryConsolidationWorker(test_db_pool)

    async def test_fast_replay_generates_summary(self, worker, test_db_pool, clean_test_data):
        """Fast Replay åº”ç”Ÿæˆå¯¹è¯æ‘˜è¦"""
        # Setup: åˆ›å»ºæµ‹è¯• Thread å’Œ Events
        thread_id = str(uuid.uuid4())
        async with test_db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, 'test_user', 'test_app', '{}')
            """, uuid.UUID(thread_id))
            clean_test_data["threads"].append(uuid.UUID(thread_id))

            for i in range(3):
                await conn.execute("""
                    INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                    VALUES ($1, $2, 'message', $3, $4)
                """, uuid.UUID(thread_id),
                    'user' if i % 2 == 0 else 'agent',
                    f'{{"text": "å¯¹è¯å†…å®¹ {i}"}}', i)

        # Act
        job = await worker.consolidate(thread_id, JobType.FAST_REPLAY)

        # Assert
        assert job.status == JobStatus.COMPLETED
        assert "summary" in job.result
        assert job.result["summary"]["memory_id"]

    async def test_deep_reflection_extracts_facts(self, worker, test_db_pool, clean_test_data):
        """Deep Reflection åº”æå– Facts"""
        thread_id = str(uuid.uuid4())
        async with test_db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, 'test_user', 'test_app', '{}')
            """, uuid.UUID(thread_id))
            clean_test_data["threads"].append(uuid.UUID(thread_id))

            # åˆ›å»ºåŒ…å«åå¥½ä¿¡æ¯çš„å¯¹è¯
            await conn.execute("""
                INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                VALUES ($1, 'user', 'message', '{"text": "æˆ‘å–œæ¬¢åƒå¯¿å¸å’Œæ„å¤§åˆ©é¢"}', 0)
            """, uuid.UUID(thread_id))

        # Act
        job = await worker.consolidate(thread_id, JobType.DEEP_REFLECTION)

        # Assert
        assert job.status == JobStatus.COMPLETED
        # Facts å¯èƒ½ä¸ºç©º (å–å†³äº LLM æå–ç»“æœ)ï¼Œä½†ä»»åŠ¡åº”æˆåŠŸå®Œæˆ

    async def test_full_consolidation_runs_both_phases(self, worker, test_db_pool, clean_test_data):
        """Full Consolidation åº”æ‰§è¡Œä¸¤ä¸ªé˜¶æ®µ"""
        thread_id = str(uuid.uuid4())
        async with test_db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO threads (id, user_id, app_name, state)
                VALUES ($1, 'test_user', 'test_app', '{}')
            """, uuid.UUID(thread_id))
            clean_test_data["threads"].append(uuid.UUID(thread_id))

            await conn.execute("""
                INSERT INTO events (thread_id, author, event_type, content, sequence_num)
                VALUES ($1, 'user', 'message', '{"text": "æµ‹è¯•æ¶ˆæ¯"}', 0)
            """, uuid.UUID(thread_id))

        # Act
        job = await worker.consolidate(thread_id, JobType.FULL_CONSOLIDATION)

        # Assert
        assert job.status == JobStatus.COMPLETED
        assert "summary" in job.result  # Fast Replay ç»“æœ
        assert "facts" in job.result or "insights" in job.result  # Deep Reflection ç»“æœ
```

---

### 6.5. äº¤ä»˜ç‰©æ¸…å•

#### 6.5.1 Schema æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„                                            | æè¿°                    | çŠ¶æ€      |
| :-------------------------------------------------- | :---------------------- | :-------- |
| `src/cognizes/engine/schema/hippocampus_schema.sql` | Hippocampus æ‰©å±• Schema | ğŸ”² å¾…å¼€å§‹ |

#### 6.5.2 ä»£ç æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„                                                  | æè¿°            | çŠ¶æ€      |
| :-------------------------------------------------------- | :-------------- | :-------- |
| `src/cognizes/engine/hippocampus/__init__.py`             | æ¨¡å—åˆå§‹åŒ–      | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/consolidation_worker.py` | è®°å¿†å·©å›º Worker | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/retention_manager.py`    | è®°å¿†ä¿æŒç®¡ç†å™¨  | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/context_assembler.py`    | ä¸Šä¸‹æ–‡ç»„è£…å™¨    | ğŸ”² å¾…å¼€å§‹ |
| `src/cognizes/engine/hippocampus/memory_service.py`       | ADK é€‚é…å™¨      | ğŸ”² å¾…å¼€å§‹ |

#### 6.5.3 æµ‹è¯•æ–‡ä»¶

| æ–‡ä»¶è·¯å¾„                                                     | æè¿°                       | çŠ¶æ€      |
| :----------------------------------------------------------- | :------------------------- | :-------- |
| `tests/integration/hippocampus/test_consolidation_worker.py` | Worker å•å…ƒæµ‹è¯•            | ğŸ”² å¾…å¼€å§‹ |
| `tests/integration/hippocampus/test_retention_manager.py`    | ä¿æŒç®¡ç†å™¨å•å…ƒæµ‹è¯•         | ğŸ”² å¾…å¼€å§‹ |
| `tests/integration/hippocampus/test_context_assembler.py`    | ä¸Šä¸‹æ–‡ç»„è£…å™¨å•å…ƒæµ‹è¯•       | ğŸ”² å¾…å¼€å§‹ |
| `tests/integration/hippocampus/test_memory_service.py`       | OpenMemoryService é›†æˆæµ‹è¯• | ğŸ”² å¾…å¼€å§‹ |

#### 6.5.4 ç›®å½•ç»“æ„

```
src/cognizes/engine/
â”œâ”€â”€ schema/
â”‚   â”œâ”€â”€ agent_schema.sql           # Phase 1 åŸºç¡€ Schema
â”‚   â””â”€â”€ hippocampus_schema.sql     # Phase 2 æ‰©å±• Schema
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ pulse/                     # Phase 1: The Pulse
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ state_manager.py
â”‚   â”‚   â””â”€â”€ pg_notify_listener.py
â”‚   â””â”€â”€ hippocampus/               # Phase 2: The Hippocampus
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ consolidation_worker.py
â”‚       â”œâ”€â”€ retention_manager.py
â”‚       â”œâ”€â”€ context_assembler.py
â”‚       â””â”€â”€ memory_service.py
tests/
â”œâ”€â”€ pulse/
â”‚   â””â”€â”€ test_state_manager.py
â””â”€â”€ hippocampus/
    â”œâ”€â”€ test_consolidation_worker.py
    â”œâ”€â”€ test_retention_manager.py
    â”œâ”€â”€ test_context_assembler.py
    â””â”€â”€ test_memory_service.py
```

---

## 7. é£é™©ä¸ç¼“è§£ç­–ç•¥

### 7.1 æŠ€æœ¯é£é™©

| é£é™©                        | å½±å“ | æ¦‚ç‡ | ç¼“è§£ç­–ç•¥                                |
| :-------------------------- | :--- | :--- | :-------------------------------------- |
| **LLM æå–ä¸ç¨³å®š**          | ä¸­   | ä¸­   | è®¾è®¡å¥å£®çš„ JSON è§£æé€»è¾‘ï¼Œå®¹é”™å¤„ç†      |
| **å‘é‡æ£€ç´¢ç²¾åº¦ä¸è¶³**        | é«˜   | ä½   | å¼•å…¥ Reranker (Phase 3)ï¼Œè°ƒä¼˜ HNSW å‚æ•° |
| **è‰¾å®¾æµ©æ–¯è¡°å‡å‚æ•°ä¸åˆç†**  | ä¸­   | ä¸­   | æä¾›å¯é…ç½®å‚æ•°ï¼Œé€šè¿‡ A/B æµ‹è¯•è°ƒä¼˜       |
| **Context Window ç»„è£…åå·®** | ä¸­   | ä½   | å®ç°ç²¾ç¡® Token ç»Ÿè®¡ (ä½¿ç”¨ tiktoken)     |

### 7.2 å·¥ç¨‹é£é™©

| é£é™©                        | å½±å“ | æ¦‚ç‡ | ç¼“è§£ç­–ç•¥                               |
| :-------------------------- | :--- | :--- | :------------------------------------- |
| **Gemini API é™æµ**         | é«˜   | ä¸­   | å®ç°æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæ‰¹é‡å¤„ç†å‡å°‘è°ƒç”¨æ¬¡æ•° |
| **å¤§è§„æ¨¡è®°å¿†æ¸…ç†é˜»å¡**      | ä¸­   | ä½   | ä½¿ç”¨ `pg_cron` å®šæ—¶ä»»åŠ¡ï¼Œåˆ†æ‰¹åˆ é™¤      |
| **Phase 1 Schema å˜æ›´å½±å“** | ä½   | ä½   | ä½¿ç”¨å¤–é”®çº¦æŸï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§           |

---

## 8. é™„å½•

### 8.1 Prompt æ¨¡æ¿å‚è€ƒ

#### Fast Replay Prompt

```
ä½ æ˜¯ä¸€ä¸ªå¯¹è¯æ‘˜è¦ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å¯¹è¯å†å²å‹ç¼©ä¸ºä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚

å¯¹è¯å†å²:
{conversation}

è¦æ±‚:
1. æ‘˜è¦é•¿åº¦ä¸è¶…è¿‡ 200 å­—
2. ä¿ç•™ç”¨æˆ·çš„å…³é”®é—®é¢˜å’Œ Agent çš„æ ¸å¿ƒå›ç­”
3. ä¿ç•™ä»»ä½•é‡è¦çš„å†³ç­–æˆ–ç»“è®º
4. ä½¿ç”¨ç¬¬ä¸‰äººç§°æè¿°

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€æˆ–è§£é‡Šã€‚
```

#### Deep Reflection Prompt

```
ä½ æ˜¯ä¸€ä¸ªç”¨æˆ·ç”»åƒåˆ†æä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹å¯¹è¯ä¸­æå–ç”¨æˆ·çš„å…³é”®ä¿¡æ¯ï¼ŒåŒ…æ‹¬åå¥½ã€è§„åˆ™å’Œäº‹å®ã€‚

å¯¹è¯å†å²:
{conversation}

è¯·ä»¥ JSON æ ¼å¼è¾“å‡ºï¼Œæ ¼å¼å¦‚ä¸‹:
{
    "facts": [
        {
            "type": "preference|rule|profile",
            "key": "åå¥½/è§„åˆ™çš„å”¯ä¸€æ ‡è¯†ï¼Œå¦‚ food_preference",
            "value": {"å…·ä½“çš„åå¥½å†…å®¹"},
            "confidence": 0.0-1.0 çš„ç½®ä¿¡åº¦åˆ†æ•°
        }
    ],
    "insights": [
        {
            "content": "ä»å¯¹è¯ä¸­æç‚¼çš„æ·±å±‚æ´å¯Ÿ",
            "importance": "high|medium|low"
        }
    ]
}

è¦æ±‚:
1. åªæå–æ˜ç¡®è¡¨è¾¾æˆ–å¯é æ¨æ–­çš„ä¿¡æ¯
2. preference: ç”¨æˆ·çš„å–œå¥½ï¼ˆå¦‚é¥®é£Ÿã€é£æ ¼åå¥½ï¼‰
3. rule: ç”¨æˆ·è®¾å®šçš„è§„åˆ™ï¼ˆå¦‚"æ¯å‘¨äº”ä¸å¼€ä¼š"ï¼‰
4. profile: ç”¨æˆ·çš„åŸºæœ¬ä¿¡æ¯ï¼ˆå¦‚èŒä¸šã€ä½ç½®ï¼‰
5. å¦‚æœæ²¡æœ‰å¯æå–çš„ä¿¡æ¯ï¼Œè¿”å›ç©ºæ•°ç»„

è¯·åªè¾“å‡º JSONï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚
```

### 8.2 è¡°å‡ç®—æ³•å‚æ•°è°ƒä¼˜æŒ‡å—

| åœºæ™¯               | æ¨è Î» (decay_rate) | æ¨èé˜ˆå€¼ (threshold) | è¯´æ˜                       |
| :----------------- | :------------------ | :------------------- | :------------------------- |
| **é«˜äº¤äº’é¢‘ç‡ App** | 0.15                | 0.15                 | åŠ é€Ÿé—å¿˜ï¼Œä¿æŒè®°å¿†æ–°é²œåº¦   |
| **ä½äº¤äº’é¢‘ç‡ App** | 0.05                | 0.05                 | å‡ç¼“é—å¿˜ï¼Œä¿ç•™æ›´å¤šå†å²è®°å¿† |
| **æ•æ„Ÿä¿¡æ¯åœºæ™¯**   | 0.3                 | 0.2                  | å¿«é€Ÿæ¸…ç†ï¼Œå‡å°‘éšç§é£é™©     |
| **çŸ¥è¯†ç§¯ç´¯åœºæ™¯**   | 0.02                | 0.02                 | é•¿æœŸä¿ç•™ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±     |

---

## 9. å‚è€ƒæ–‡çŒ®

<a id="ref1"></a>[1] Psychology Today, "Types of Memory," _Psychology Today_, 2024. [Online]. Available: https://www.psychologytoday.com/us/basics/memory/types-of-memory

<a id="ref2"></a>[2] LangChain, "LangGraph Memory Overview," _LangChain Documentation_, 2025. [Online]. Available: https://docs.langchain.com/oss/python/langgraph/memory

<a id="ref3"></a>[3] Google, "ADK Memory Documentation," _Google ADK Docs_, 2025. [Online]. Available: https://google.github.io/adk-docs/sessions/memory/

<a id="ref4"></a>[4] Google, "ADK Sessions Documentation," _Google ADK Docs_, 2025. [Online]. Available: https://google.github.io/adk-docs/sessions/

<a id="ref5"></a>[5] LangChain, "LangGraph Memory Agent," _GitHub Repository_, 2024. [Online]. Available: https://github.com/langchain-ai/memory-agent

<a id="ref6"></a>[6] LangChain, "LangGraph Memory Template," _GitHub Repository_, 2024. [Online]. Available: https://github.com/langchain-ai/memory-template

<a id="ref7"></a>[7] SII-GAIR, "Context Engineering 2.0: The Context of Context Engineering," _SII-GAIR Technical Report_, 2025.

<a id="ref8"></a>[8] H. Ebbinghaus, "Memory: A Contribution to Experimental Psychology," _Teachers College, Columbia University_, 1913.
