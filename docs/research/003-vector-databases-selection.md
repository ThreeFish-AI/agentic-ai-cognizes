---
id: vector-databases-selection
sidebar_position: 3.1
title: 向量数据库的选型决策路径
last_update:
  author: Aurelius Huang
  created_at: 2025-12-25
  updated_at: 2025-12-26
  version: 1.2
  status: Reviewed
tags:
  - Vector Database
  - ANN Algorithm
  - RAG
  - Agentic Infra
---

## **1. 背景与技术原理**

随着大语言模型（LLM）和生成式 AI 的爆发，**向量数据库（Vector Database）** 已从一个小众的推荐系统基础设施组件，跃升为 AI 技术栈中的核心"海马体"（长期记忆）。在 RAG（检索增强生成）架构中，它不仅是外部知识的存储库，更是连接冻结参数的大模型与实时动态世界之间的桥梁，解决了 LLM 的**幻觉（Hallucination）**和**知识截止（Knowledge Cutoff）**两大痛点<sup>[[1]](#ref1)</sup>。

### **1.1 为什么需要向量数据库？**

传统数据库（SQL/NoSQL）的设计初衷是处理结构化数据，它们擅长**精确匹配**（Exact Match，如 WHERE id = 123）或**范围查询**（Range Query，如 price > 100）。然而，人类产生的数据八层以上是非结构化的（文本、图像、音频、视频），这些数据本质上是模糊且充满语义细微差别（Nuances）的。

- **语义鸿沟 (The Semantic Gap):**
  - **问题:** 当用户搜索“手机屏幕裂了”时，基于关键词倒排索引（Inverted Index）的传统搜索引擎（如 Solr, Lucene 默认配置）只能机械匹配包含“屏幕”或“裂”这两个词的记录。
  - **局限:** 这种基于 Token 的匹配逻辑无法召回“碎屏险理赔流程”、“iPhone 维修服务”或“显示器玻璃更换”这些虽不含关键词但意图高度相关的文档。**同义词（Polysemy）**（如“苹果”是水果还是公司？）和**一词多义**问题是传统搜索难以逾越的障碍。此外，对于多语言场景（搜中文出英文文档），关键词匹配更是束手无策。
  - **演进:** 搜索技术经历了从 TF-IDF/BM25（词频统计）<sup>[[2]](#ref2)</sup>到 Dense Retrieval（稠密向量检索），再到目前的 Hybrid Search（混合检索）和 Late Interaction（延迟交互，如 ColBERT）的演进。向量数据库正是 Dense Retrieval 时代的基石。
- **Embedding 的魔力:**
  - **原理:** 向量数据库的核心逻辑是将非结构化数据通过 **Embedding 模型**（如 OpenAI text-embedding-3, BGE-M3, Cohere Embed v3, CLIP for Images）转化为高维浮点数向量（Dense Vectors）。这是一个将离散的符号（文字/像素）映射到连续向量空间的过程<sup>[[3]](#ref3)</sup>。
  - **几何意义:** 在这个通常为 768、1024 或 1536 维的数学空间中，语义相似的数据点在几何距离上更近。例如，在向量空间中，“国王 - 男人 + 女人”的向量坐标会惊人地接近“女王”。这使得计算机可以通过计算向量间的距离（欧氏距离、余弦相似度、内积）来理解“含义”。
- **计算挑战与“维度诅咒”:**
  - **规模:** 向量数据库不仅要存储海量的高维数组，更要在毫秒级时间内，从十亿级（Billion-scale）数据中找到与查询向量距离最近的 Top-K 个结果。
  - **算法困境:** 传统的线性扫描（Brute-force/KNN）复杂度是 O(N)。虽然精确度 100%，但在数据量达到千万级时，计算延迟就会达到秒级甚至分钟级，完全无法满足实时交互需求。为了解决这个问题，需要引入 **ANN（Approximate Nearest Neighbor，近似最近邻）** 算法。这是一种“以精度换速度”的艺术，允许牺牲 1%-5% 的召回率来换取 100 倍的性能提升。

### **1.2 核心索引算法解剖**

在选型之前，必须理解底层的 **ANN（近似最近邻）** 算法，因为不同的数据库往往是对这些基础算法的封装或变种。

- **HNSW (Hierarchical Navigable Small World - 分层导航小世界图):** 目前最主流的图算法，通过构建多层图结构来实现快速导航。
  - **原理:** 模仿社交网络的“六度分隔”理论。它构建多层图结构：
    - **上层 (Upper Layers):** 稀疏的“高速公路”，节点少，连接跨度大，用于快速定位目标向量的大概区域。
    - **底层 (Layer 0):** 包含所有节点的密集“街道”网，用于局部贪婪搜索，精确定位最近邻<sup>[[4]](#ref4)</sup>。
  - **复杂度:** 查询时间复杂度为 O(log N)，速度极快。
  - **关键参数:**
    - M: 每个节点的最大连接数。M 越大，图越密，查询召回率越高，但内存占用增加，构建变慢。
    - ef_construction: 构建索引时的搜索深度。值越大，索引质量越高，但构建时间越长。
    - ef_search: 查询时的搜索队列长度。值越大，召回率越高，但 QPS 下降。
  - **内存代价:** 需要存储图的邻接表，索引构建较慢。假设每个节点连接 M 个邻居，每个连接需要 4 字节（ID），则额外内存开销约为 $N \times M \times 4$ 字节。对于 10 亿向量，这可能意味着几十 GB 的额外 RAM。
  - **适用:** 对延迟要求极高（<10ms），内存充足的场景。
- **IVF (Inverted File Index - 倒排文件索引):**
  - **原理:** 利用 K-Means 聚类将向量空间划分为 nlist 个 Voronoi 单元（聚类中心）<sup>[[5]](#ref5)</sup>。
  - **流程:**
    1. **训练:** 从数据中采样，计算聚类中心。
    2. **分配:** 将每个向量分配到最近的聚类中心。
    3. **查询:** 先找到距离查询向量最近的 nprobe 个聚类中心，然后只在这些聚类包含的向量中进行精确搜索。
  - **优劣:** 内存占用比 HNSW 低，构建速度快。但召回率受 nlist 和 nprobe 参数影响巨大，且在高维空间中可能出现“聚类不均”导致的性能抖动（某些聚类包含过多数据，变成热点）。
- **PQ (Product Quantization - 积量化):**
  - **原理:** 一种有损压缩技术。将高维向量（如 1024 维）切分为 M 个子向量（如 8 个 128 维的子向量），对每个子向量空间进行独立聚类（通常 256 个中心，用 1 字节表示），最终用 M 个聚类中心的 ID 组合代替原始浮点数 <sup>[[5]](#ref5)</sup>。
  - **效果:** 可以将 1024 维 float32 向量（4KB）压缩到仅 M 字节（例如 8 字节或 16 字节），压缩比惊人。
  - **代价:** 精度损失。通常需要配合 **重排序 (Re-ranking)** 机制：先用 PQ 快速召回 Top-N，再从磁盘读取原始向量做精确排序。
- **DiskANN (Vamana Graph):**
  - **原理:** 微软研发的基于磁盘的图算法。它设计了一种特殊的图结构（Vamana），使得在图遍历时，访问的节点虽然物理上分散在磁盘的不同 Page 中，但其邻居节点的布局被优化以减少 I/O 次数。它仅在内存中保留少量高层导航点，绝大多数数据存储在廉价的 NVMe SSD 上<sup>[[6]](#ref6)</sup>。
  - **意义:** 打破了“向量搜索必须全内存”的魔咒，将成本降低了一个数量级，是实现单机百亿级检索的关键技术。

### **1.3 市场格局：多元流派“百家争鸣”**

随着技术的演进，市场格局已从最初的“原生 vs 改良”演变为更加多元的生态，各种流派在性能、功能、成本和易用性之间寻找不同的平衡点：

- **原生派 (Specialized/Native):**
  - **代表:** Milvus, Pinecone, Qdrant, Weaviate, Chroma, Vald, LanceDB。
  - **哲学:** “为了向量而生”。它们通常采用**存算分离**架构，存储引擎、索引构建、查询优化器全部针对高维向量的数学特性进行了重写。例如，它们通常会优先保证向量索引常驻内存、优化向量计算的 SIMD 指令集、采用专用的磁盘索引算法（DiskANN）、针对向量距离计算进行优化，以追求极致的 QPS 和低延迟，以及召回效果。
  - **适用:** 对性能要求极高、数据规模巨大、或者需要专用 AI 功能（如混合搜索调优）的场景。
- **改良派 (Integrated/General-purpose):**
  - **代表:** PostgreSQL (pgvector), VectorChord, Redis, Elasticsearch, ClickHouse, MongoDB Atlas。
  - **哲学:** “向量只是数据的一种类型”。通过插件或新版本，在现有成熟数据库之上增加向量字段和索引功能。这种方式最大的优势是**数据一致性（事务（ACID）支持）**和**架构简洁性（JOIN 操作下沉到 DB 层）**——你不需要为了 AI 功能而维护一套全新的数据库集群。
  - **适用:** 现有技术栈的延伸、中小规模数据、强依赖关系型数据关联过滤的场景。
- **巨头派 (Hyperscaler):**
  - **代表:** Google Vertex AI Vector Search, AWS OpenSearch Service (Serverless), Azure AI Search。
  - **哲学:** 云厂商提供的全托管基础设施，深度集成自家生态（如 Google 的 Gemini 模型链路），解决“最后一公里”的集成问题。通常提供与云存储、云安全体系的无缝对接。
  - **适用:** 云原生、数据规模巨大、需要与云存储、云安全体系无缝对接的场景。
- **计算引擎派 (Serving Engine):**
  - **代表:** Vespa。
  - **哲学:** 不仅仅是存储，更强调在检索过程中进行复杂的实时计算、特征工程和机器学习推理。它们通常用于需要**多阶段排序（Multi-stage Ranking）**的复杂推荐系统。
  - **适用:** 需要复杂计算的场景。
- **基础库 (Library):**
  - **代表:** FAISS, ScaNN, Annoy, USearch。
  - **哲学:** 它们不是完整的数据库系统（无 CRUD、无持久化保证、无副本机制），而是算法核心库。通常作为其他系统的底层引擎或用于离线批处理任务。

## **2. 主流向量库的调研解读（纵向）**

### **2.1 Milvus (Zilliz)**

- **定位:** 云原生、高度可扩展的开源向量数据库，LF AI & Data 基金会毕业项目，是目前全球最成熟、功能最全的开源界的顶级水准代表<sup>[[7]](#ref7)</sup><sup>[[10]](#ref10)</sup>。
- **核心架构:**
  - **彻底的存算分离:** Milvus 2.0 引入了极其复杂的微服务架构，将系统拆分为接入层（Proxy）、协调服务（Coordinator）、工作节点（Worker Nodes）和存储层（Storage）<sup>[[7]](#ref7)</sup>。这种架构虽然复杂，但带来了极致的弹性——你可以单独扩容 Query Node 来应对“双十一”流量，而不影响 Data Node。
    - **接入层 (Access Layer):** 负责协议处理和请求验证，完全无状态。
    - **日志即主干 (Log as the Backbone):** 这是其设计的精髓。Milvus 使用 Pulsar 或 Kafka 作为骨干消息队列。所有的增删改查操作首先作为“日志”写入消息队列，保证了数据流的高吞吐和原子性。
      - **Checkpoints:** 系统定期生成快照（Checkpoints），这不仅是为了缓冲写入压力，更是为了在分布式系统中确保**最终一致性**和**故障恢复能力**（重放日志即可恢复状态）。
    - **Knowhere 向量执行引擎:** Milvus 底层使用 C++ 编写的 Knowhere 引擎，它统一封装了 FAISS, HNSW, Annoy 等算法库，并针对 SIMD (AVX-512) 指令集进行了深度优化，确保了单核性能的极致。
    - **工作节点 (Worker Nodes):**
      - **Query Node:** 负责内存中的查询计算，支持横向扩展以提升 QPS。
      - **Data Node:** 负责消费日志并转化为 Segment 文件（落盘）。
      - **Index Node:** 专门负责构建索引，这是一个 CPU/GPU 密集型任务，独立出来避免影响在线查询性能。
    - **Segment 管理:** 数据被分为 Growing Segments（内存中，可变，用于处理实时写入）和 Sealed Segments（已落盘，不可变，用于构建索引）。查询时会自动合并两者的结果。
      - **对象存储 (Object Storage):** 最终数据（Segment 文件）存储在 S3/MinIO 中，大幅降低了存储成本。
  - **Milvus 2.6 新特性:**
    - **Streaming Node（流批分离）:** 将实时流式写入与批量索引构建分离到不同节点，避免写入流量冲击查询性能，实现更稳定的 P99 延迟。
    - **Woodpecker（零磁盘 WAL）:** 采用云原生的对象存储（S3/MinIO）直接作为 WAL，**彻底摆脱对 Pulsar/Kafka 的依赖**，显著降低运维复杂度和资源消耗（可减少 3-6 个中间件节点），是解决"Milvus 运维复杂"痛点的关键改进。
- **支持的索引类型:**
  - **内存索引:** HNSW, HNSW_SQ (4x 压缩), HNSW_PQ (8-32x 压缩), HNSW_PRQ (残差量化), IVF_FLAT, IVF_SQ8, IVF_PQ, SCANN, FLAT
  - **磁盘索引:** DiskANN (Vamana Graph)<sup>[[6]](#ref6)</sup>
  - **GPU 索引:** GPU_IVF_FLAT, GPU_IVF_PQ, GPU_CAGRA (NVIDIA RAFT)
  - **稀疏/二进制向量:** SPARSE_INVERTED_INDEX, SPARSE_WAND, BIN_FLAT, BIN_IVF_FLAT
- **优势:**
  - **极致扩展性:** 架构决定上限。由于状态下沉到 S3 和 Etcd，计算节点可以近乎线性地扩展，能够稳定支撑十亿（Billion）甚至万亿（Trillion）级向量规模。
  - **硬件加速与高级索引:** 支持 GPU 加速索引（RAFT 算法）<sup>[[8]](#ref8)</sup>，支持 DiskANN，使用 NVMe SSD 代替昂贵的 DRAM 存储向量数据，用相对低成本的硬件处理海量数据（成本可降低 10 倍，延迟会从微秒级增加到毫秒级）。
  - **一致性模型:** Milvus 支持多种一致性级别：Strong, Bounded, Session, Eventually（默认通常是 Bounded Staleness，有界过时，这意味着写入的数据可能需要几百毫秒（基于时间戳 TSO）才能被搜索到。如果业务强依赖“写完即读”，需在查询时手动调整一致性级别，但这会牺牲性能）。
  - **企业级特性:** 完整的 RBAC 权限控制、多租户资源隔离（Partition Key）、CDC（变更数据捕获）支持。
  - **生态与工具:** 拥有可视化管理工具 Attu，以及极其完善的 Python/Java/Go SDK。
- **劣势:**
  - **运维极其复杂:** 部署和维护一套高可用的 Milvus Cluster 是一项艰巨的任务，需要维护 Etcd (元数据)、Pulsar/Kafka (消息流)、MinIO/S3 (对象存储) 这一整套依赖组件。排查问题时需要在多个组件日志间跳转。需要专业的 K8s 和中间件运维能力。
  - **资源消耗:** Milvus 是一个“重型”系统，即便在空载状态下，为了维持微服务架构的运行，也需要消耗相当数量的 CPU 和内存资源用于各组件间的心跳和通信（最小化的高可用部署通常需要至少 3 个 Etcd 节点、3 个 Pulsar/Kafka 节点、以及多个 Milvus 组件节点。冷启动资源占用可能高达 16GB+ RAM）。
  - **学习曲线:** 概念众多（Collection, Partition, Segment, Channel），配置参数极其丰富，新手容易迷失。

### **2.2 Pinecone**

- **定位:** 闭源、SaaS 优先的“向量数据库即服务”，是 OpenAI 首选的合作伙伴，主打“开发者体验”。
- **核心架构:**
  - **Serverless 架构 (2.0):** Pinecone Serverless 彻底重构了底层，实现了存储与计算的物理隔离。
    - **分层存储 (Tiered Storage):** 向量数据首先写入 Blob Storage (S3)。在查询时，热数据会被加载到计算节点的 NVMe SSD 缓存或内存中。这种机制允许 Pinecone 提供极其廉价的存储成本（约为传统 Pod 模式的 1/50）。
    - **计算层:** 索引构建和查询计算按需启动。这意味着你不需要为闲置的 Pod 付费，且系统可以根据流量自动瞬间扩容（Scale-to-zero 能力）。解决了“向量数据库不仅贵而且利用率低”的行业痛点。
    - **多租户隔离:** 利用 Kubernetes 命名空间和 cgroups 技术，在共享的计算池中实现租户间的性能隔离。
  - **索引维护:** 不同于开源库需要手动触发索引重建，Pinecone 维护了一套闭源的专有索引算法，针对云环境的冷热数据调度进行了深度优化。Pinecone 后台有自动的索引压缩和整理机制，确保持续写入下的查询性能不退化。
- **优势:**
  - **DX (开发者体验) 第一:** 几乎是零配置。注册账号 -> 获取 API Key -> 创建 Index -> 写入数据，全过程仅需几分钟。
  - **Integrated Inference (内置推理):** 支持直接发送原始文本，Pinecone 自动调用内置 Embedding 模型生成向量，简化开发流程。
  - **Dense + Sparse 向量:** 同时支持稠密向量（语义搜索）和稀疏向量（关键词匹配），实现原生 Hybrid Search。
  - **内置重排序 (Reranking):** 支持在搜索中直接调用重排序模型，提升检索精度。
    | 重排序模型 | 最大 Token | 最大文档数 | 特点 |
    |------------|------------|------------|------|
    | `cohere-rerank-3.5` | 40,000 | 200 | 高精度、多字段支持 |
    | `bge-reranker-v2-m3` | 1,024 | 100 | 平衡性能与精度 |
    | `pinecone-rerank-v0` | 512 | 100 | Pinecone 自研、低延迟 |
  - **弹性与成本:** Serverless 模式对初创公司极度友好（按读写单位 WU/RU 付费），大大降低了 POC 阶段的成本风险，且不需要预估容量。
- **劣势:**
  - **数据主权与合规:** 作为一个纯 SaaS 服务，数据必须离开企业内网存储在 Pinecone 的云端（通常是 AWS/GCP 的美东/欧西区域）。对于金融、医疗、政府或对数据隐私极其敏感的国内企业，这通常是一票否决项。
  - **黑盒:** 无法进行深度的参数调优，性能瓶颈难以排查。出现性能抖动或诡异结果时，由于无法接触底层日志和配置，排查问题比较被动，只能依赖工单支持。
  - **Pod vs Serverless:** 这是一个关键的选型点。
    - **Pod-based:** 性能稳定，延迟极低（<10ms），但通过预置容量付费，闲置成本高。适合稳定高流量的生产环境。
    - **Serverless:** 按需付费，成本极低，但存在**冷启动**问题。如果索引长时间未访问，首次查询可能会有数百毫秒的延迟。不适合对 P99 延迟要求极高的实时推荐场景。

### **2.3 Weaviate**

- **定位:** 开源、AI 原生，强调“知识”不仅仅是向量，深度融合了倒排索引与向量索引，更像是一个“带有向量能力的知识图谱”。
- **核心架构:**
  - **模块化 (Module System):** Weaviate 允许在数据库内部加载 text2vec-openai, img2vec 或 generative-cohere 等模块。这意味着你可以直接把原始文本/图片发给数据库，数据库自己去调用模型生成向量并存储，甚至在检索后直接调用 LLM 生成答案（RAG 内置化）。大大简化应用层的代码逻辑。
  - **混合搜索 (Hybrid Search):** 这是 Weaviate 的王牌。它在底层同时维护了倒排索引（BM25，用于稀疏检索）和 HNSW 向量索引（用于稠密检索）。查询时，它会分别执行两个搜索，然后通过 **RRF (Reciprocal Rank Fusion)** 或 **Alpha 加权** 算法合并结果。
  - **对象存储模型:** 它的数据存储是以 Class（类）为单位，支持定义 Schema 和 Cross-Reference（跨类引用）。这使得它不仅能搜向量，还能像图数据库一样进行多跳查询。例如：“查找主要讨论人工智能且作者是 MIT 教授的文章”。
  - **Ref2Vec:** Weaviate 的特色功能，允许将一个对象“向量化”为它引用的其他对象的聚合。这对于推荐系统非常有用（例如，用户的向量 = 他喜欢的文章向量的平均值）。
  - **Schema First:** 强类型系统，要求先定义 Schema (Class, Properties)。这有助于数据治理，但也降低了灵活性。
  - **Named Vectors (多向量检索):** 同一对象可以存储多个不同的向量（如标题向量、内容向量、图片向量），支持 Multi-target Vector Search，实现更精细的检索控制。
  - **Dynamic Index:** 智能索引切换，小数据集使用 Flat 索引，数据量增长后自动切换到 HNSW 索引，平衡性能和资源。
  - **量化技术对比:**
    | 量化方法 | 压缩比 | 召回影响 | 特点 |
    |----------|--------|----------|------|
    | **PQ** (Product Quantization) | ~24x | 中等 | 需要训练，适用 HNSW |
    | **BQ** (Binary Quantization) | 32x | 较大 | 无训练，V3 Embedding 模型效果好 |
    | **SQ** (Scalar Quantization) | 4x | 较小 | 8-bit 压缩，256 个桶，**推荐** |
    | **RQ** (Rotational Quantization) | 4x/32x | 较小 | 无训练，即时启用 |
  - **量化策略:** Weaviate 使用**过度获取 + 重排序**策略来弥补量化导致的精度损失。
  - **类 GraphQL 接口:** 所有的查询都通过 GraphQL 进行，类似于图数据库的查询体验。虽然灵活性极高，但对于习惯 SQL 或简单 REST API 的团队来说，构建复杂的 Query 可能需要一定的学习成本。
- **优势:**
  - **混合搜索之王:** 原生支持 **RRF（Reciprocal Rank Fusion）** 融合算法。Weaviate 在底层同时维护了倒排索引（BM25）和向量索引。在进行查询时，用户可以通过 alpha 参数平滑调节关键词匹配和语义匹配的权重，并通过 RRF（Reciprocal Rank Fusion）算法合并结果。这是目前提升 RAG 准确率最有效的手段之一。
    > [!NOTE] > 由于关键词搜索（BM25）的分数通常无上限，而向量相似度是归一化的（通常 0-1），直接加权求和效果往往不佳。RRF 通过基于排名的融合算法，完美解决了不同检索器分数标度不一致的问题，显著提升 RAG 的召回准确率。
  - **结构化数据过滤:** 由于其 Schema 的设计，处理带有复杂元数据（如作者、时间、标签、分类）的结构化过滤时，效率远高于单纯的向量库（后者通常使用 Pre-filtering 或 Post-filtering，效率较低）。
- **劣势:**
  - **写入放大（Write Amplification）:** 由于同时维护多种索引（向量+倒排+正排），Weaviate 的写入开销较大。在进行大批量数据导入、大规模数据迁移（Backfill）时，务必调整 ef_construction 和 max_connections 参数，并关闭自动刷新，否则写入速度会非常慢（在同等硬件下通常慢于纯向量库）。
  - **资源消耗:** Go 编写的内核在高并发场景下表现良好，但内存管理不如 Rust/C++ 精细，GC 可能会导致偶尔的延迟抖动。
  - **学习曲线:** 它的 API 风格（特别是 GraphQL）对于习惯了 RESTful 或 SQL 的开发者来说有一定门槛。需要花时间理解它的 Class, Property, Cross-Ref 概念。

### **2.4 Qdrant**

- **定位:** 采用 Rust 编写的高性能开源向量数据库，主打高性能和安全性<sup>[[11]](#ref11)</sup>。
- **核心架构:**
  - **Rust & 内存管理:** 受益于 Rust，Qdrant 没有 GC 暂停问题。它大量使用 mmap 技术，将磁盘文件直接映射到虚拟内存空间。这使得 Qdrant 可以在内存不足时，自动利用操作系统的 Page Cache 机制，平滑地退化到磁盘读取模式，而不会像纯内存数据库那样直接 OOM（内存溢出），在性能和成本间取得平衡。
  - **HNSW 优化:** Qdrant 对 HNSW 进行了定制优化，支持在图遍历过程中进行**动态预过滤 (Pre-filtering)**。它通过维护额外的连接（Links）来确保即使在严格过滤条件下，图的连通性也不会被破坏，从而使得带有复杂过滤条件的向量搜索依然能保持极高的效率。
  - **ACORN Search Algorithm:** 专为复杂过滤场景设计的搜索算法，解决传统 HNSW 在高过滤率下图连通性被破坏的问题，显著提升过滤查询性能。
  - **Tenant Index:** 支持基于租户 ID 的索引优化，适合多租户 SaaS 应用，避免跨租户数据扫描。
  - **量化策略:** 支持多种量化方式：Scalar Quantization (int8, 4x 压缩)、Binary Quantization (1-bit, 32x 压缩)、1.5-bit/2-bit Quantization（精度与压缩的平衡）、Product Quantization (PQ)、Asymmetric Quantization。
  - **FastEmbed 集成:** 原生集成 FastEmbed 库，支持在数据库端直接生成向量，简化应用开发。
  - **Segment Merge:** Qdrant 类似于 Lucene，通过合并小的 Segment 来优化读取。在写入高峰期，可以暂时调大 optimizers_config 的阈值，避免频繁合并影响写入性能。
- **优势:**
  - **单机性能怪兽:** 在同等硬件下，Qdrant 往往能提供比 Java/Go 竞品更高的 QPS。
  - **部署灵活性:** 它既支持像 SQLite 一样的本地库模式（直接在 Python 进程中运行），也支持分布式集群模式。这让从原型到生产的迁移变得平滑。
  - **资源效率:** 相比 Milvus，Qdrant 更加轻量，对内存和 CPU 的利用率极高，非常适合边缘计算（使计算尽可能靠近数据源、以减少延迟和带宽使用的网络理念）或资源受限的场景。
  - **二进制量化 (Binary Quantization):** 支持将浮点向量压缩为二进制位（0/1），极大地减少了内存占用（比如在 1000 万以上数据量时开启 **Binary Quantization**，或 **Scalar Quantization**，如果维度 > 1024，这可以将内存占用减少 4x-32x，且对召回率影响极小（通常 < 2%）。同时利用 Hamming 距离计算，速度极快，且在重排序（Rescoring）机制下能保持较高的检索精度。
- **劣势:**
  - **分布式成熟度:** 虽然支持分布式部署（Sharding/Replication），但相比 Milvus 这种天生分布式的架构，Qdrant 的超大规模（百亿级）集群运维经验和公开案例相对较少，分片管理和再平衡（Rebalancing）机制仍在完善中。

### **2.5 Chroma**

- **定位:** 面向开发者的“极简”向量数据库，主打 Python 生态和本地开发，是一些 AI 应用开发框架（如 LangChain）的默认搭档。
- **核心架构:**
  - **架构演进:** 早期 Chroma V1 简单地封装了 DuckDB（用于元数据）和 ClickHouse（用于向量）。但这种架构在扩展性上存在局限。Chroma V2 正在向更原生的分布式架构演进，使用了自研的 Log service 和 Segment manager。
  - **嵌入式优先:** 默认情况下，Chroma 是一个嵌入式数据库，运行在应用进程内。这意味着它没有网络开销，非常适合本地开发和测试。
- **优势:**
  - **DX（开发者体验）极致:** pip install chromadb 即可使用，API 设计极其符合 Python 工程师的直觉。它内置了轻量级的嵌入模型（Sentence Transformers），甚至不需要配置 API Key，就能在 Notebook 中跑 Demo。这是目前开发 RAG 原型最快的路径。
  - **生态集成:** 与 LangChain, LlamaIndex, AutoGPT 等新一代 AI 框架的集成最为紧密，社区教程极多。
  - **无需独立进程:** 在默认模式下，它是一个嵌入式数据库，运行在应用进程内，没有网络开销，也没有独立的运维负担。
- **劣势:**
  - **生产环境挑战:** 早期版本的 Chroma 使用 ClickHouse/DuckDB 作为底层，目前正在自研存储引擎。在处理数千万级数据时，内存占用和查询延迟会出现明显瓶颈。
  - **并发能力:** 在嵌入式模式下，它是单进程的，无法处理高并发请求。即便是服务端模式，目前的性能和稳定性也尚未达到 Milvus/Qdrant 的水平。
  - **功能缺失:** 虽然目前推出了服务端模式和云服务，但在高可用（HA）、多租户隔离、细粒度权限管理等企业级特性上，相比 Milvus/Pinecone 仍有差距。它更像是一个“开发库”而非“生产级数据库”。

### **2.6 PGVector (PostgreSQL)**

- **定位:** 不是一个独立的数据库，是 PostgreSQL 的开源插件，让 PG 具备存储和检索向量的能力<sup>[[14]](#ref14)</sup>。
- **核心架构:**
  - **原生集成:** 它引入了 vector 数据类型，并利用 PG 的现有存储引擎（Heap Files）和缓冲池（Shared Buffers）。这意味着向量数据和其他关系型数据存储在同一个 Page 中，遵循相同的 WAL 日志和 MVCC 机制。
  - **索引机制:**
    - **IVFFlat:** 基于聚类的倒排索引。优点是构建快，内存占用小。缺点是召回率受聚类中心数量（lists）影响大，且不适合高频更新（更新后会导致聚类失效，需要 REINDEX）。
    - **HNSW:** 图索引。优点是查询快，召回率高，对更新友好（支持并行构建）。缺点是构建极慢，内存占用高，HNSW 是一种内存密集型索引，对 PG 的 Shared Buffers 压力巨大。
  - **并行索引构建:** 支持 `max_parallel_maintenance_workers` 配置，利用多核 CPU 加速 HNSW 索引构建。
  - **新数据类型 (v0.7.0+):**
    - **halfvec:** 16-bit 半精度向量，内存占用减半（2x 压缩），适合对精度要求不高的场景。
    - **sparsevec:** 稀疏向量类型，仅存储非零元素，适合 BM25 和 TF-IDF 等稀疏表示。
    - **bit:** 二进制向量，支持 Hamming 距离计算。
  - **迭代索引扫描 (v0.8.0+):** 近似索引的过滤会在索引扫描**后**应用，可能导致返回结果不足。迭代扫描可自动扫描更多索引直到获得足够结果：
    - `SET hnsw.iterative_scan = strict_order;` — 结果按距离精确排序
    - `SET hnsw.iterative_scan = relaxed_order;` — 允许轻微乱序，但召回更高（推荐）
      | 参数 | 描述 | 默认值 |
      |------|------|--------|
      | `hnsw.max_scan_tuples` | HNSW 最大扫描元组数 | 20000 |
      | `ivfflat.max_probes` | IVFFlat 最大探测列表数 | 全部 |
- **优势:**
  - **单一技术栈红利:** 这一点怎么强调都不为过。使用 PGVector 意味着你不需要引入新的运维组件，不需要做数据同步（ETL），可以直接利用 PG 强大的事务（ACID）、备份恢复（PITR）、复制（Replication）和行级安全（RLS）机制。
  - **便捷的混合查询:** 你可以写出 `SELECT * FROM items WHERE category_id = 5 AND created_at > '2023-01-01' ORDER BY embedding <-> query_vector` 这样的 SQL，完美结合关系型数据和向量数据，无需应用层做 Join。
- **劣势:**
  - **资源争抢 (Noisy Neighbor):** 向量搜索是计算密集型（大量的距离计算）和内存密集型（访问大量随机 Page）操作。特别是 HNSW 索引构建和查询时，大量的随机读写会频繁置换 PostgreSQL 的 **Shared Buffers**，导致核心业务表（如订单表、用户表）的热数据被挤出内存，从而引发整体数据库的延迟抖动。**建议:** 在生产环境中使用只读副本（Read Replica）专门处理向量搜索流量，以物理隔离对主库 OLTP 业务的影响。
    - **TOAST 表机制:** PG 会将大字段（如高维向量）存储在 TOAST 表中（超过一定大小的数据会被压缩并存储在 TOAST 表中）。每次查询都需要从 TOAST 表解压数据，这增加了 I/O 开销。因此，PGVector 在处理超高维向量（如 4096 维）时性能下降明显。
  - **构建速度与膨胀:** HNSW 索引在 PG 中构建速度较慢，且占用空间较大。在频繁更新向量的场景下，PG 的 VACUUM 机制可能成为瓶颈，导致表膨胀（Bloat），需要更频繁的维护。
  - **维度限制:** 最大支持 16,000 维向量。
- **生产环境实践 (Production Reality):**
  - **SQL 示例:**  
    CREATE EXTENSION vector;  
    CREATE TABLE items (id bigserial PRIMARY KEY, embedding vector(1536));  
    -- HNSW 索引构建关键配置  
    CREATE INDEX ON items USING hnsw (embedding vector_l2_ops) WITH (m = 16, ef_construction = 64);
  - **参数调优:**
    - 构建索引时：务必临时调大 maintenance_work_mem（例如设为系统 RAM 的 20%），并增加 max_parallel_maintenance_workers，否则 HNSW 索引构建可能需要数小时。
    - 查询时：调整 hnsw.ef_search 参数，平衡精度和速度。
  - **真空清理 (Vacuum):** 频繁的向量更新会导致表膨胀。必须监控 PG 的 Auto-vacuum 进程，确保它能及时清理死元组，否则索引性能会急剧下降。

### **2.7 VectorChord (pgvecto.rs)**

- **定位:** 基于 Rust 开发的高性能 PostgreSQL 向量扩展，是 pgvecto.rs 的继任者，旨在成为 pgvector 的"高性能替代者"<sup>[[15]](#ref15)</sup>。
- **核心架构:**
  - **Rust Core & pgrx:** 不同于 pgvector 的 C 语言实现，VectorChord 利用 Rust 语言的内存安全特性和 SIMD 指令集优化，通过 pgrx 框架集成到 Postgres 中。
  - **原生量化 (Native Quantization):** 它的核心杀手锏是**深度集成的量化索引**。它不只是简单的 HNSW，而是原生支持标量量化 (Scalar Quantization) 和二进制量化 (Binary Quantization)。这意味着它可以在索引构建阶段就将向量压缩 4x 到 32x，极大地减少了内存占用。
  - **RaBitQ 算法:** 实现了具有理论误差边界保证的量化算法<sup>[[16]](#ref16)</sup>，是 VectorChord 的核心技术创新，在保持高召回率的同时实现极致压缩。
  - **索引类型:**
    - **vchordrq:** 基于 RaBitQ 的量化索引，高压缩率和高查询性能。
    - **vchordg (v0.5.0+):** 基于磁盘的图索引，内存消耗更低，适合超大规模数据。
  - **vchordg 图索引参数:**
    | 参数 | 描述 | 默认值 | 建议 |
    |------|------|--------|------|
    | `bits` | RaBitQ 量化比率 | 2 | 2 = 高召回，1 = 低内存 |
    | `m` | 每顶点最大邻居数 | 32 | 对应 HNSW/DiskANN 的 M |
    | `ef_construction` | 构建时动态列表大小 | 64 | 越大越慢但质量越好 |
    | `alpha` | 剪枝时的 alpha 值 | [1.0, 1.2] | 对应 DiskANN 的 alpha |
  - **预过滤 (Prefilter v0.4.0+):** 通过 `SET vchordrq.prefilter = on` 启用，允许向量索引利用过滤条件进行剪枝。在 1% 选择率时可获得 200% QPS 提升，10% 选择率时可获得 5% QPS 提升。
  - **Similarity Filter:** 支持在向量搜索中进行相似度阈值过滤，仅返回满足相似度要求的结果。
  - **磁盘友好 (Disk-Efficient):** 引入了类似 DiskANN 的算法设计（VChord 索引），优化了 I/O 访问模式，使其在 SSD 上的表现远超传统 IVFFlat。
- **优势:**
  - **性能碾压:** 在某些基准测试中，得益于激进的量化策略，其 QPS 和构建速度可以达到 pgvector 的数倍，甚至在大规模数据集上接近专用向量数据库（如 Qdrant）。
  - **成本效益:** 极高的压缩率意味着你可以在同样的内存硬件中存储更多的向量，显著降低 TCO。
- **劣势:**
  - **成熟度与生态:** 相比 pgvector 已成为 AWS/Google Cloud RDS 的标配，VectorChord 目前主要需要自建或使用特定的托管服务，社区生态（如 ORM 支持、文档丰富度）尚在快速成长期。
  - **开源协议:** 核心项目 pgvecto.rs 采用 Apache 2.0 协议，但部分高级特性或企业版组件可能涉及不同的授权模式，企业选型时需确认。

### **2.8 FAISS (Meta)**

- **定位:** Facebook AI Research 开源的向量搜索库（Library），**不是数据库**<sup>[[22]](#ref22)</sup>。
- **核心架构:** 提供了一系列高效的索引算法（IVF, PQ, HNSW, LSH 等）的 C++ 实现及 Python 封装。
- **优势:**
  - **算法基石:** 它是许多向量数据库（如 Milvus 早期, Pinecone 早期）的底层引擎。
  - **极致性能:** 在单机环境下，FAISS 往往代表了算法性能的上限，特别是其对 GPU 的支持非常完善，能在单卡上处理十亿级量化索引。
- **劣势:**
  - **功能缺失:** 没有 CRUD（增删改查），不支持实时插入（通常需要重建索引），没有分布式，没有高可用，没有元数据过滤。掉电即失。主要用于构建自己的向量检索系统或离线批处理任务。

### **2.9 Elasticsearch (8.x+)**

- **定位:** 老牌搜索霸主，8.0 版本后原生内置向量搜索能力<sup>[[17]](#ref17)</sup>。
- **核心架构:**
  - **Lucene 核心:** 基于 Lucene 的 HNSW 实现。这意味着 ES 的向量搜索共享了 Lucene 的 segment merge 机制。向量数据被视为一种特殊的 Field。
  - **量化索引 (8.x):**
    - **int8_hnsw:** 8-bit 标量量化 HNSW 索引，内存占用降低 4x，查询速度提升。
    - **int4_hnsw:** 4-bit 量化 HNSW 索引，进一步压缩，适合对精度要求不高的场景。
- **搜索模式:** 支持 kNN Query (近似搜索) 和 script_score Query (精确暴力扫描)，需根据场景选择。
- **优势:**
  - **文本搜索霸主:** 如果你的应用严重依赖全文检索（分词、模糊匹配、高亮、词干提取），ES 是不可替代的。向量搜索可以作为 rescore 阶段的补充，增强语义召回。这种 **“BM25 (Keyword) + kNN (Vector)”** 的组合是目前最稳健的搜索策略。
  - **成熟度:** 拥有最完善的监控、可视化（Kibana）和运维工具链。企业内部通常已有 ES 集群，复用成本低。
- **劣势:**
  - **资源沉重:** 向量索引是堆外内存（Off-heap），但频繁的搜索会产生大量的堆内对象，导致 JVM GC 压力增大。JVM 调优复杂（ES 的 GC 问题在向量搜索的高内存压力下可能更加频繁）。
  - **性能折中:** 在纯向量搜索的 QPS 和延迟上，通常不如专门优化的 C++/Rust 向量库。ES 的 HNSW 实现目前在性能上相比 FAISS 或 Milvus 仍有差距。
  - **查询语法性能:** 使用 kNN 查询（近似搜索）速度快，但只能近似搜索。如果使用 script_score（精确暴力扫描）则性能极差，需谨慎区分。在 8.x 早期版本，knn 查询不支持与一些复杂的 filter 高效结合（Pre-filtering 性能差），虽然新版本有所改进，但仍需测试。
  - **Segment Merge:** Lucene 的不可变 Segment 机制意味着每次 Update 都是 Delete + Insert，导致后台 Merge 压力大，影响实时写入吞吐。

### **2.10 MongoDB Atlas**

- **定位:** MongoDB Atlas 云数据平台的全托管向量搜索服务。它是 MongoDB 聚合管道（Aggregation Pipeline）中的一个功能模块（不是一个独立的数据库）<sup>[[18]](#ref18)</sup>。
- **核心架构:**
  - **Lucene 集成:** 向量搜索底层依赖于 Apache Lucene 库。MongoDB 通过内部的同步机制（基于 Oplog），将主数据库（mongod 进程）中的数据实时同步到 Sidecar（侧车）搜索节点（mongot 进程）中，并在那里构建 HNSW 索引。
  - **聚合管道 ($vectorSearch):** 向量搜索并非独立的 API，而是作为聚合管道的第一阶段 $vectorSearch 存在。这意味着你可以将向量搜索的结果无缝传递给后续的 MongoDB 阶段（如 $match, $project, $lookup），在数据库内部完成复杂的“向量 + 标量 + 关联查询”逻辑，无需应用层组装。
  - **预过滤 (Pre-filtering):** 利用 Lucene 的能力，支持在 HNSW 遍历前进行高效的元数据过滤（基于 MQL 语法），这对于多租户或带权限控制的 RAG 系统至关重要。
  - **搜索模式:**
    - **ANN (Approximate):** 默认的近似最近邻搜索，使用 HNSW 索引，速度快但可能有召回损失。
    - **ENN (Exact):** 精确最近邻搜索，暴力扫描所有向量，100% 召回但速度较慢，适合小规模或对精度要求极高的场景。
- **优势:**
  - **开发者体验**: 对于已经使用 MongoDB 的团队，学习成本几乎为零。无需引入新的数据库组件，无需维护 ETL 管道，直接在现有的 JSON 文档中增加一个 embedding 字段即可。
  - **灵活性**: JSON 文档模型天生适合存储非结构化数据的元数据。你可以随时增加字段用于过滤，而无需像 SQL 数据库那样执行 ALTER TABLE。
  - **统一技术栈**: 实现了 Operational Data（业务数据）和 Vector Data（向量数据）的物理统一，彻底解决了数据一致性问题。
- **劣势:**
  - **Vendor Lock-in**: 目前向量搜索功能深度绑定 MongoDB Atlas 公有云服务。虽然 MongoDB 社区版开源，但 Vector Search 功能并不包含在自托管的社区版中（需依赖 Atlas 环境）。
  - **性能天花板**: 虽然基于 Lucene 的 HNSW 性能不错，但在亿级以上规模或对 QPS 有极致要求（如 10ms 内返回）的场景下，相比 C++/Rust 编写的原生向量库（如 Milvus, Qdrant）仍有差距。
- **生产环境实战 (Production Reality):**
  - **资源隔离**: 早期版本中，搜索进程与数据库进程共享资源，容易发生争抢。现在的 Atlas 架构支持独立搜索节点 (Dedicated Search Nodes)，允许独立扩展搜索计算资源，而不影响核心数据库的读写性能。架构师在选型时务必评估是否需要开启此选项（虽有额外成本，但生产环境建议开启）。
  - **索引开销**: Lucene 的索引段合并（Segment Merge）机制在处理高频更新时会消耗大量 CPU。如果你的业务场景是“每秒更新数万条向量”，MongoDB 的写入延迟可能会增加。

### **2.11 Vertex AI Vector Search (Google)**

- **定位:** Google Cloud 全托管服务，原 Matching Engine<sup>[[19]](#ref19)</sup>。
- **核心架构:**
  - **ScaNN:** Google 独家的 ScaNN 算法，通过**各向异性量化（Anisotropic Quantization）**在压缩率和召回率之间取得了业界领先的平衡<sup>[[9]](#ref9)</sup>。它能理解向量在空间中的分布密度，从而更智能地量化，损失更少的信息。
  - **全托管:** 完全屏蔽底层基础设施，自动处理分片、复制和自动扩缩容。
- **优势:**
  - **极高性能:** 能够以极低的延迟处理数百万 QPS，专为亿级 QPS 设计。如果数据量达到亿级且并发极高，它是最稳的选择。
  - **生态整合:** 与 Vertex AI 的其他服务（Embedding API, Gemini, BigQuery）无缝集成。你可以直接将 BigQuery 中的数据同步到 Vector Search，实现数仓与向量库的联动，快速构建企业级 RAG 管道。
- **劣势:**
  - **Vendor Lock-in:** 深度绑定 Google Cloud 生态，难以迁移到 AWS 或本地。
  - **更新延迟:** 它的索引更新机制传统上更适合批量更新（Batch Update）。虽然现在支持流式更新（Streaming Update），但相比 Redis 或 Milvus 的实时性，仍有一定限制（如数秒的可见性延迟），不适合需要“写入即刻可查”（Read-your-writes）强一致性的业务场景。

### **2.12 Redis Stack (RediSearch)**

- **定位:** 基于内存的高性能键值数据库，通过 RediSearch 模块支持向量。
- **核心架构:**
  - **纯内存:** 所有向量数据和索引常驻内存。利用 Redis 现有的 Hash 或 JSON 数据结构存储向量。
- **优势:**
  - **速度极快:** 真正的内存级延迟（sub-millisecond），非常适合实时推荐、实时反欺诈、会话状态管理等对延迟极其敏感的场景。
  - **通用性:** 可以同时作为**缓存**、**消息队列**和**向量库**使用，简化架构。如果你的技术栈里已经有 Redis，开启这个模块的成本很低。
- **劣势:**
  - **成本昂贵:** 内存是昂贵的资源。存储十亿级向量（假设每向量 1KB，加上索引开销）需要海量 RAM，TCO（总拥有成本）极高。它不适合存储海量的“冷”数据。
  - **持久化弱:** 虽然有 RDB/AOF，但相比专用数据库，在大规模数据下的快照恢复时间较长，冷启动慢。

### **2.13 ClickHouse**

- **定位:** 实时 OLAP 分析数据库，近期增加了向量支持<sup>[[21]](#ref21)</sup>。
- **核心架构:**
  - **列式存储:** 擅长高速扫描。
  - **向量支持:** 提供了 Distance 函数（L2, Cosine）和实验性的向量索引。
  - **索引类型:**
    - **usearch:** 基于 usearch 库的 HNSW 实现（实验性）。
    - **Annoy:** Approximate Nearest Neighbors Oh Yeah 索引。
  - **QBit 量化 (Quantized Bit):** 支持可变精度量化（4-64 bit），通过 `quantization` 参数配置，减少 I/O 和内存占用，在精度和速度间权衡。
- **优势:**
  - **向量分析 (Vector Analytics):** 如果你需要对向量数据进行复杂的聚合分析——例如“统计不同类别商品的平均向量中心”、“分析用户兴趣向量随时间的漂移”、“计算全量数据的 K-Means 聚类”，ClickHouse 是无敌的。
  - **高吞吐扫描:** 对于不需要极高 QPS 但需要处理海量数据精确扫描（Exact Search）的场景，其线性扫描速度极快。
- **劣势:**
  - **非 ANN 专长:** 虽然支持索引，但其核心设计并非为毫秒级高并发 ANN 搜索设计。点查（Point Query）延迟通常高于专用向量库。不适合作为在线服务的检索后端。

### **2.14 LanceDB**

- **定位:** 基于 Lance 数据格式的嵌入式/Serverless 向量数据库，主打多模态数据管理和“零拷贝”。
- **核心架构:**
  - **Lance 格式:** 一种专为 ML 设计的列式存储格式，兼容 Apache Arrow，支持 **零拷贝（Zero-copy）** 读取。这意味着数据可以直接从磁盘/S3 映射到内存中供 CPU 使用，无需序列化/反序列化开销。
  - **存算分离:** 数据存在 S3/磁盘，计算按需进行。
- **优势:**
  - **嵌入式与云原生:** 可以像 SQLite 一样嵌入在 Python 进程中，也可以扩展到云端 S3。
  - **多模态友好:** Lance 格式不仅高效存储向量，还能高效管理向量对应的原始数据（图片、视频帧），避免了“向量存库里，图片存 S3”带来的管理割裂。
  - **Time Travel:** 原生支持数据版本控制，这对于复现 AI 实验结果非常重要。
  - **Hybrid Search:** 支持向量搜索 + 全文搜索的混合检索模式，提升召回准确率。
  - **Reranking:** 内置 Reranker 支持，可在检索后对结果进行重排序优化。
- **劣势:**
  - **早期阶段:** 社区和生态相比 Milvus/Chroma 还处于快速成长期，文档和第三方集成正在完善中。

### **2.15 Vespa**

- **定位:** 源自 Yahoo 的大数据实时计算与服务引擎。
- **核心架构:**
  - **复杂计算流水线:** 支持在查询时进行多阶段计算（Tensor 运算、机器学习模型推理）。
- **优势:**
  - **不仅是搜索:** 可以在检索阶段直接运行复杂的排序模型（如 LightGBM, DNN）。典型的“召回+粗排+精排”流程可以在 Vespa 内部一次性完成，减少了网络传输开销。
  - **真·实时:** 专为高频写入和更新设计，数据立即可查，没有“索引构建延迟”窗口。
- **劣势:**
  - **极其复杂:** 配置文件极其繁琐（需要编写 Search Definition），学习曲线极陡峭。除非你有超大规模且逻辑复杂的推荐/广告业务，否则不要轻易触碰。

## **3. 主流向量库的差异盘点（横向）**

为了更全面地展示差异，这里对上述选型在**基础运维**和**技术特性**两个维度进行了对比。

### **3.1 基础维度与运维成本**

| 数据库          | 核心语言    | 部署架构          | 托管服务           | 运维复杂度                      | 典型适用场景                                             |
| :-------------- | :---------- | :---------------- | :----------------- | :------------------------------ | :------------------------------------------------------- |
| **Milvus**      | Go/C++      | 微服务/分布式     | Zilliz Cloud       | **极高** (Etcd/Pulsar/MinIO)    | 十亿级+、追求极致性能、有专业运维团队                    |
| **Pinecone**    | -           | SaaS Only         | Pinecone           | **零** (完全托管)               | 追求快速上线、零运维、资金充裕、数据可出境无需私有化     |
| **Weaviate**    | Go          | 单机/分布式       | Weaviate Cloud     | **中** (依赖少)                 | 混合搜索、RAG 知识库、结构化过滤、知识图谱类应用         |
| **Qdrant**      | Rust        | 单机/分布式       | Qdrant Cloud       | **中低** (单二进制文件)         | 高性能单机/集群、推荐系统、注重高资源效率                |
| **Chroma**      | Python/Rust | 嵌入式/C-S        | Chroma Cloud(Beta) | **低** (本地) / **中** (服务端) | 开发原型、中小规模应用、Python 开发者                    |
| **PGVector**    | C           | Extension         | 各大云厂商 RDS     | **低** (复用现有 PG 运维)       | 现有 PG 用户、中小规模（<2000 万）、全栈工程师           |
| **VectorChord** | Rust        | Extension         | VectorChord Cloud  | **低** (PG 扩展)                | PG 用户但需更高性能/更低成本                             |
| **FAISS**       | C++         | Library           | -                  | **无** (代码库)                 | 离线批处理、自研向量引擎底层                             |
| **ES (8.x)**    | Java        | 分布式            | Elastic Cloud      | **高** (JVM/Shard 调优)         | 极其依赖全文检索（Keyword）、已有 ES 集群、日志分析+向量 |
| **MongoDB**     | C++         | 分布式/Cloud      | MongoDB Atlas      | **低 (全托管)**                 | 现有 Mongo 用户、元数据灵活、敏捷开发                    |
| **Vertex AI**   | -           | Cloud Only        | Google Cloud       | **零** (但需配置 VPC)           | 绑定 Google 生态、超大规模、高吞吐                       |
| **Redis**       | C           | 分布式            | Redis Enterprise   | **中高** (内存管理)             | 实时性要求极高、缓存+向量、不差钱                        |
| **ClickHouse**  | C++         | 分布式            | ClickHouse Cloud   | **中** (ZooKeeper)              | 向量分析、大规模数据扫描、OLAP                           |
| **LanceDB**     | Rust        | 嵌入式/Serverless | LanceDB Cloud      | **低** (嵌入式)                 | 多模态数据、Python 本地开发、S3 存储                     |
| **Vespa**       | C++/Java    | 分布式            | Vespa Cloud        | **极高** (复杂配置)             | 复杂的推荐/广告系统、实时模型推理                        |

### **3.2 技术能力与高级特性**

| 特性         | Milvus                           | Qdrant                            | Weaviate                            | Pinecone                   | Vertex AI | Redis      | Elasticsearch | MongoDB                     | LanceDB               | PGVector                | VectorChord          |
| :----------- | :------------------------------- | :-------------------------------- | :---------------------------------- | :------------------------- | :-------- | :--------- | :------------ | :-------------------------- | :-------------------- | :---------------------- | :------------------- |
| **索引算法** | HNSW, IVF_FLAT, DiskANN, GPU-IVF | HNSW (定制优化, 支持量化)         | HNSW, Flat, Dynamic                 | Proprietary (基于 Graph)   | ScaNN     | HNSW, Flat | HNSW          | HNSW (Lucene)               | IVF-PQ                | HNSW, IVFFlat           | HNSW, VChord         |
| **磁盘索引** | **支持 (DiskANN)** - 降本神器    | 支持 (Mmap / Binary Quantization) | 支持 (PQ + Product Quantization)    | 支持 (Serverless 分层存储) | N/A       | 弱         | 依赖 OS       | N/A (Atlas Tiering)         | **强**                | 依赖 OS Page Cache      | **强 (VChord)**      |
| **混合搜索** | 支持 (需手动结合 / RRF)          | 支持 (Query 层面)                 | **原生强项** (Alpha 调节 + Ranking) | 支持 (Hybrid Search)       | 支持      | 支持       | **极强**      | 支持 (Reciprocal Rank)      | 支持                  | 需自行写 SQL + 代码逻辑 | 支持 (PG SQL)        |
| **多模态**   | 强 (支持多向量检索)              | 中                                | 强 (模块化自动向量化)               | 中                         | 强        | 弱         | 中            | 中                          | **强**                | 弱 (需应用层处理)       | 中                   |
| **扩展性**   | **极高 (存储计算彻底分离)**      | 高 (分片)                         | 高 (分片)                           | 自动扩展 (Serverless)      | 极高      | 高 (分片)  | 高 (分片)     | 高 (分片)                   | 自动扩展 (Serverless) | 受限于 PG 单实例上限    | 受限于 PG 单实例上限 |
| **冷热分离** | 支持                             | 支持                              | 支持                                | 自动 (Tiered Storage)      | 自动      | 弱         | 支持          | 自动 (Atlas Online Archive) | **强**                | 依赖 OS Page Cache      | 支持 (PG)            |

### **3.3 量化与压缩支持**

向量量化是降低内存占用和提升查询速度的关键技术。以下对比各数据库的量化能力：

| 数据库          | Scalar Quantization (SQ) | Product Quantization (PQ) | Binary Quantization (BQ) | 其他量化                                           | 压缩比 |
| :-------------- | :----------------------- | :------------------------ | :----------------------- | :------------------------------------------------- | :----- |
| **Milvus**      | ✅ HNSW_SQ (int8)        | ✅ HNSW_PQ, IVF_PQ        | ✅ BIN_FLAT              | HNSW_PRQ (残差量化)                                | 4x-32x |
| **Qdrant**      | ✅ Scalar (int8)         | ✅ Product Quantization   | ✅ Binary (1-bit)        | 1.5-bit, 2-bit, Asymmetric                         | 4x-32x |
| **Weaviate**    | ✅ SQ                    | ✅ PQ                     | ✅ BQ                    | -                                                  | 4x-32x |
| **Pinecone**    | ✅ (内置优化)            | ✅ (内置优化)             | -                        | 专有算法                                           | 自动   |
| **PGVector**    | ✅ halfvec (16-bit)      | -                         | ❌                       | sparsevec (稀疏)                                   | 2x     |
| **VectorChord** | ✅ SQ                    | -                         | ✅ BQ                    | **RaBitQ** (理论误差边界)<sup>[[16]](#ref16)</sup> | 4x-32x |
| **ES (8.x)**    | ✅ int8_hnsw             | ❌                        | ❌                       | int4_hnsw                                          | 4x-8x  |
| **LanceDB**     | ✅                       | ✅ IVF-PQ                 | ❌                       | Lance 原生压缩                                     | 4x-16x |
| **FAISS**       | ✅                       | ✅ (各种组合)             | ✅                       | OPQ, LSQ, SQ4                                      | 4x-64x |
| **ClickHouse**  | ✅ QBit (4-64 bit)       | ❌                        | ❌                       | 可变精度量化                                       | 2x-8x  |

### **3.4 性能基准测试参考**

以下数据综合自 ANN-Benchmarks<sup>[[23]](#ref23)</sup>和 VectorDBBench<sup>[[24]](#ref24)</sup>等权威测评，仅供选型参考。实际性能受硬件配置、数据集特征、索引参数等因素影响显著。

| 数据库            | 典型 QPS (百万级数据) | P99 延迟 | 可扩展规模 | 最佳场景                 |
| :---------------- | :-------------------- | :------- | :--------- | :----------------------- |
| **Milvus**        | 10,000-50,000         | <10ms    | 十亿级+    | 大规模、高吞吐、自托管   |
| **Qdrant**        | 10,000-50,000         | <10ms    | 亿级       | 高性能过滤、资源效率优先 |
| **Weaviate**      | 5,000-20,000          | <100ms   | 亿级       | 混合搜索、RAG 应用       |
| **Pinecone**      | 100,000+              | <50ms    | 亿级       | 全托管、快速上线         |
| **PGVector**      | 1,000-5,000           | 10-100ms | 千万级     | 现有 PG 用户、中小规模   |
| **VectorChord**   | 3,000-10,000          | 5-50ms   | 千万级     | PG 用户需更高性能        |
| **Elasticsearch** | 3,000-10,000          | 10-100ms | 亿级       | 全文+向量混合            |
| **Redis Stack**   | 50,000+               | <1ms     | 千万级     | 超低延迟、实时应用       |

> [!NOTE] > **测试环境说明:** 以上数据基于 768-1536 维向量、HNSW 索引、90%+ 召回率的典型配置。生产环境部署前应使用实际数据进行基准测试。

> [!IMPORTANT] > **VectorDBBench 关键发现 (1M 数据集, $1000/月成本):**
>
> - **Zilliz Cloud (8cu):** P99 延迟 2.5ms, QPS 9,704
> - **Milvus (16c64g-sq8):** P99 延迟 2.2ms, QPS 3,465
> - **Qdrant Cloud (16c64g):** P99 延迟 6.4ms, QPS 1,242
> - **Pinecone (p2.x8):** P99 延迟 13.7ms, QPS 1,147

## **4. 选型决策**

选型没有绝对的“最好”，只有“最合适”。建议从**数据隐私**、**数据规模**、**查询复杂度**、**运维能力**这四个维度构建决策树。

### **4.1 选型决策树**

1. **数据隐私与合规性（Showstopper）**
   - **核心问题:** 你的数据能否离开私有网络？能否存储在第三方 SaaS 平台？这里可以增加关于 'BYOC' (Bring Your Own Cloud) 或 VPC Peering 的考量。很多企业版 SaaS 服务（如 Zilliz Cloud Enterprise, Pinecone Enterprise）支持在客户的 VPC 中部署数据平面，这是一种折中方案。
   - **路径 A (必须私有化):** 排除 Pinecone。
     - 如果有强大的 K8s 运维团队 -> **Milvus**。
     - 如果希望架构简单、性能强劲 -> **Qdrant**。
     - 如果数据量不大且已有 PG -> **PGVector**。
   - **路径 B (可以上公有云):**
     - 追求最快上线速度 -> **Pinecone** 或 **Zilliz Cloud**。
2. **数据规模与成本（TCO）**
   - **< 100 万向量:** 这是一个非常小的规模。任何方案在性能上差异都不大。
     - **推荐:** **PGVector** 或 **Chroma**。不要为了这么点数据引入复杂的 Milvus 集群，那是“高射炮打蚊子”。
   - **100 万 - 5000 万向量:** 这是大多数企业 RAG 知识库的规模区间。
     - **推荐:** **Qdrant** 和 **Weaviate**。它们在单机大内存机器上就能跑得非常欢，且能提供比 PGVector 更好的 QPS 和延迟。
   - **> 1 亿 - 10 亿+向量:** 这是推荐系统、日志分析的规模。
     - **推荐:** **Milvus** 或 **Pinecone**。你需要分片（Sharding）、副本（Replication）和复杂的资源调度。Milvus 的云原生架构在处理这种规模时优势尽显。
3. **查询模式（Query Pattern）**
   - **场景:** “我想搜关于‘合同法’的文档，且必须是 2023 年发布的 PDF 文件。”
   - **分析:** 这是一个典型的**混合搜索**（关键词 + 向量）+ **结构化过滤**场景。
   - **推荐:** **Weaviate** 或 **Elasticsearch**。Weaviate 的混合搜索打分机制非常成熟；Elasticsearch 则是传统的文本搜索之王，增加了向量能力后如虎添翼。
   - **场景:** “根据这张图片，推荐相似的图片。”
   - **分析:** 这是一个纯向量相似度检索，对 Latency 要求极高。
   - **推荐:** **Qdrant**、**Weaviate** 或 **Milvus**。
4. **团队技术栈**
   - **Rust/Go 极客团队:** 选择 Qdrant，你会喜欢它的代码质量和性能。
   - **Python/AI 算法团队:** Chroma 是你们的舒适区。
   - **传统 Java/后端团队:** Milvus 或 Elasticsearch 更符合你们的微服务架构习惯。
   - **全栈/DBA 团队:** PGVector 是最安全的选择。

### **4.2 场景决策路径**

1. **场景 A：极简的 RAG 应用 / 个人项目 / POC**
   - **推荐:** **Chroma** 或 **LanceDB**。
   - **理由:** 它们可以嵌入在你的应用进程中运行，无需 Docker，直接存文件或 S3。这种“无服务器”体验能让你在 5 分钟内跑通代码，极其适合快速验证想法。不要在 POC 阶段浪费时间去部署 Kubernetes。
   - **反面教材:** 为了存几千条 PDF 数据，去部署一套 Milvus 集群，光是启动 Pulsar 和 Etcd 就花了半天时间。
   - **升级路径**：
     - **初始阶段:** 使用 **Chroma** 或 **PGVector**。无需申请预算，直接在开发环境跑通流程。
     - **生产阶段:** 如果文档数量超过 500 万，或者需要精细的权限控制（如 HR 文档只能 HR 看），迁移至 **Weaviate** 或 **Qdrant**。因为它们对 Filter Search 的优化更好。
2. **场景 B：企业级知识库 (+精准的文本匹配)**
   - **推荐:** **Weaviate**、**Elasticsearch**、**MongoDB Atlas**。
   - **理由:** 纯向量搜索在匹配专有名词（如产品型号“X-2000”、人名、法律条款号）时效果很差，因为这些词在 Embedding 空间中可能被“平均化”了（Out-of-vocabulary 问题），必须结合关键词检索解决 OOV 问题。Weaviate 利用 RRF 自动平衡关键词和向量权重、ES 的 kNN + Filter、MongoDB Atlas 的 Lucene 全文检索（$vectorSearch + $search）都具备良好的 Hybrid Search 能力。
3. **场景 C：高并发推荐系统 / 广告投放 / 图像搜索**
   - **推荐:** **Milvus** (通用高性能), **Vertex AI** (Google 生态), 或 **Vespa** (复杂计算)。
   - **理由:** 这里的瓶颈是 QPS（每秒查询数）、Latency（延迟）、写入实时性。如果需要每秒处理数万次查询，且要求 10ms 内返回，**Redis** 或 **Qdrant** 是单机性价比之选；如果规模大到需要分布式，Milvus 的存算分离架构能让你独立扩容查询节点（Query Node）以保证高并发（Milvus 对 GPU 索引的支持可以进一步降低超大规模下的延迟），消息队列架构保证了写入不丢数据。Vespa 可以在检索时做复杂的重排序（Re-ranking），适合广告业务。
4. **场景 D：极速实时会话 / 状态判断**
   - **推荐:** **Redis Stack**。
   - **理由:** 只有全内存能满足亚毫秒级（<1ms）的延迟需求。例如，根据用户当前的点击流实时推荐下一个商品，或者在对话系统中实时检索上下文，Redis 的速度是无可比拟的。
   - **成本警示:** 务必计算好 RAM 成本，Redis 存向量非常贵，不适合存海量历史数据。
5. **场景 E：海量数据的向量分析 (Vector Analytics)**
   - **推荐:** **ClickHouse**。
   - **理由:** 你的目的不是查 Top-K，而是为了统计“聚类中心的分布”、“向量维度的相关性”，或者“找出所有距离中心点 > 0.8 的异常点”。OLAP 数据库的列式存储和向量函数是最佳选择。
6. **场景 F：已有的 PostgreSQL 业务**
   - **推荐:** **PGVector** 或 **VectorChord**。
   - **理由:** 永远不要低估“不引入新数据库”带来的维护红利。
     - 默认首选 **PGVector**，因为它已是行业标准，云厂商支持最好。
     - 如果你发现 PGVector 内存占用太高，或者 QPS 达不到要求，**不要急着迁移到 Milvus**，先尝试安装 **VectorChord (pgvecto.rs)** 扩展。它可能在不改变架构的情况下，通过更高效的量化索引解决你的性能问题。
7. **场景 G：初创公司的 AI 特性开发**
   - **核心痛点:** 没钱招运维，只想专注写 Prompt 和业务逻辑。
   - **推荐:** **Pinecone (Serverless 模式)**。
   - **理由:** 这是一个“用钱买时间”的最佳案例。虽然单位数据的存储成本可能高于自建，但考虑到省去的运维工程师薪资（通常比服务器贵得多）和节省的 debug 时间，Pinecone 依然是 ROI 最高的选择。

### **4.3 多云多分区（Multi-Region/Cloud）环境选型路径**

出于业务全球化与成本考虑，单一数据中心可能无法满足低延迟访问、合规性（如 GDPR）、议价能力等要求。**多云（Multi-Cloud）或多分区（Multi-Region）架构下，向量数据库选型将面临着新的挑战：CAP 定理的取舍、昂贵的跨域流量费以及数据同步机制**。

#### **核心挑战**

1. **物理延迟 (Speed of Light):** 向量搜索对延迟极其敏感。如果用户在新加坡，查询请求还要飞到美东数据中心，物理延迟（~200ms）会直接吃掉所有的算法优化红利。**基本原则：计算靠近用户（Data Locality）。**
2. **数据一致性 (Consistency):** 在 RAG 场景中，如果美东更新了知识库，欧洲用户多久能搜到？是强一致（Raft 跨域，极慢）还是最终一致（异步复制）？
3. **数据主权 (Data Sovereignty):** 某些国家的法律规定，特定用户数据（如 Embedding 对应的原始文本）不能出境。这要求数据库支持**按地域分片（Geo-partitioning）**，而不仅仅是全量复制。

#### **架构模式与选型推荐**

1. **模式 A：主从异步复制 (Global Read, Central Write)**

- **场景:** 全球化的 RAG 知识库、电商搜索。写入量较小（运营后台更新），读取量巨大且分布在全球。
- **架构:** 在一个核心 Region（如美东）写入，通过 CDC 或内置复制机制异步同步到全球边缘 Region。
- **推荐方案:**
  - **Milvus:** 利用 **MilvusCDC** 或底层消息队列（Pulsar/Kafka）的 Geo-replication 功能。这是最成熟的方案，支持 Active-Passive 和双向同步。
  - **Redis Enterprise (Active-Active):** 基于 CRDTs（无冲突复制数据类型）实现真正的多活。任何节点的写入都会自动合并同步到全球。虽然贵，但对于需要“全球即时状态同步”的场景（如全球用户会话管理）是唯一解。
  - **Zilliz Cloud / Pinecone Enterprise:** 商业版 SaaS 通常提供“Global Database”选项，一键配置跨区只读副本，省去自建管道的麻烦。

2. **模式 B：完全孤岛模式 (Sovereign Clouds)**

- **场景:** 金融、政府项目，严格的数据合规要求。欧洲的数据只能在欧洲存算。
- **架构:** 各 Region 部署独立的向量库集群，应用层根据用户所在地路由请求。各集群间**不进行数据同步**，或仅同步脱敏后的通用知识。
- **推荐方案:**
  - **Qdrant / Weaviate / LanceDB:** 这些支持单机/轻量级部署的数据库优势明显。你不需要在每个 Region 都部署一套庞大的 Milvus 微服务，只需在当地部署一套轻量集群即可。

1. **模式 C：边缘计算向量库 (Edge Vector Store)**

- **场景:** 移动端设备、IoT 设备、极端边缘环境。
- **架构:** 向量库直接运行在用户设备或边缘节点（Cloudflare Workers, AWS Lambda@Edge）上。
- **推荐方案:**
  - **LanceDB / Chroma (Embedded):** 配合 S3 存储，可以在 Serverless 环境中快速冷启动，非常适合边缘推理。
  - **SQLite-vss:** 极度轻量，直接嵌入在端侧应用中。

#### **避坑指南**

- **不要跨大洋拉 Raft 集群:** 严禁将 Etcd 或 Zookeeper 的节点部署在不同的大洲。Raft 协议要求半数以上节点确认才能写入，跨洋延迟会导致整个集群的写入瘫痪。**跨域只能做异步复制，不能做共识层。**
- **警惕 Egress Cost (流量刺客):** 云厂商的跨域流量费极贵。如果你的向量维度很高（如 1536 维）且更新频繁，全量同步的带宽成本可能超过数据库本身的计算成本。**建议:** 在同步前对向量进行量化压缩（Binary Quantization），或者仅同步原始文本，在目标区域重新 Embedding（用算力换带宽）。

### **4.4 总结**

- **通用最强 & 大规模首选:**
  - Milvus：凭借架构的先进性和社区活跃度占据高端市场。
  - Qdrant：凭借高性能和易用性迅速吞噬中端市场。
- **开发体验最爽 & 快速落地:**
  - Pinecone：OpenAI 钦点，依然是标杆，但面临着来自 Zilliz Cloud 和 MongoDB Atlas、 Vector Search 等巨头的强力挑战。
  - Chroma, LanceDB
- **PGVector 的“降维打击”:** **永远不要低估“不引入新数据库”带来的维护红利。**对于 80% 的非超大规模应用来说，**“Just use Postgres”** 正在成为默认选项。这也逼迫原生向量数据库必须在**性能、多模态、易用性**上拿出显著优于 PG 的必杀技。
- **文本结合 & 搜索增强:** Weaviate、Elasticsearch
- **特定领域:** Redis (极致实时), ClickHouse (OLAP), Vespa (复杂推荐), Vertex AI (Google 全家桶)

## **5. 量化指标与 TCO 控制**

### **5.1 核心评估指标**

- **Recall@K (召回率):** 查出来的 Top-K 结果中，有多少是真的最近邻？
- **QPS (每秒查询数):** 在特定召回率下的最大吞吐量。
- **Latency (P99 延迟):** 尾部延迟，决定了用户体验。
- **Cost per Query:** 每次查询的硬件成本。

**评估工具：**

- **ann-benchmarks:** 行业标准，包含 HNSW, Faiss, Annoy 等算法的基准数据。
- **VectorDBBench:** 专门针对主流向量数据库（Milvus, Weaviate, Qdrant 等）的对比测试工具。

### **5.2 内存估算公式 (Memory Footprint)**

在选型时，不要只关注软件是否免费，却忽略了硬件成本。

对于最常用的 **HNSW** 索引，内存占用主要由两部分组成：原始向量 + 图索引结构。

$$
  \text{Memory} \approx N \times ( d \times 4 \text{B} + M \times 2 \times 4 \text{B} ) \times \text{Overhead}
$$

- N: 向量总数
- d: 维度 (如 1536)
- 4B: float32 占用 4 字节
- M: HNSW 的每个节点最大连接数 (通常 16-64)
- Overhead: 系统的额外开销 (通常 1.2 - 1.5)

**案例:** 1000 万条 OpenAI 向量 (1536 维)，M=16。

- Raw Vector: $10^7 \times 1536 \times 4B ≈ 57 GB$
- HNSW Graph: $10^7 \times 16 \times 2 \times4B ≈ 1.2 GB$
- Total: ~60GB RAM。
- **结论:** 你需要一台 64GB 甚至 128GB 内存的服务器。

### **5.3 TCO 控制策略**

1. **标量量化 (SQ8):** 将 float32 转为 int8，内存减少 4 倍。上述案例仅需 ~15GB。
2. **二进制量化 (Binary Quantization):** 将 float32 转为 bit，内存减少 32 倍。适合维度 > 1024 的场景。
3. **磁盘索引 (DiskANN):** 仅将图形结构存在内存，向量存在 NVMe SSD。内存消耗可降低 10 倍以上。推荐 Milvus, Qdrant, VectorChord。

## **6. 一些趋势**

向量数据库领域正在经历从“野蛮生长”到“洗牌整合”的阶段，把握一些趋势可以在选型时避免在长线上踩坑。

- **存算分离与 Serverless 普及:** 所有的向量数据库都在向 Serverless 嵌入式（Embedded）演进。用户不再关心“集群”和“分片”，只关心使用 API “存了多少向量，查了多少次”。未来的向量数据库可能更像是一个底层的存储引擎库，而不是一个独立的服务器软件。
- **标配磁盘索引 (DiskANN):** 随着模型上下文变长，向量维度增加，全内存存储成本太高。基于 NVMe SSD 的高效向量索引（如 Vamana 算法）将成为标配，让单机存储容量提升 10 倍以上。
- **检索技术的融合 (Retrieval Convergence):** 纯向量检索（Dense Retrieval）已被证明在精确匹配上存在短板。未来的数据库将内置更复杂的检索流水线：**稀疏向量 (Sparse/Splade) + 稠密向量 (Dense) + 重排序 (Re-ranking)**。数据库将不再只是存储，而是通过内置轻量级模型（如 BGE-M3），承担部分 Compute 任务，实现“检索即推理”。同时，结合知识图谱的 **GraphRAG** 正在成为新热点，向量数据库与**图数据库**的边界将进一步模糊（如 Neo4j 增加向量支持，Weaviate 增强图特性），以解决复杂推理问题。
- **“Just use Postgres” 成为主流:** 随着 PGVector 性能的优化（如并行索引构建、量化支持）以及 VectorChord 等高性能插件的出现，对于 80% 的非超大规模应用，独立部署向量数据库的 ROI 越来越低。PostgreSQL 正在成为向量数据库领域的“丰田卡罗拉”——**不是最快的，但是最可靠、最通用的**。
- **Agentic Memory (Agent 记忆体):** 随着 AI Agent 的兴起，向量数据库将演变为 Agent 的“长期记忆体”（Long-term Memory）。这对数据的实时更新（Real-time Update）、版本控制（Versioning）和多模态理解能力提出了更高要求。这可能是 LanceDB 等新一代数据库弯道超车的机会。

## **7. References**

<a id="ref1"></a>[1] P. Lewis, E. Perez, A. Piktus, et al., "Retrieval-augmented generation for knowledge-intensive NLP tasks," _Proc. Adv. Neural Inf. Process. Syst._, vol. 33, pp. 9459–9474, 2020.

<a id="ref2"></a>[2] A. Moffat and J. Zobel, "BM25 revisited," _Proc. Aust. Document Comput. Symp._, pp. 1–4, 2014.

<a id="ref3"></a>[3] A. Vaswani, N. Shazeer, N. Parmar, et al., "Attention is all you need," _Adv. Neural Inf. Process. Syst._, vol. 30, pp. 5998–6008, 2017.

<a id="ref4"></a>[4] Y. A. Malkov and D. A. Yashunin, "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 42, no. 4, pp. 824–836, 2018.

<a id="ref5"></a>[5] H. Jégou, M. Douze, and C. Schmid, "Product quantization for nearest neighbor search," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 33, no. 1, pp. 117–128, 2011.

<a id="ref6"></a>[6] S. Jayaram Subramanya, D. Kadekodi, R. Krishnaswamy, and H. V. Simhadri, "DiskANN: Fast accurate billion-point nearest neighbor search on a single node," _Adv. Neural Inf. Process. Syst._, vol. 32, 2019.

<a id="ref7"></a>[7] J. Wang, X. Yi, R. Guo, et al., "Milvus: A purpose-built vector data management system," _Proc. Int. Conf. Manag. Data (SIGMOD)_, pp. 2614–2627, 2021.

<a id="ref8"></a>[8] J. Johnson, M. Douze, and H. Jégou, "Billion-scale similarity search with GPUs," _IEEE Trans. Big Data_, vol. 7, no. 3, pp. 535–547, 2019.

<a id="ref9"></a>[9] R. Guo, P. Sun, E. Lindgren, et al., "Accelerating large-scale inference with anisotropic vector quantization," _Int. Conf. Mach. Learn._, pp. 3887–3896, 2020.

<a id="ref10"></a>[10] Milvus Official Documentation, "Architecture Overview," Zilliz, 2024. [Online]. Available: https://milvus.io/docs/architecture_overview.md

<a id="ref11"></a>[11] Qdrant Official Documentation, "What is Qdrant?," Qdrant, 2024. [Online]. Available: https://qdrant.tech/documentation/overview/

<a id="ref12"></a>[12] Weaviate Official Documentation, "Vector Indexing," Weaviate, 2024. [Online]. Available: https://weaviate.io/developers/weaviate/concepts/vector-index

<a id="ref13"></a>[13] Pinecone Official Documentation, "Pinecone Documentation," Pinecone, 2024. [Online]. Available: https://docs.pinecone.io/

<a id="ref14"></a>[14] pgvector, "Open-source vector similarity search for Postgres," GitHub, 2024. [Online]. Available: https://github.com/pgvector/pgvector

<a id="ref15"></a>[15] VectorChord, "Scalable, fast, and disk-friendly vector search in Postgres," TensorChord, 2024. [Online]. Available: https://github.com/tensorchord/VectorChord

<a id="ref16"></a>[16] J. Gao and C. Long, "RaBitQ: Quantizing High-Dimensional Vectors with a Theoretical Error Bound for Approximate Nearest Neighbor Search," _Proc. ACM Manag. Data_, vol. 2, no. 3, pp. 1–27, 2024.

<a id="ref17"></a>[17] Elasticsearch Official Documentation, "kNN search in Elasticsearch," Elastic, 2024. [Online]. Available: https://www.elastic.co/guide/en/elasticsearch/reference/current/knn-search.html

<a id="ref18"></a>[18] MongoDB Atlas Documentation, "Vector Search Overview," MongoDB, 2024. [Online]. Available: https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-overview/

<a id="ref19"></a>[19] Google Cloud Documentation, "Vector Search," Vertex AI, 2024. [Online]. Available: https://cloud.google.com/vertex-ai/docs/vector-search/overview

<a id="ref20"></a>[20] LanceDB, "Vector Database for RAG, Agents & Hybrid Search," LanceDB Inc., 2024. [Online]. Available: https://lancedb.com/

<a id="ref21"></a>[21] ClickHouse Documentation, "Exact and Approximate Vector Search," ClickHouse, 2024. [Online]. Available: https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/annindexes

<a id="ref22"></a>[22] FAISS Wiki, "A library for efficient similarity search and clustering of dense vectors," Meta AI Research, 2024. [Online]. Available: https://github.com/facebookresearch/faiss/wiki

<a id="ref23"></a>[23] M. Aumüller, E. Bernhardsson, and A. Faitfull, "ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms," _Proc. Int. Conf. Similarity Search and Appl. (SISAP)_, pp. 34–49, 2020. [Online]. Available: https://ann-benchmarks.com/

<a id="ref24"></a>[24] Zilliz, "VectorDBBench: An Open-Source Vector Database Benchmark Tool," Zilliz, 2024. [Online]. Available: https://zilliz.com/vector-database-benchmark-tool
