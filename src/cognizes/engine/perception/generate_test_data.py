"""
æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ (generate_test_data.py)

ç”Ÿæˆå‘é‡æ•°æ®ç”¨äºéªŒè¯ High-Selectivity Filtering åœºæ™¯çš„ Recall@10ã€‚
æ”¯æŒé…ç½®ä¸åŒæ•°æ®è§„æ¨¡ï¼š10 ä¸‡ (å¿«é€Ÿæµ‹è¯•) å’Œ 1000 ä¸‡ (æ€§èƒ½éªŒè¯)ã€‚

ç”¨æ³•:
    python generate_test_data.py --scale quick    # 10 ä¸‡æ¡
    python generate_test_data.py --scale full     # 1000 ä¸‡æ¡
"""

from __future__ import annotations

import argparse
import asyncio
import random
import time
import uuid

import asyncpg
import numpy as np

# æ•°æ®è§„æ¨¡é…ç½®
SCALE_CONFIG = {
    "quick": {"total_records": 100_000, "batch_size": 5_000, "description": "å¿«é€Ÿæµ‹è¯• (10 ä¸‡æ¡)"},
    "full": {"total_records": 10_000_000, "batch_size": 10_000, "description": "æ€§èƒ½éªŒè¯ (1000 ä¸‡æ¡)"},
}


async def generate_test_data(
    pool: asyncpg.Pool,
    total_records: int,
    batch_size: int,
    rare_user_ratio: float = 0.01,
):
    """
    ç”Ÿæˆæµ‹è¯•æ•°æ®

    Args:
        pool: æ•°æ®åº“è¿æ¥æ± 
        total_records: æ€»è®°å½•æ•°
        batch_size: æ‰¹é‡æ’å…¥å¤§å°
        rare_user_ratio: ç¨€æœ‰ç”¨æˆ·æ•°æ®å æ¯” (é»˜è®¤ 1%)
    """
    rare_user_id = "rare_user_001"
    common_users = [f"common_user_{i:04d}" for i in range(100)]

    print(f"\nğŸ“Š æ•°æ®ç”Ÿæˆå‚æ•°:")
    print(f"   - æ€»è®°å½•æ•°: {total_records:,}")
    print(f"   - ç¨€æœ‰ç”¨æˆ·: {rare_user_id} ({rare_user_ratio:.1%})")
    print(f"   - é¢„è®¡ç¨€æœ‰ç”¨æˆ·è®°å½•: {int(total_records * rare_user_ratio):,}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size:,}")
    print(f"   - é¢„è®¡æ‰¹æ¬¡æ•°: {total_records // batch_size}")

    start_time = time.time()

    for batch_idx, batch_start in enumerate(range(0, total_records, batch_size)):
        batch_end = min(batch_start + batch_size, total_records)
        records = []

        for i in range(batch_start, batch_end):
            # æŒ‰æ¯”ä¾‹åˆ†é…ç”¨æˆ·
            if random.random() < rare_user_ratio:
                user_id = rare_user_id
            else:
                user_id = random.choice(common_users)

            # ç”Ÿæˆéšæœºå‘é‡ (1536 ç»´ï¼ŒåŒ¹é… OpenAI ada-002)
            embedding = np.random.randn(1536).astype(np.float32).tolist()

            # ç”Ÿæˆä¸°å¯Œçš„å…ƒæ•°æ®ç”¨äº Complex Predicates æµ‹è¯•
            metadata = {
                "index": i,
                "batch": batch_idx,
                "priority": random.randint(1, 5),
                "tags": random.sample(["research", "note", "task", "meeting", "important"], k=random.randint(1, 3)),
                "author": {"role": random.choice(["user", "admin", "expert"])},
                "status": random.choice(["draft", "published", "archived"]),
                "access_level": random.randint(1, 5),
            }

            records.append(
                (
                    str(uuid.uuid4()),
                    user_id,
                    "test_app",
                    f"Test content for document {i}. This is sample text for semantic search testing.",
                    embedding,
                    metadata,
                )
            )

        # æ‰¹é‡æ’å…¥
        await pool.executemany(
            """
            INSERT INTO memories (id, user_id, app_name, content, embedding, metadata)
            VALUES ($1, $2, $3, $4, $5, $6)
        """,
            records,
        )

        # è¿›åº¦æ˜¾ç¤º
        progress = batch_end / total_records * 100
        elapsed = time.time() - start_time
        rate = batch_end / elapsed if elapsed > 0 else 0
        eta = (total_records - batch_end) / rate if rate > 0 else 0

        print(
            f"\r   â³ è¿›åº¦: {progress:5.1f}% ({batch_end:,}/{total_records:,}) | é€Ÿç‡: {rate:,.0f}/s | ETA: {eta:.0f}s",
            end="",
            flush=True,
        )

    elapsed = time.time() - start_time
    print(f"\n\nâœ… æ•°æ®ç”Ÿæˆå®Œæˆ! è€—æ—¶: {elapsed:.1f}s")


async def verify_data_distribution(pool: asyncpg.Pool):
    """éªŒè¯æ•°æ®åˆ†å¸ƒ"""
    print("\nğŸ“ˆ æ•°æ®åˆ†å¸ƒéªŒè¯:")

    total_count = await pool.fetchval("SELECT COUNT(*) FROM memories WHERE app_name = 'test_app'")
    rare_count = await pool.fetchval("SELECT COUNT(*) FROM memories WHERE user_id = 'rare_user_001'")

    print(f"   - æ€»è®°å½•æ•°: {total_count:,}")
    print(f"   - ç¨€æœ‰ç”¨æˆ·è®°å½•: {rare_count:,} ({rare_count / total_count:.2%})")

    # éªŒè¯å…ƒæ•°æ®åˆ†å¸ƒ
    admin_count = await pool.fetchval("""
        SELECT COUNT(*) FROM memories
        WHERE metadata @> '{"author": {"role": "admin"}}'
    """)
    print(f"   - admin è§’è‰²è®°å½•: {admin_count:,} ({admin_count / total_count:.2%})")


async def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆ High-Selectivity æµ‹è¯•æ•°æ®")
    parser.add_argument("--scale", choices=["quick", "full"], default="quick", help="æ•°æ®è§„æ¨¡: quick=10ä¸‡, full=1000ä¸‡")
    parser.add_argument("--db-url", default="postgresql://aigc:@localhost/cognizes-engine", help="æ•°æ®åº“è¿æ¥ URL")
    parser.add_argument("--clean", action="store_true", help="æ¸…ç†ç°æœ‰æµ‹è¯•æ•°æ®åå†ç”Ÿæˆ")
    args = parser.parse_args()

    config = SCALE_CONFIG[args.scale]
    print(f"ğŸš€ {config['description']}")

    pool = await asyncpg.create_pool(args.db_url, min_size=2, max_size=10)

    if args.clean:
        print("\nğŸ—‘ï¸ æ¸…ç†ç°æœ‰æµ‹è¯•æ•°æ®...")
        await pool.execute("DELETE FROM memories WHERE app_name = 'test_app'")

    await generate_test_data(pool, total_records=config["total_records"], batch_size=config["batch_size"])

    await verify_data_distribution(pool)

    print("\nğŸ’¡ ä¸‹ä¸€æ­¥: è¿è¡ŒåŸºå‡†æµ‹è¯•éªŒè¯ Recall@10")
    print("   python benchmark.py --user-id rare_user_001")

    await pool.close()


if __name__ == "__main__":
    asyncio.run(main())
