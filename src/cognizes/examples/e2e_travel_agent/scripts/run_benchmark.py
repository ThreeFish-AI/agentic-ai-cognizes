"""
æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼šå¯¹æ¯” Google InMemory ä¸ Open Agent Engine (PostgreSQL) çš„å“åº”å»¶è¿Ÿ
"""

import asyncio
import time
import statistics
import sys
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ  src ç›®å½•åˆ° Python è·¯å¾„
_HERE = Path(__file__).resolve().parent
_SRC = _HERE.parent / "src"
_PROJECT_ROOT = _HERE.parents[4]  # agentic-ai-engine

if str(_SRC) not in sys.path:
    sys.path.insert(0, str(_SRC))
if str(_PROJECT_ROOT / "src") not in sys.path:
    sys.path.insert(0, str(_PROJECT_ROOT / "src"))

# å¯¼å…¥æœåŠ¡
from services import create_services
from config import config, BackendType
from agent import create_travel_agent
from google.adk.runners import Runner


@dataclass
class BenchmarkResult:
    backend: str
    total_requests: int
    avg_latency_ms: float
    p50_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    errors: int


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, num_requests: int = 50, concurrent_users: int = 5):
        self.num_requests = num_requests
        self.concurrent_users = concurrent_users
        self.test_messages = [
            "å¸®æˆ‘æŸ¥ä¸€ä¸‹å»å·´å˜å²›çš„æœºç¥¨",
            "æ¨èå‡ ä¸ªé€‚åˆåº¦å‡çš„åœ°æ–¹",
            "æˆ‘æƒ³è®¢ä¸€ä¸ªæµ·æ™¯é…’åº—",
            "æŸ¥ä¸€ä¸‹æ˜å¤©çš„å¤©æ°”",
            "æˆ‘ä¸å–œæ¬¢è¾£çš„é£Ÿç‰©ï¼Œæœ‰ä»€ä¹ˆæ¨èï¼Ÿ",
        ]

    async def run_single_request(
        self, runner: Runner, session_service, user_id: str, session_id: str, message: str
    ) -> tuple[float, bool]:
        """æ‰§è¡Œå•æ¬¡è¯·æ±‚å¹¶è¿”å›å»¶è¿Ÿ"""
        from google.genai import types

        start = time.perf_counter()
        try:
            new_message = types.Content(parts=[types.Part(text=message)])
            response_text = None
            async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=new_message):
                if hasattr(event, "content") and event.content:
                    for part in event.content.parts:
                        if hasattr(part, "text") and part.text:
                            response_text = part.text
            success = response_text is not None and len(response_text) > 0
            latency = (time.perf_counter() - start) * 1000
            return (latency, success)
        except Exception as e:
            latency = (time.perf_counter() - start) * 1000
            print(f"Request failed: {e}")
            return (latency, False)

    async def run_benchmark(self, backend_type: BackendType) -> BenchmarkResult:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        # è®¾ç½®åç«¯
        config.backend = backend_type

        # åˆ›å»ºæœåŠ¡
        session_service, memory_service = await create_services()
        agent = create_travel_agent()
        runner = Runner(
            agent=agent, app_name="benchmark_app", session_service=session_service, memory_service=memory_service
        )

        # é¢„çƒ­
        print(f"Warming up {backend_type.value} backend...")
        warmup_session = await session_service.create_session(app_name="benchmark_app", user_id="warmup_user")
        for i in range(2):
            await self.run_single_request(runner, session_service, "warmup_user", warmup_session.id, "test")

        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        print(f"Running benchmark for {backend_type.value}...")
        latencies = []
        errors = 0

        async def user_workload(user_id: str):
            nonlocal errors
            # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºç‹¬ç«‹çš„ Session
            user_session = await session_service.create_session(app_name="benchmark_app", user_id=user_id)
            user_latencies = []
            for i in range(self.num_requests // self.concurrent_users):
                msg = self.test_messages[i % len(self.test_messages)]
                latency, success = await self.run_single_request(runner, session_service, user_id, user_session.id, msg)
                user_latencies.append(latency)
                if not success:
                    errors += 1
            return user_latencies

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*[user_workload(f"user_{i}") for i in range(self.concurrent_users)])

        # åˆå¹¶å»¶è¿Ÿæ•°æ®
        for user_latencies in results:
            latencies.extend(user_latencies)

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        latencies.sort()
        return BenchmarkResult(
            backend=backend_type.value,
            total_requests=len(latencies),
            avg_latency_ms=statistics.mean(latencies),
            p50_latency_ms=latencies[int(len(latencies) * 0.50)],
            p95_latency_ms=latencies[int(len(latencies) * 0.95)],
            p99_latency_ms=latencies[int(len(latencies) * 0.99)],
            errors=errors,
        )

    async def compare_backends(self) -> dict:
        """å¯¹æ¯”ä¸¤ç§åç«¯çš„æ€§èƒ½"""
        results = {}

        # æµ‹è¯• Google InMemory (åŸºçº¿)
        results["google"] = await self.run_benchmark(BackendType.GOOGLE_MANAGED)

        # æµ‹è¯• PostgreSQL (æˆ‘ä»¬çš„å®ç°)
        results["postgres"] = await self.run_benchmark(BackendType.OPEN_ENGINE)

        return results


def print_comparison_report(results: dict):
    """æ‰“å°æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
    google = results["google"]
    postgres = results["postgres"]

    print("\n" + "=" * 70)
    print("ğŸ“Š æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š - Open Agent Engine vs Google InMemory")
    print("=" * 70)

    print(f"\n{'æŒ‡æ ‡':<25} {'Google InMemory':>18} {'PostgreSQL':>18} {'å·®å¼‚':>12}")
    print("-" * 70)

    metrics = [
        ("Total Requests", google.total_requests, postgres.total_requests),
        ("Avg Latency (ms)", google.avg_latency_ms, postgres.avg_latency_ms),
        ("P50 Latency (ms)", google.p50_latency_ms, postgres.p50_latency_ms),
        ("P95 Latency (ms)", google.p95_latency_ms, postgres.p95_latency_ms),
        ("P99 Latency (ms)", google.p99_latency_ms, postgres.p99_latency_ms),
        ("Errors", google.errors, postgres.errors),
    ]

    for name, g_val, p_val in metrics:
        diff = p_val - g_val if isinstance(g_val, float) else p_val - g_val
        diff_str = f"+{diff:.2f}" if diff > 0 else f"{diff:.2f}"
        print(f"{name:<25} {g_val:>18.2f} {p_val:>18.2f} {diff_str:>12}")

    # éªŒè¯ KPI
    p99_diff = postgres.p99_latency_ms - google.p99_latency_ms
    print("\n" + "-" * 70)
    if p99_diff < 100:
        print(f"âœ… KPI è¾¾æˆ: P99 å»¶è¿Ÿå·®å¼‚ {p99_diff:.2f}ms < 100ms é˜ˆå€¼")
    else:
        print(f"âŒ KPI æœªè¾¾æˆ: P99 å»¶è¿Ÿå·®å¼‚ {p99_diff:.2f}ms >= 100ms é˜ˆå€¼")
    print("=" * 70)


async def main():
    benchmark = PerformanceBenchmark(num_requests=50, concurrent_users=5)
    results = await benchmark.compare_backends()
    print_comparison_report(results)


if __name__ == "__main__":
    asyncio.run(main())
